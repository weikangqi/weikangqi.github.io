<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Blog</title>
  
  <subtitle>maoblog</subtitle>
  <link href="https://weikangqi.github.io/atom.xml" rel="self"/>
  
  <link href="https://weikangqi.github.io/"/>
  <updated>2025-09-11T03:36:51.304Z</updated>
  <id>https://weikangqi.github.io/</id>
  
  <author>
    <name>John Doe</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title></title>
    <link href="https://weikangqi.github.io/2025/09/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E9%87%8F%E5%8C%96/%E8%AE%AD%E7%BB%83%E9%87%8F%E5%8C%96%E9%95%BF%E5%B0%BE%E7%BA%A6%E6%9D%9F/"/>
    <id>https://weikangqi.github.io/2025/09/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E9%87%8F%E5%8C%96/%E8%AE%AD%E7%BB%83%E9%87%8F%E5%8C%96%E9%95%BF%E5%B0%BE%E7%BA%A6%E6%9D%9F/</id>
    <published>2025-09-11T03:36:51.304Z</published>
    <updated>2025-09-11T03:36:51.304Z</updated>
    
    
    
    
    
  </entry>
  
  <entry>
    <title></title>
    <link href="https://weikangqi.github.io/2025/09/opencl%20cl_arm_import_memory/"/>
    <id>https://weikangqi.github.io/2025/09/opencl%20cl_arm_import_memory/</id>
    <published>2025-09-11T03:36:51.303Z</published>
    <updated>2025-09-11T03:36:51.303Z</updated>
    
    
    
    
    
  </entry>
  
  <entry>
    <title></title>
    <link href="https://weikangqi.github.io/2025/09/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%8E%A8%E7%90%86/MNN-LLM/"/>
    <id>https://weikangqi.github.io/2025/09/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%8E%A8%E7%90%86/MNN-LLM/</id>
    <published>2025-09-11T03:36:51.303Z</published>
    <updated>2025-09-11T03:36:51.303Z</updated>
    
    
    
    
    
  </entry>
  
  <entry>
    <title></title>
    <link href="https://weikangqi.github.io/2025/09/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%8E%A8%E7%90%86/Moe/"/>
    <id>https://weikangqi.github.io/2025/09/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%8E%A8%E7%90%86/Moe/</id>
    <published>2025-09-11T03:36:51.303Z</published>
    <updated>2025-09-11T03:36:51.303Z</updated>
    
    
    
    
    
  </entry>
  
  <entry>
    <title>MNN Kvcache压缩</title>
    <link href="https://weikangqi.github.io/2025/08/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E9%87%8F%E5%8C%96/MNN%20KVcacahe/"/>
    <id>https://weikangqi.github.io/2025/08/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E9%87%8F%E5%8C%96/MNN%20KVcacahe/</id>
    <published>2025-08-06T15:30:16.000Z</published>
    <updated>2025-09-11T03:36:51.304Z</updated>
    
    
    
    
    
    <category term="LLM" scheme="https://weikangqi.github.io/tags/LLM/"/>
    
    <category term="量化" scheme="https://weikangqi.github.io/tags/%E9%87%8F%E5%8C%96/"/>
    
  </entry>
  
  <entry>
    <title>Lookahead</title>
    <link href="https://weikangqi.github.io/2025/07/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%8E%A8%E7%90%86/Lookahead/"/>
    <id>https://weikangqi.github.io/2025/07/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%8E%A8%E7%90%86/Lookahead/</id>
    <published>2025-07-03T15:30:16.000Z</published>
    <updated>2025-09-11T03:36:51.303Z</updated>
    
    <content type="html"><![CDATA[<p>Jacobi 迭代把自回归的N次迭代转换为N个方程，然后联合求解。而 <a href="https://zhida.zhihu.com/search?content_id=257559026&amp;content_type=Article&amp;match_order=1&amp;q=Jacobi+Decoding&amp;zhida_source=entity">Jacobi Decoding</a> 将每次迭代上一次的输出整体作为下一次的输入，其实就是把每一个 token 上的输出视作一个 2-gram，并以此作为Draft Model。论文“Break the Sequential Dependency of LLM Inference Using Lookahead Decoding”的作者想到，如果可以记录下更多的历史信息，就可以制造一个 N-gram 作为 Draft Model，这样就能提高 Speculative Decoding 的准确率。这就是Lookahead Decoding。简要来说，Lookahead=N-gram+Jacobi iteration+parallel verification，其利用 jacobi 迭代法同时提取和验证 n-grams，打破自回归解码的顺序依赖性，从而降低解码次数，实现推理加速。相比之前的并行解码，Lookahead Decoding即不需要草稿模型，也不需要像Medusa那样微调head。论文作者将 Jacobi Decoding 视为Lookahead Decoding在 2-gram 情况下的特例。</p><img src="https://github.com/hao-ai-lab/LookaheadDecoding/raw/main/media/lookahead-decoding.gif" alt="img" style="zoom: 33%;" /><p>为了加速解码过程，每个Lookahead Decoding步骤被分为两个并行分支：生成n-gram的lookahead分支和验证n-gram的verification分支，两者都在一个前向传播过程中执行。</p><ul><li>Lookahead（前瞻）分支：这是原始雅可比解码的过程。因为不一致性的问题，此过程不会被用作主要投机验证的机制，而是作为一种采样收集或者说生成 n-gram 的并行解码过程。Lookahead 分支的目的是生成新的 N-Grams，加上其中新生成的 Token 就可以用于构建下一次 Verify 分支的候选序列。</li><li>Verification（验证）分支：这个分支从n-gram集合中匹配的多个candidates作为投机验证输入，完成具体的投机采样过程。verification分支会选择并验证有希望的 n-gram ，并且会将其用于更新下一次 Lookahead 分支的序列。</li></ul><h2 id="推理"><a class="markdownIt-Anchor" href="#推理"></a> 推理</h2><img src="https://raw.githubusercontent.com/weikangqi/picgo/main/image-20250902163700561.png" alt="image-20250902163700561" style="zoom:50%;" /><p>推理时，由于 LLM 解码主要受内存带宽限制，因此我们在同一步骤中合并前瞻和验证分支，利用 GPU 的并行处理能力来隐藏开销。 mask就是并行解码的关键。本示例的mask具体如上图所示。</p><h2 id="prepare-for-next-iteration"><a class="markdownIt-Anchor" href="#prepare-for-next-iteration"></a> <strong>Prepare for next iteration</strong></h2><p>当前迭代结束之后，会为下一次迭代做好准备。具体是：</p><ul><li>更新 2D Window。用后一层替代前一层（最后一层由最新输出得到的logits填充），并根据被接受的 tokens 的数量截断每一层。在当前的序列中随机采样填充被截断的部分。</li><li>更新 n-gram。如何生成新的n-gram？其实就是就是在2d-window里面从上往下找。</li><li>更新下一次前向的 Attention Mask 和 KV Cache。假设接受了 k 个 tokens，就据此扩展 Attention Mask，并将这 k 个 tokens 的 cache 拼接到 KV Cache 上。</li><li>当满足退出条件，例如生成的 tokens 长度达到<code>max_length</code>时，返回结果。</li></ul><img src="https://raw.githubusercontent.com/weikangqi/picgo/main/lookahead-perf.png" alt="img" style="zoom: 33%;" />]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;Jacobi 迭代把自回归的N次迭代转换为N个方程，然后联合求解。而 &lt;a href=&quot;https://zhida.zhihu.com/search?content_id=257559026&amp;amp;content_type=Article&amp;amp;match_order=</summary>
      
    
    
    
    
    <category term="LLM" scheme="https://weikangqi.github.io/tags/LLM/"/>
    
    <category term="投机解码" scheme="https://weikangqi.github.io/tags/%E6%8A%95%E6%9C%BA%E8%A7%A3%E7%A0%81/"/>
    
  </entry>
  
  <entry>
    <title>Jacobi Decoding</title>
    <link href="https://weikangqi.github.io/2025/07/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%8E%A8%E7%90%86/jacobi%20decoding/"/>
    <id>https://weikangqi.github.io/2025/07/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%8E%A8%E7%90%86/jacobi%20decoding/</id>
    <published>2025-07-02T15:30:16.000Z</published>
    <updated>2025-09-11T03:36:51.303Z</updated>
    
    <content type="html"><![CDATA[<h1 id="jacobi-decoding"><a class="markdownIt-Anchor" href="#jacobi-decoding"></a> Jacobi Decoding</h1><p>在神经机器翻译（NMT）中，Transformer 模型已成为主流。然而，虽然 Transformer 在训练阶段可以高度并行化，但在推理阶段却依赖<strong>自回归解码</strong>（autoregressive decoding），即一次生成一个 token，每个新 token 又依赖前面已经生成的 token。这种<strong>顺序依赖</strong>导致推理速度成为瓶颈，特别是在需要低延迟的实际应用中。以往的研究多集中在 <strong>非自回归翻译（NAT）</strong>，它可以一次性并行生成整句翻译。但 NAT 方法往往需要重新设计模型结构、消耗大量训练资源，而且翻译质量通常会有所下降。</p><p>这篇 ACL 2023 的论文提出了一条<strong>全新的路径</strong>：与其重新训练模型，不如<strong>改变解码算法本身</strong>，让现有的自回归模型也能“并行解码”。</p><img src="https://raw.githubusercontent.com/weikangqi/picgo/main/equations.png" alt="img" style="zoom: 50%;" /><p>Jacobi Decoding 就是寻求更少的迭代，寻找方程组的解（fixed point）。Jacobi decoding 在生产上，目前还没有相对自回归获得比较大的加速比。可以查考下面的流程：</p><img src="https://raw.githubusercontent.com/weikangqi/picgo/main/jacobi-iteration.gif" alt="img" style="zoom: 33%;" /><p>从这个图可以看到，Jacobi decoding每次迭代更新多个token，每次计算输出步长是M，可以作为一个batch并行的推理，相对单步耗时是增加的，但是对于是访存受限型的来讲，如果accept率可以上升还是又加速效果的。</p><ol><li>PJ：以句子为单位，每次处理一个句子</li><li>PGJ：以字词为单位，每次处理一个字词</li><li>HGJ：PGJ 的基础上，增加EOS 字符中途快速退出的兼容</li></ol><img src="https://raw.githubusercontent.com/weikangqi/picgo/main/image-20250902112750988.png" alt="image-20250902112750988" style="zoom:50%;" /><h2 id="初始化guess-token"><a class="markdownIt-Anchor" href="#初始化guess-token"></a> 初始化Guess Token</h2><p>如果没有特别初始化，默认Guess Token填为&lt;pad&gt;</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">JacobiDecoder</span><span class="token punctuation">(</span>MTDecoder<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> tokenizer<span class="token punctuation">,</span> model<span class="token punctuation">,</span> initializer<span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span>tokenizer<span class="token punctuation">,</span> model<span class="token punctuation">,</span> initializer<span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>name <span class="token operator">=</span> <span class="token string">"jacobi"</span>        self<span class="token punctuation">.</span>acronym <span class="token operator">=</span> <span class="token string">"j"</span>    <span class="token decorator annotation punctuation">@torch<span class="token punctuation">.</span>no_grad</span><span class="token punctuation">(</span><span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">decode</span><span class="token punctuation">(</span>        self<span class="token punctuation">,</span>        input_ids<span class="token punctuation">,</span>        attention_mask<span class="token punctuation">,</span>        target_len<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span>        gold_target<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span>        init_tensor<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span>        compute_ddg<span class="token punctuation">:</span> <span class="token builtin">bool</span> <span class="token operator">=</span> <span class="token boolean">False</span><span class="token punctuation">,</span>        logits_preprocessor<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span>        <span class="token operator">*</span>args<span class="token punctuation">,</span>        <span class="token operator">**</span>kwargs    <span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">if</span> init_tensor <span class="token keyword">is</span> <span class="token boolean">None</span><span class="token punctuation">:</span>            init_tensor <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span>                <span class="token punctuation">[</span>self<span class="token punctuation">.</span>pad_token_id<span class="token punctuation">]</span> <span class="token operator">*</span> input_ids<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span> <span class="token operator">*</span> max_length<span class="token punctuation">,</span>                device<span class="token operator">=</span>self<span class="token punctuation">.</span>device<span class="token punctuation">,</span>            <span class="token punctuation">)</span><span class="token punctuation">.</span>reshape<span class="token punctuation">(</span>input_ids<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">,</span> max_length<span class="token punctuation">)</span>        <span class="token keyword">elif</span> self<span class="token punctuation">.</span>is_mbart<span class="token punctuation">:</span> <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>    <span class="token keyword">def</span> <span class="token function">initialize</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> init_transl<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">if</span> self<span class="token punctuation">.</span>initializer <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>            init_tensor<span class="token punctuation">,</span> _ <span class="token operator">=</span> self<span class="token punctuation">.</span>initializer<span class="token punctuation">.</span>init_translation<span class="token punctuation">(</span>init_transl<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span>        <span class="token keyword">else</span><span class="token punctuation">:</span>            init_tensor <span class="token operator">=</span> <span class="token boolean">None</span>        <span class="token keyword">return</span> init_tensor    <span class="token keyword">def</span> <span class="token function">compute_decode_kwargs</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> input_ids<span class="token punctuation">,</span> attention_mask<span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span><span class="token punctuation">:</span>        gold_autoregressive <span class="token operator">=</span> self<span class="token punctuation">.</span>generate_gold_autoregressive<span class="token punctuation">(</span>input_ids<span class="token punctuation">,</span> attention_mask<span class="token punctuation">)</span>        init_tensor <span class="token operator">=</span> self<span class="token punctuation">.</span>initialize<span class="token punctuation">(</span>init_transl<span class="token operator">=</span>gold_autoregressive<span class="token punctuation">)</span>        logits_preprocessor <span class="token operator">=</span> self<span class="token punctuation">.</span>generate_logits_preprocessor<span class="token punctuation">(</span>input_ids<span class="token punctuation">)</span>        <span class="token keyword">return</span> <span class="token punctuation">&#123;</span>            <span class="token string">"init_tensor"</span><span class="token punctuation">:</span> init_tensor<span class="token punctuation">.</span>clone<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            <span class="token string">"target_len"</span><span class="token punctuation">:</span> gold_autoregressive<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span>            <span class="token string">"gold_target"</span><span class="token punctuation">:</span> gold_autoregressive<span class="token punctuation">,</span>            <span class="token string">"logits_preprocessor"</span><span class="token punctuation">:</span> logits_preprocessor        <span class="token punctuation">&#125;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;jacobi-decoding&quot;&gt;&lt;a class=&quot;markdownIt-Anchor&quot; href=&quot;#jacobi-decoding&quot;&gt;&lt;/a&gt; Jacobi Decoding&lt;/h1&gt;
&lt;p&gt;在神经机器翻译（NMT）中，Transformer 模型已成为主流</summary>
      
    
    
    
    
    <category term="LLM" scheme="https://weikangqi.github.io/tags/LLM/"/>
    
    <category term="投机解码" scheme="https://weikangqi.github.io/tags/%E6%8A%95%E6%9C%BA%E8%A7%A3%E7%A0%81/"/>
    
  </entry>
  
  <entry>
    <title>QLora</title>
    <link href="https://weikangqi.github.io/2025/07/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E9%87%8F%E5%8C%96/QLora/"/>
    <id>https://weikangqi.github.io/2025/07/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E9%87%8F%E5%8C%96/QLora/</id>
    <published>2025-07-02T15:30:16.000Z</published>
    <updated>2025-09-11T03:36:51.304Z</updated>
    
    <content type="html"><![CDATA[<h1 id="qlora"><a class="markdownIt-Anchor" href="#qlora"></a> <strong>QLora</strong></h1><p><a href="https://zhuanlan.zhihu.com/p/666234324">zhuanlan.zhihu.com/…</a></p><p>QLora属于非线性量化，使用NF4量化策略。基于分块的分位数量化的量化策略；其次是双重量化，包含对普通参数的一次量化和对量化常数的再一次量化，可以进一步减小缓存占用；具有k-比特的有损最小熵编码具有如下特性：<strong>当将输入数据进行量化时，每个可能的k-bit的整数值出现的频率是相等的（k-bit 如4bit，16个整型数字出现的频率一样，所有数字被均匀分配到这16个数字，这样具有最小损失）</strong>。如果我们粗暴的使用round操作去低精度的更近的值，我们可能造成大量的数据都被量化到同一个数上，这样特征之间的差异性在量化过程中就被丢失了。</p><h3 id="分位数量化"><a class="markdownIt-Anchor" href="#分位数量化"></a> <strong>分位数量化</strong></h3><p>首先将参数进行分布统计，以下是正态分布图</p><p>​            <img src="https://raw.githubusercontent.com/weikangqi/picgo/main/MTMxMDI2NjQyOTU5MTk3NjM_330702_ND9XoBvHEZaNxz_U_1754895845" alt="img" style="zoom: 33%;" /></p><p>​</p><p>x轴代表了参数范围，y轴代表了出现的概率，如果我们要量化成4bit，那么x轴应该分成16份。每一份的面积相等。下一步需要确定分为数值。预训练模型的参数往往是符合正态分布的，累积分布函数的反函数来简化分位数计算。</p><p>以下是累计分布函数，以4bit为例，讲y轴分成16份，求出x轴坐标。</p><p>​            <img src="https://raw.githubusercontent.com/weikangqi/picgo/main/MTMxMDI2NjQyOTU5MTk3NjM_358076_FNsq37BJqEx0ESBM_1754898669" alt="img" style="zoom: 33%;" /></p><p>​</p><p>i是指bin的边界（0 - 1）有17，16个区间，使用16个点来代表。每个点的值是两个边界的平均值。于是可以得到qi。0和1的CDF的反函数的解分别是负无穷和正无穷，因此我们不能将0和1代入式(3)。为了解决这个问题，我们可以设置一个偏移（offset）位。使用偏移位后我们计算的区间不再是[0,1]，而是[1-offerset,offset]</p><p>量化步骤</p><ol><li>对参数进行归一化，x/x_max</li><li>对参数找到最接近的qi</li><li>将i 整型代表浮点数，存储到张量中</li></ol><h3 id="分块"><a class="markdownIt-Anchor" href="#分块"></a> 分块</h3><p>分块k位量化的策略是通过将张量分成若干个块，让每个块都有独立的量化常数c，从而解决模型参数的极大极小的异常值的问题。分块量化的另外一个好处是减少了核之间的通信，可以实现更好的并行性，并充分利用硬件的多核的能力。</p><h3 id="存储"><a class="markdownIt-Anchor" href="#存储"></a> 存储</h3><p>需要存储一个归一化参数C  =x_max，以及一张整型到浮点数的映射表。</p><h3 id="双重量化"><a class="markdownIt-Anchor" href="#双重量化"></a> 双重量化</h3><p>双重量化就是除了上面的量化再量化一次，归一化参数C，可以进行8bit量化减小存储。</p><p>累计分布函数CDF：x轴如下从小到大表示数据分布，y轴代表了累计分布的概率</p><p>​            <img src="https://raw.githubusercontent.com/weikangqi/picgo/main/MTMxMDI2NjQyOTU5MTk3NjM_220726_46YxVCtYclWW7bHO_1754896591" alt="img" style="zoom: 23%;" /></p><p>​</p><p>逆累计分布函数ICDF</p><p>所以“逆累积分布函数”的意思其实是“反累积分布函数”</p><p><strong>累积分布：分位点－&gt;概率，</strong></p><p><strong>逆累积分布：概率－&gt;分位点。</strong></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;qlora&quot;&gt;&lt;a class=&quot;markdownIt-Anchor&quot; href=&quot;#qlora&quot;&gt;&lt;/a&gt; &lt;strong&gt;QLora&lt;/strong&gt;&lt;/h1&gt;
&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/66623432</summary>
      
    
    
    
    
    <category term="LLM" scheme="https://weikangqi.github.io/tags/LLM/"/>
    
    <category term="量化" scheme="https://weikangqi.github.io/tags/%E9%87%8F%E5%8C%96/"/>
    
  </entry>
  
  <entry>
    <title>LLama3 Python</title>
    <link href="https://weikangqi.github.io/2025/05/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%9F%BA%E7%A1%80/LLama3/"/>
    <id>https://weikangqi.github.io/2025/05/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%9F%BA%E7%A1%80/LLama3/</id>
    <published>2025-05-07T15:30:16.000Z</published>
    <updated>2025-09-11T03:36:51.303Z</updated>
    
    <content type="html"><![CDATA[<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token comment"># Copyright (c) Meta Platforms, Inc. and affiliates.</span><span class="token comment"># This software may be used and distributed in accordance with the terms of the Llama 3 Community License Agreement.</span><span class="token keyword">import</span> math<span class="token keyword">from</span> dataclasses <span class="token keyword">import</span> dataclass<span class="token keyword">from</span> typing <span class="token keyword">import</span> Optional<span class="token punctuation">,</span> Tuple<span class="token keyword">import</span> fairscale<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>model_parallel<span class="token punctuation">.</span>initialize <span class="token keyword">as</span> fs_init<span class="token keyword">import</span> torch<span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>functional <span class="token keyword">as</span> F<span class="token keyword">from</span> fairscale<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>model_parallel<span class="token punctuation">.</span>layers <span class="token keyword">import</span> <span class="token punctuation">(</span>    ColumnParallelLinear<span class="token punctuation">,</span>    RowParallelLinear<span class="token punctuation">,</span>    VocabParallelEmbedding<span class="token punctuation">,</span><span class="token punctuation">)</span><span class="token keyword">from</span> torch <span class="token keyword">import</span> nn<span class="token decorator annotation punctuation">@dataclass</span><span class="token keyword">class</span> <span class="token class-name">ModelArgs</span><span class="token punctuation">:</span>    dim<span class="token punctuation">:</span> <span class="token builtin">int</span> <span class="token operator">=</span> <span class="token number">4096</span>    n_layers<span class="token punctuation">:</span> <span class="token builtin">int</span> <span class="token operator">=</span> <span class="token number">32</span>    n_heads<span class="token punctuation">:</span> <span class="token builtin">int</span> <span class="token operator">=</span> <span class="token number">32</span>    n_kv_heads<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span><span class="token builtin">int</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span>    vocab_size<span class="token punctuation">:</span> <span class="token builtin">int</span> <span class="token operator">=</span> <span class="token operator">-</span><span class="token number">1</span>    multiple_of<span class="token punctuation">:</span> <span class="token builtin">int</span> <span class="token operator">=</span> <span class="token number">256</span>  <span class="token comment"># make SwiGLU hidden layer size multiple of large power of 2</span>    ffn_dim_multiplier<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span><span class="token builtin">float</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span>    norm_eps<span class="token punctuation">:</span> <span class="token builtin">float</span> <span class="token operator">=</span> <span class="token number">1e-5</span>    rope_theta<span class="token punctuation">:</span> <span class="token builtin">float</span> <span class="token operator">=</span> <span class="token number">500000</span>    max_batch_size<span class="token punctuation">:</span> <span class="token builtin">int</span> <span class="token operator">=</span> <span class="token number">32</span>    max_seq_len<span class="token punctuation">:</span> <span class="token builtin">int</span> <span class="token operator">=</span> <span class="token number">2048</span><span class="token keyword">class</span> <span class="token class-name">RMSNorm</span><span class="token punctuation">(</span>torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> dim<span class="token punctuation">:</span> <span class="token builtin">int</span><span class="token punctuation">,</span> eps<span class="token punctuation">:</span> <span class="token builtin">float</span> <span class="token operator">=</span> <span class="token number">1e-6</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>eps <span class="token operator">=</span> eps        self<span class="token punctuation">.</span>weight <span class="token operator">=</span> nn<span class="token punctuation">.</span>Parameter<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>ones<span class="token punctuation">(</span>dim<span class="token punctuation">)</span><span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">_norm</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">return</span> x <span class="token operator">*</span> torch<span class="token punctuation">.</span>rsqrt<span class="token punctuation">(</span>x<span class="token punctuation">.</span><span class="token builtin">pow</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">.</span>mean<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> keepdim<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span> <span class="token operator">+</span> self<span class="token punctuation">.</span>eps<span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>        output <span class="token operator">=</span> self<span class="token punctuation">.</span>_norm<span class="token punctuation">(</span>x<span class="token punctuation">.</span><span class="token builtin">float</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span>type_as<span class="token punctuation">(</span>x<span class="token punctuation">)</span>        <span class="token keyword">return</span> output <span class="token operator">*</span> self<span class="token punctuation">.</span>weight<span class="token keyword">def</span> <span class="token function">precompute_freqs_cis</span><span class="token punctuation">(</span>dim<span class="token punctuation">:</span> <span class="token builtin">int</span><span class="token punctuation">,</span> end<span class="token punctuation">:</span> <span class="token builtin">int</span><span class="token punctuation">,</span> theta<span class="token punctuation">:</span> <span class="token builtin">float</span> <span class="token operator">=</span> <span class="token number">10000.0</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    freqs <span class="token operator">=</span> <span class="token number">1.0</span> <span class="token operator">/</span> <span class="token punctuation">(</span>theta <span class="token operator">**</span> <span class="token punctuation">(</span>torch<span class="token punctuation">.</span>arange<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> dim<span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token punctuation">:</span> <span class="token punctuation">(</span>dim <span class="token operator">//</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">.</span><span class="token builtin">float</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">/</span> dim<span class="token punctuation">)</span><span class="token punctuation">)</span>    t <span class="token operator">=</span> torch<span class="token punctuation">.</span>arange<span class="token punctuation">(</span>end<span class="token punctuation">,</span> device<span class="token operator">=</span>freqs<span class="token punctuation">.</span>device<span class="token punctuation">,</span> dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span>float32<span class="token punctuation">)</span>    freqs <span class="token operator">=</span> torch<span class="token punctuation">.</span>outer<span class="token punctuation">(</span>t<span class="token punctuation">,</span> freqs<span class="token punctuation">)</span>    freqs_cis <span class="token operator">=</span> torch<span class="token punctuation">.</span>polar<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>ones_like<span class="token punctuation">(</span>freqs<span class="token punctuation">)</span><span class="token punctuation">,</span> freqs<span class="token punctuation">)</span>  <span class="token comment"># complex64</span>    <span class="token keyword">return</span> freqs_cis<span class="token keyword">def</span> <span class="token function">reshape_for_broadcast</span><span class="token punctuation">(</span>freqs_cis<span class="token punctuation">:</span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">,</span> x<span class="token punctuation">:</span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">)</span><span class="token punctuation">:</span>    ndim <span class="token operator">=</span> x<span class="token punctuation">.</span>ndim    <span class="token keyword">assert</span> <span class="token number">0</span> <span class="token operator">&lt;=</span> <span class="token number">1</span> <span class="token operator">&lt;</span> ndim    <span class="token keyword">assert</span> freqs_cis<span class="token punctuation">.</span>shape <span class="token operator">==</span> <span class="token punctuation">(</span>x<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> x<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span>    shape <span class="token operator">=</span> <span class="token punctuation">[</span>d <span class="token keyword">if</span> i <span class="token operator">==</span> <span class="token number">1</span> <span class="token keyword">or</span> i <span class="token operator">==</span> ndim <span class="token operator">-</span> <span class="token number">1</span> <span class="token keyword">else</span> <span class="token number">1</span> <span class="token keyword">for</span> i<span class="token punctuation">,</span> d <span class="token keyword">in</span> <span class="token builtin">enumerate</span><span class="token punctuation">(</span>x<span class="token punctuation">.</span>shape<span class="token punctuation">)</span><span class="token punctuation">]</span>    <span class="token keyword">return</span> freqs_cis<span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token operator">*</span>shape<span class="token punctuation">)</span><span class="token keyword">def</span> <span class="token function">apply_rotary_emb</span><span class="token punctuation">(</span>    xq<span class="token punctuation">:</span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">,</span>    xk<span class="token punctuation">:</span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">,</span>    freqs_cis<span class="token punctuation">:</span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">,</span><span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> Tuple<span class="token punctuation">[</span>torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">,</span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">]</span><span class="token punctuation">:</span>    xq_ <span class="token operator">=</span> torch<span class="token punctuation">.</span>view_as_complex<span class="token punctuation">(</span>xq<span class="token punctuation">.</span><span class="token builtin">float</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>reshape<span class="token punctuation">(</span><span class="token operator">*</span>xq<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span>    xk_ <span class="token operator">=</span> torch<span class="token punctuation">.</span>view_as_complex<span class="token punctuation">(</span>xk<span class="token punctuation">.</span><span class="token builtin">float</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>reshape<span class="token punctuation">(</span><span class="token operator">*</span>xk<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span>    freqs_cis <span class="token operator">=</span> reshape_for_broadcast<span class="token punctuation">(</span>freqs_cis<span class="token punctuation">,</span> xq_<span class="token punctuation">)</span>    xq_out <span class="token operator">=</span> torch<span class="token punctuation">.</span>view_as_real<span class="token punctuation">(</span>xq_ <span class="token operator">*</span> freqs_cis<span class="token punctuation">)</span><span class="token punctuation">.</span>flatten<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">)</span>    xk_out <span class="token operator">=</span> torch<span class="token punctuation">.</span>view_as_real<span class="token punctuation">(</span>xk_ <span class="token operator">*</span> freqs_cis<span class="token punctuation">)</span><span class="token punctuation">.</span>flatten<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">)</span>    <span class="token keyword">return</span> xq_out<span class="token punctuation">.</span>type_as<span class="token punctuation">(</span>xq<span class="token punctuation">)</span><span class="token punctuation">,</span> xk_out<span class="token punctuation">.</span>type_as<span class="token punctuation">(</span>xk<span class="token punctuation">)</span><span class="token keyword">def</span> <span class="token function">repeat_kv</span><span class="token punctuation">(</span>x<span class="token punctuation">:</span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">,</span> n_rep<span class="token punctuation">:</span> <span class="token builtin">int</span><span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">:</span>    <span class="token triple-quoted-string string">"""torch.repeat_interleave(x, dim=2, repeats=n_rep)"""</span>    bs<span class="token punctuation">,</span> slen<span class="token punctuation">,</span> n_kv_heads<span class="token punctuation">,</span> head_dim <span class="token operator">=</span> x<span class="token punctuation">.</span>shape    <span class="token keyword">if</span> n_rep <span class="token operator">==</span> <span class="token number">1</span><span class="token punctuation">:</span>        <span class="token keyword">return</span> x    <span class="token keyword">return</span> <span class="token punctuation">(</span>        x<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token boolean">None</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span>        <span class="token punctuation">.</span>expand<span class="token punctuation">(</span>bs<span class="token punctuation">,</span> slen<span class="token punctuation">,</span> n_kv_heads<span class="token punctuation">,</span> n_rep<span class="token punctuation">,</span> head_dim<span class="token punctuation">)</span>        <span class="token punctuation">.</span>reshape<span class="token punctuation">(</span>bs<span class="token punctuation">,</span> slen<span class="token punctuation">,</span> n_kv_heads <span class="token operator">*</span> n_rep<span class="token punctuation">,</span> head_dim<span class="token punctuation">)</span>    <span class="token punctuation">)</span><span class="token keyword">class</span> <span class="token class-name">Attention</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> args<span class="token punctuation">:</span> ModelArgs<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>n_kv_heads <span class="token operator">=</span> args<span class="token punctuation">.</span>n_heads <span class="token keyword">if</span> args<span class="token punctuation">.</span>n_kv_heads <span class="token keyword">is</span> <span class="token boolean">None</span> <span class="token keyword">else</span> args<span class="token punctuation">.</span>n_kv_heads        model_parallel_size <span class="token operator">=</span> fs_init<span class="token punctuation">.</span>get_model_parallel_world_size<span class="token punctuation">(</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>n_local_heads <span class="token operator">=</span> args<span class="token punctuation">.</span>n_heads <span class="token operator">//</span> model_parallel_size        self<span class="token punctuation">.</span>n_local_kv_heads <span class="token operator">=</span> self<span class="token punctuation">.</span>n_kv_heads <span class="token operator">//</span> model_parallel_size        self<span class="token punctuation">.</span>n_rep <span class="token operator">=</span> self<span class="token punctuation">.</span>n_local_heads <span class="token operator">//</span> self<span class="token punctuation">.</span>n_local_kv_heads        self<span class="token punctuation">.</span>head_dim <span class="token operator">=</span> args<span class="token punctuation">.</span>dim <span class="token operator">//</span> args<span class="token punctuation">.</span>n_heads        self<span class="token punctuation">.</span>wq <span class="token operator">=</span> ColumnParallelLinear<span class="token punctuation">(</span>            args<span class="token punctuation">.</span>dim<span class="token punctuation">,</span>            args<span class="token punctuation">.</span>n_heads <span class="token operator">*</span> self<span class="token punctuation">.</span>head_dim<span class="token punctuation">,</span>            bias<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span>            gather_output<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span>            init_method<span class="token operator">=</span><span class="token keyword">lambda</span> x<span class="token punctuation">:</span> x<span class="token punctuation">,</span>        <span class="token punctuation">)</span>        self<span class="token punctuation">.</span>wk <span class="token operator">=</span> ColumnParallelLinear<span class="token punctuation">(</span>            args<span class="token punctuation">.</span>dim<span class="token punctuation">,</span>            self<span class="token punctuation">.</span>n_kv_heads <span class="token operator">*</span> self<span class="token punctuation">.</span>head_dim<span class="token punctuation">,</span>            bias<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span>            gather_output<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span>            init_method<span class="token operator">=</span><span class="token keyword">lambda</span> x<span class="token punctuation">:</span> x<span class="token punctuation">,</span>        <span class="token punctuation">)</span>        self<span class="token punctuation">.</span>wv <span class="token operator">=</span> ColumnParallelLinear<span class="token punctuation">(</span>            args<span class="token punctuation">.</span>dim<span class="token punctuation">,</span>            self<span class="token punctuation">.</span>n_kv_heads <span class="token operator">*</span> self<span class="token punctuation">.</span>head_dim<span class="token punctuation">,</span>            bias<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span>            gather_output<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span>            init_method<span class="token operator">=</span><span class="token keyword">lambda</span> x<span class="token punctuation">:</span> x<span class="token punctuation">,</span>        <span class="token punctuation">)</span>        self<span class="token punctuation">.</span>wo <span class="token operator">=</span> RowParallelLinear<span class="token punctuation">(</span>            args<span class="token punctuation">.</span>n_heads <span class="token operator">*</span> self<span class="token punctuation">.</span>head_dim<span class="token punctuation">,</span>            args<span class="token punctuation">.</span>dim<span class="token punctuation">,</span>            bias<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span>            input_is_parallel<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>            init_method<span class="token operator">=</span><span class="token keyword">lambda</span> x<span class="token punctuation">:</span> x<span class="token punctuation">,</span>        <span class="token punctuation">)</span>        self<span class="token punctuation">.</span>cache_k <span class="token operator">=</span> torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span>            <span class="token punctuation">(</span>                args<span class="token punctuation">.</span>max_batch_size<span class="token punctuation">,</span>                args<span class="token punctuation">.</span>max_seq_len<span class="token punctuation">,</span>                self<span class="token punctuation">.</span>n_local_kv_heads<span class="token punctuation">,</span>                self<span class="token punctuation">.</span>head_dim<span class="token punctuation">,</span>            <span class="token punctuation">)</span>        <span class="token punctuation">)</span><span class="token punctuation">.</span>cuda<span class="token punctuation">(</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>cache_v <span class="token operator">=</span> torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span>            <span class="token punctuation">(</span>                args<span class="token punctuation">.</span>max_batch_size<span class="token punctuation">,</span>                args<span class="token punctuation">.</span>max_seq_len<span class="token punctuation">,</span>                self<span class="token punctuation">.</span>n_local_kv_heads<span class="token punctuation">,</span>                self<span class="token punctuation">.</span>head_dim<span class="token punctuation">,</span>            <span class="token punctuation">)</span>        <span class="token punctuation">)</span><span class="token punctuation">.</span>cuda<span class="token punctuation">(</span><span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>        self<span class="token punctuation">,</span>        x<span class="token punctuation">:</span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">,</span>        start_pos<span class="token punctuation">:</span> <span class="token builtin">int</span><span class="token punctuation">,</span>        freqs_cis<span class="token punctuation">:</span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">,</span>        mask<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">]</span><span class="token punctuation">,</span>    <span class="token punctuation">)</span><span class="token punctuation">:</span>        bsz<span class="token punctuation">,</span> seqlen<span class="token punctuation">,</span> _ <span class="token operator">=</span> x<span class="token punctuation">.</span>shape        xq<span class="token punctuation">,</span> xk<span class="token punctuation">,</span> xv <span class="token operator">=</span> self<span class="token punctuation">.</span>wq<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>wk<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>wv<span class="token punctuation">(</span>x<span class="token punctuation">)</span>        xq <span class="token operator">=</span> xq<span class="token punctuation">.</span>view<span class="token punctuation">(</span>bsz<span class="token punctuation">,</span> seqlen<span class="token punctuation">,</span> self<span class="token punctuation">.</span>n_local_heads<span class="token punctuation">,</span> self<span class="token punctuation">.</span>head_dim<span class="token punctuation">)</span>        xk <span class="token operator">=</span> xk<span class="token punctuation">.</span>view<span class="token punctuation">(</span>bsz<span class="token punctuation">,</span> seqlen<span class="token punctuation">,</span> self<span class="token punctuation">.</span>n_local_kv_heads<span class="token punctuation">,</span> self<span class="token punctuation">.</span>head_dim<span class="token punctuation">)</span>        xv <span class="token operator">=</span> xv<span class="token punctuation">.</span>view<span class="token punctuation">(</span>bsz<span class="token punctuation">,</span> seqlen<span class="token punctuation">,</span> self<span class="token punctuation">.</span>n_local_kv_heads<span class="token punctuation">,</span> self<span class="token punctuation">.</span>head_dim<span class="token punctuation">)</span>        xq<span class="token punctuation">,</span> xk <span class="token operator">=</span> apply_rotary_emb<span class="token punctuation">(</span>xq<span class="token punctuation">,</span> xk<span class="token punctuation">,</span> freqs_cis<span class="token operator">=</span>freqs_cis<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>cache_k <span class="token operator">=</span> self<span class="token punctuation">.</span>cache_k<span class="token punctuation">.</span>to<span class="token punctuation">(</span>xq<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>cache_v <span class="token operator">=</span> self<span class="token punctuation">.</span>cache_v<span class="token punctuation">.</span>to<span class="token punctuation">(</span>xq<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>cache_k<span class="token punctuation">[</span><span class="token punctuation">:</span>bsz<span class="token punctuation">,</span> start_pos <span class="token punctuation">:</span> start_pos <span class="token operator">+</span> seqlen<span class="token punctuation">]</span> <span class="token operator">=</span> xk        self<span class="token punctuation">.</span>cache_v<span class="token punctuation">[</span><span class="token punctuation">:</span>bsz<span class="token punctuation">,</span> start_pos <span class="token punctuation">:</span> start_pos <span class="token operator">+</span> seqlen<span class="token punctuation">]</span> <span class="token operator">=</span> xv        keys <span class="token operator">=</span> self<span class="token punctuation">.</span>cache_k<span class="token punctuation">[</span><span class="token punctuation">:</span>bsz<span class="token punctuation">,</span> <span class="token punctuation">:</span> start_pos <span class="token operator">+</span> seqlen<span class="token punctuation">]</span>        values <span class="token operator">=</span> self<span class="token punctuation">.</span>cache_v<span class="token punctuation">[</span><span class="token punctuation">:</span>bsz<span class="token punctuation">,</span> <span class="token punctuation">:</span> start_pos <span class="token operator">+</span> seqlen<span class="token punctuation">]</span>        <span class="token comment"># repeat k/v heads if n_kv_heads &lt; n_heads</span>        keys <span class="token operator">=</span> repeat_kv<span class="token punctuation">(</span>            keys<span class="token punctuation">,</span> self<span class="token punctuation">.</span>n_rep        <span class="token punctuation">)</span>  <span class="token comment"># (bs, cache_len + seqlen, n_local_heads, head_dim)</span>        values <span class="token operator">=</span> repeat_kv<span class="token punctuation">(</span>            values<span class="token punctuation">,</span> self<span class="token punctuation">.</span>n_rep        <span class="token punctuation">)</span>  <span class="token comment"># (bs, cache_len + seqlen, n_local_heads, head_dim)</span>        xq <span class="token operator">=</span> xq<span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>  <span class="token comment"># (bs, n_local_heads, seqlen, head_dim)</span>        keys <span class="token operator">=</span> keys<span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>  <span class="token comment"># (bs, n_local_heads, cache_len + seqlen, head_dim)</span>        values <span class="token operator">=</span> values<span class="token punctuation">.</span>transpose<span class="token punctuation">(</span>            <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span>        <span class="token punctuation">)</span>  <span class="token comment"># (bs, n_local_heads, cache_len + seqlen, head_dim)</span>        scores <span class="token operator">=</span> torch<span class="token punctuation">.</span>matmul<span class="token punctuation">(</span>xq<span class="token punctuation">,</span> keys<span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token operator">/</span> math<span class="token punctuation">.</span>sqrt<span class="token punctuation">(</span>self<span class="token punctuation">.</span>head_dim<span class="token punctuation">)</span>        <span class="token keyword">if</span> mask <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>            scores <span class="token operator">=</span> scores <span class="token operator">+</span> mask  <span class="token comment"># (bs, n_local_heads, seqlen, cache_len + seqlen)</span>        scores <span class="token operator">=</span> F<span class="token punctuation">.</span>softmax<span class="token punctuation">(</span>scores<span class="token punctuation">.</span><span class="token builtin">float</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span>type_as<span class="token punctuation">(</span>xq<span class="token punctuation">)</span>        output <span class="token operator">=</span> torch<span class="token punctuation">.</span>matmul<span class="token punctuation">(</span>scores<span class="token punctuation">,</span> values<span class="token punctuation">)</span>  <span class="token comment"># (bs, n_local_heads, seqlen, head_dim)</span>        output <span class="token operator">=</span> output<span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">.</span>contiguous<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>view<span class="token punctuation">(</span>bsz<span class="token punctuation">,</span> seqlen<span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>        <span class="token keyword">return</span> self<span class="token punctuation">.</span>wo<span class="token punctuation">(</span>output<span class="token punctuation">)</span><span class="token keyword">class</span> <span class="token class-name">FeedForward</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>        self<span class="token punctuation">,</span>        dim<span class="token punctuation">:</span> <span class="token builtin">int</span><span class="token punctuation">,</span>        hidden_dim<span class="token punctuation">:</span> <span class="token builtin">int</span><span class="token punctuation">,</span>        multiple_of<span class="token punctuation">:</span> <span class="token builtin">int</span><span class="token punctuation">,</span>        ffn_dim_multiplier<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span><span class="token builtin">float</span><span class="token punctuation">]</span><span class="token punctuation">,</span>    <span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>        hidden_dim <span class="token operator">=</span> <span class="token builtin">int</span><span class="token punctuation">(</span><span class="token number">2</span> <span class="token operator">*</span> hidden_dim <span class="token operator">/</span> <span class="token number">3</span><span class="token punctuation">)</span>        <span class="token comment"># custom dim factor multiplier</span>        <span class="token keyword">if</span> ffn_dim_multiplier <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>            hidden_dim <span class="token operator">=</span> <span class="token builtin">int</span><span class="token punctuation">(</span>ffn_dim_multiplier <span class="token operator">*</span> hidden_dim<span class="token punctuation">)</span>        hidden_dim <span class="token operator">=</span> multiple_of <span class="token operator">*</span> <span class="token punctuation">(</span><span class="token punctuation">(</span>hidden_dim <span class="token operator">+</span> multiple_of <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">)</span> <span class="token operator">//</span> multiple_of<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>w1 <span class="token operator">=</span> ColumnParallelLinear<span class="token punctuation">(</span>            dim<span class="token punctuation">,</span> hidden_dim<span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> gather_output<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> init_method<span class="token operator">=</span><span class="token keyword">lambda</span> x<span class="token punctuation">:</span> x        <span class="token punctuation">)</span>        self<span class="token punctuation">.</span>w2 <span class="token operator">=</span> RowParallelLinear<span class="token punctuation">(</span>            hidden_dim<span class="token punctuation">,</span> dim<span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> input_is_parallel<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> init_method<span class="token operator">=</span><span class="token keyword">lambda</span> x<span class="token punctuation">:</span> x        <span class="token punctuation">)</span>        self<span class="token punctuation">.</span>w3 <span class="token operator">=</span> ColumnParallelLinear<span class="token punctuation">(</span>            dim<span class="token punctuation">,</span> hidden_dim<span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> gather_output<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> init_method<span class="token operator">=</span><span class="token keyword">lambda</span> x<span class="token punctuation">:</span> x        <span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">return</span> self<span class="token punctuation">.</span>w2<span class="token punctuation">(</span>F<span class="token punctuation">.</span>silu<span class="token punctuation">(</span>self<span class="token punctuation">.</span>w1<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token operator">*</span> self<span class="token punctuation">.</span>w3<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token keyword">class</span> <span class="token class-name">TransformerBlock</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> layer_id<span class="token punctuation">:</span> <span class="token builtin">int</span><span class="token punctuation">,</span> args<span class="token punctuation">:</span> ModelArgs<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>n_heads <span class="token operator">=</span> args<span class="token punctuation">.</span>n_heads        self<span class="token punctuation">.</span>dim <span class="token operator">=</span> args<span class="token punctuation">.</span>dim        self<span class="token punctuation">.</span>head_dim <span class="token operator">=</span> args<span class="token punctuation">.</span>dim <span class="token operator">//</span> args<span class="token punctuation">.</span>n_heads        self<span class="token punctuation">.</span>attention <span class="token operator">=</span> Attention<span class="token punctuation">(</span>args<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>feed_forward <span class="token operator">=</span> FeedForward<span class="token punctuation">(</span>            dim<span class="token operator">=</span>args<span class="token punctuation">.</span>dim<span class="token punctuation">,</span>            hidden_dim<span class="token operator">=</span><span class="token number">4</span> <span class="token operator">*</span> args<span class="token punctuation">.</span>dim<span class="token punctuation">,</span>            multiple_of<span class="token operator">=</span>args<span class="token punctuation">.</span>multiple_of<span class="token punctuation">,</span>            ffn_dim_multiplier<span class="token operator">=</span>args<span class="token punctuation">.</span>ffn_dim_multiplier<span class="token punctuation">,</span>        <span class="token punctuation">)</span>        self<span class="token punctuation">.</span>layer_id <span class="token operator">=</span> layer_id        self<span class="token punctuation">.</span>attention_norm <span class="token operator">=</span> RMSNorm<span class="token punctuation">(</span>args<span class="token punctuation">.</span>dim<span class="token punctuation">,</span> eps<span class="token operator">=</span>args<span class="token punctuation">.</span>norm_eps<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>ffn_norm <span class="token operator">=</span> RMSNorm<span class="token punctuation">(</span>args<span class="token punctuation">.</span>dim<span class="token punctuation">,</span> eps<span class="token operator">=</span>args<span class="token punctuation">.</span>norm_eps<span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>        self<span class="token punctuation">,</span>        x<span class="token punctuation">:</span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">,</span>        start_pos<span class="token punctuation">:</span> <span class="token builtin">int</span><span class="token punctuation">,</span>        freqs_cis<span class="token punctuation">:</span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">,</span>        mask<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">]</span><span class="token punctuation">,</span>    <span class="token punctuation">)</span><span class="token punctuation">:</span>        h <span class="token operator">=</span> x <span class="token operator">+</span> self<span class="token punctuation">.</span>attention<span class="token punctuation">(</span>self<span class="token punctuation">.</span>attention_norm<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">,</span> start_pos<span class="token punctuation">,</span> freqs_cis<span class="token punctuation">,</span> mask<span class="token punctuation">)</span>        out <span class="token operator">=</span> h <span class="token operator">+</span> self<span class="token punctuation">.</span>feed_forward<span class="token punctuation">(</span>self<span class="token punctuation">.</span>ffn_norm<span class="token punctuation">(</span>h<span class="token punctuation">)</span><span class="token punctuation">)</span>        <span class="token keyword">return</span> out<span class="token keyword">class</span> <span class="token class-name">Transformer</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> params<span class="token punctuation">:</span> ModelArgs<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>params <span class="token operator">=</span> params        self<span class="token punctuation">.</span>vocab_size <span class="token operator">=</span> params<span class="token punctuation">.</span>vocab_size        self<span class="token punctuation">.</span>n_layers <span class="token operator">=</span> params<span class="token punctuation">.</span>n_layers        self<span class="token punctuation">.</span>tok_embeddings <span class="token operator">=</span> VocabParallelEmbedding<span class="token punctuation">(</span>            params<span class="token punctuation">.</span>vocab_size<span class="token punctuation">,</span> params<span class="token punctuation">.</span>dim<span class="token punctuation">,</span> init_method<span class="token operator">=</span><span class="token keyword">lambda</span> x<span class="token punctuation">:</span> x        <span class="token punctuation">)</span>        self<span class="token punctuation">.</span>layers <span class="token operator">=</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>ModuleList<span class="token punctuation">(</span><span class="token punctuation">)</span>        <span class="token keyword">for</span> layer_id <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>params<span class="token punctuation">.</span>n_layers<span class="token punctuation">)</span><span class="token punctuation">:</span>            self<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>append<span class="token punctuation">(</span>TransformerBlock<span class="token punctuation">(</span>layer_id<span class="token punctuation">,</span> params<span class="token punctuation">)</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>norm <span class="token operator">=</span> RMSNorm<span class="token punctuation">(</span>params<span class="token punctuation">.</span>dim<span class="token punctuation">,</span> eps<span class="token operator">=</span>params<span class="token punctuation">.</span>norm_eps<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>output <span class="token operator">=</span> ColumnParallelLinear<span class="token punctuation">(</span>            params<span class="token punctuation">.</span>dim<span class="token punctuation">,</span> params<span class="token punctuation">.</span>vocab_size<span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> init_method<span class="token operator">=</span><span class="token keyword">lambda</span> x<span class="token punctuation">:</span> x        <span class="token punctuation">)</span>        self<span class="token punctuation">.</span>freqs_cis <span class="token operator">=</span> precompute_freqs_cis<span class="token punctuation">(</span>            params<span class="token punctuation">.</span>dim <span class="token operator">//</span> params<span class="token punctuation">.</span>n_heads<span class="token punctuation">,</span>            params<span class="token punctuation">.</span>max_seq_len <span class="token operator">*</span> <span class="token number">2</span><span class="token punctuation">,</span>            params<span class="token punctuation">.</span>rope_theta<span class="token punctuation">,</span>        <span class="token punctuation">)</span>    <span class="token decorator annotation punctuation">@torch<span class="token punctuation">.</span>inference_mode</span><span class="token punctuation">(</span><span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> tokens<span class="token punctuation">:</span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">,</span> start_pos<span class="token punctuation">:</span> <span class="token builtin">int</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        _bsz<span class="token punctuation">,</span> seqlen <span class="token operator">=</span> tokens<span class="token punctuation">.</span>shape        h <span class="token operator">=</span> self<span class="token punctuation">.</span>tok_embeddings<span class="token punctuation">(</span>tokens<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>freqs_cis <span class="token operator">=</span> self<span class="token punctuation">.</span>freqs_cis<span class="token punctuation">.</span>to<span class="token punctuation">(</span>h<span class="token punctuation">.</span>device<span class="token punctuation">)</span>        freqs_cis <span class="token operator">=</span> self<span class="token punctuation">.</span>freqs_cis<span class="token punctuation">[</span>start_pos <span class="token punctuation">:</span> start_pos <span class="token operator">+</span> seqlen<span class="token punctuation">]</span>        mask <span class="token operator">=</span> <span class="token boolean">None</span>        <span class="token keyword">if</span> seqlen <span class="token operator">></span> <span class="token number">1</span><span class="token punctuation">:</span>            mask <span class="token operator">=</span> torch<span class="token punctuation">.</span>full<span class="token punctuation">(</span><span class="token punctuation">(</span>seqlen<span class="token punctuation">,</span> seqlen<span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token builtin">float</span><span class="token punctuation">(</span><span class="token string">"-inf"</span><span class="token punctuation">)</span><span class="token punctuation">,</span> device<span class="token operator">=</span>tokens<span class="token punctuation">.</span>device<span class="token punctuation">)</span>            mask <span class="token operator">=</span> torch<span class="token punctuation">.</span>triu<span class="token punctuation">(</span>mask<span class="token punctuation">,</span> diagonal<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>            <span class="token comment"># When performing key-value caching, we compute the attention scores</span>            <span class="token comment"># only for the new sequence. Thus, the matrix of scores is of size</span>            <span class="token comment"># (seqlen, cache_len + seqlen), and the only masked entries are (i, j) for</span>            <span class="token comment"># j > cache_len + i, since row i corresponds to token cache_len + i.</span>            mask <span class="token operator">=</span> torch<span class="token punctuation">.</span>hstack<span class="token punctuation">(</span>                <span class="token punctuation">[</span>torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span><span class="token punctuation">(</span>seqlen<span class="token punctuation">,</span> start_pos<span class="token punctuation">)</span><span class="token punctuation">,</span> device<span class="token operator">=</span>tokens<span class="token punctuation">.</span>device<span class="token punctuation">)</span><span class="token punctuation">,</span> mask<span class="token punctuation">]</span>            <span class="token punctuation">)</span><span class="token punctuation">.</span>type_as<span class="token punctuation">(</span>h<span class="token punctuation">)</span>        <span class="token keyword">for</span> layer <span class="token keyword">in</span> self<span class="token punctuation">.</span>layers<span class="token punctuation">:</span>            h <span class="token operator">=</span> layer<span class="token punctuation">(</span>h<span class="token punctuation">,</span> start_pos<span class="token punctuation">,</span> freqs_cis<span class="token punctuation">,</span> mask<span class="token punctuation">)</span>        h <span class="token operator">=</span> self<span class="token punctuation">.</span>norm<span class="token punctuation">(</span>h<span class="token punctuation">)</span>        output <span class="token operator">=</span> self<span class="token punctuation">.</span>output<span class="token punctuation">(</span>h<span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">float</span><span class="token punctuation">(</span><span class="token punctuation">)</span>        <span class="token keyword">return</span> output<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>]]></content>
    
    
      
      
    <summary type="html">&lt;pre class=&quot;line-numbers language-python&quot; data-language=&quot;python&quot;&gt;&lt;code class=&quot;language-python&quot;&gt;&lt;span class=&quot;token comment&quot;&gt;# Copyright (c) M</summary>
      
    
    
    
    
    <category term="LLM" scheme="https://weikangqi.github.io/tags/LLM/"/>
    
  </entry>
  
  <entry>
    <title>Attention(MHA, MLA, GQA, MQA)</title>
    <link href="https://weikangqi.github.io/2025/02/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%9F%BA%E7%A1%80/attention%20%E6%9C%BA%E5%88%B6/"/>
    <id>https://weikangqi.github.io/2025/02/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%9F%BA%E7%A1%80/attention%20%E6%9C%BA%E5%88%B6/</id>
    <published>2025-02-07T15:30:16.000Z</published>
    <updated>2025-09-11T03:36:51.303Z</updated>
    
    <content type="html"><![CDATA[<h1 id="attention-mha-mla-gqamqa"><a class="markdownIt-Anchor" href="#attention-mha-mla-gqamqa"></a> Attention: MHA, MLA, GQA,MQA</h1><p><img src="https://github.com/haukzero/from-mha-to-mla/blob/master/img/mha_mqa_gqa.png?raw=true" alt="mha_mqa_gqa.png" /></p><h2 id="mha"><a class="markdownIt-Anchor" href="#mha"></a> MHA</h2><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> torch<span class="token keyword">from</span> torch <span class="token keyword">import</span> nn<span class="token keyword">from</span> typing <span class="token keyword">import</span> Optional<span class="token keyword">def</span> <span class="token function">apply_rope</span><span class="token punctuation">(</span>x<span class="token punctuation">:</span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">,</span> <span class="token operator">*</span>args<span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">return</span> x<span class="token keyword">def</span> <span class="token function">update_kv_cache</span><span class="token punctuation">(</span>key_states<span class="token punctuation">:</span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">,</span> value_states<span class="token punctuation">:</span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">return</span> key_states<span class="token punctuation">.</span>repeat<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> value_states<span class="token punctuation">.</span>repeat<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token keyword">class</span> <span class="token class-name">MultiHeadAttention</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>        self<span class="token punctuation">,</span>        hidden_size<span class="token punctuation">:</span> <span class="token builtin">int</span><span class="token punctuation">,</span>        num_heads<span class="token punctuation">:</span> <span class="token builtin">int</span><span class="token punctuation">,</span>        head_dim<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span><span class="token builtin">int</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span>        use_cache<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span><span class="token builtin">bool</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">False</span><span class="token punctuation">,</span>    <span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>hidden_size <span class="token operator">=</span> hidden_size        self<span class="token punctuation">.</span>num_heads <span class="token operator">=</span> num_heads        self<span class="token punctuation">.</span>head_dim <span class="token operator">=</span> head_dim <span class="token keyword">or</span> hidden_size <span class="token operator">//</span> num_heads        self<span class="token punctuation">.</span>scale <span class="token operator">=</span> self<span class="token punctuation">.</span>head_dim<span class="token operator">**</span><span class="token operator">-</span><span class="token number">0.5</span>        self<span class="token punctuation">.</span>use_cache <span class="token operator">=</span> use_cache        self<span class="token punctuation">.</span>q_proj <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>self<span class="token punctuation">.</span>hidden_size<span class="token punctuation">,</span> self<span class="token punctuation">.</span>num_heads <span class="token operator">*</span> self<span class="token punctuation">.</span>head_dim<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>k_proj <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>self<span class="token punctuation">.</span>hidden_size<span class="token punctuation">,</span> self<span class="token punctuation">.</span>num_heads <span class="token operator">*</span> self<span class="token punctuation">.</span>head_dim<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>v_proj <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>self<span class="token punctuation">.</span>hidden_size<span class="token punctuation">,</span> self<span class="token punctuation">.</span>num_heads <span class="token operator">*</span> self<span class="token punctuation">.</span>head_dim<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>o_proj <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>self<span class="token punctuation">.</span>num_heads <span class="token operator">*</span> self<span class="token punctuation">.</span>head_dim<span class="token punctuation">,</span> self<span class="token punctuation">.</span>hidden_size<span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>        self<span class="token punctuation">,</span>        hidden_states<span class="token punctuation">:</span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">,</span>        position_ids<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>torch<span class="token punctuation">.</span>LongTensor<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span>    <span class="token punctuation">)</span><span class="token punctuation">:</span>        bsz<span class="token punctuation">,</span> q_len<span class="token punctuation">,</span> _ <span class="token operator">=</span> hidden_states<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span>        <span class="token comment"># q: (bsz, num_heads, q_len, head_dim)</span>        query_states <span class="token operator">=</span> <span class="token punctuation">(</span>            self<span class="token punctuation">.</span>q_proj<span class="token punctuation">(</span>hidden_states<span class="token punctuation">)</span>            <span class="token punctuation">.</span>view<span class="token punctuation">(</span>bsz<span class="token punctuation">,</span> q_len<span class="token punctuation">,</span> self<span class="token punctuation">.</span>num_heads<span class="token punctuation">,</span> self<span class="token punctuation">.</span>head_dim<span class="token punctuation">)</span>            <span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>        <span class="token punctuation">)</span>        key_states <span class="token operator">=</span> <span class="token punctuation">(</span>            self<span class="token punctuation">.</span>k_proj<span class="token punctuation">(</span>hidden_states<span class="token punctuation">)</span>            <span class="token punctuation">.</span>view<span class="token punctuation">(</span>bsz<span class="token punctuation">,</span> q_len<span class="token punctuation">,</span> self<span class="token punctuation">.</span>num_heads<span class="token punctuation">,</span> self<span class="token punctuation">.</span>head_dim<span class="token punctuation">)</span>            <span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>        <span class="token punctuation">)</span>        value_states <span class="token operator">=</span> <span class="token punctuation">(</span>            self<span class="token punctuation">.</span>v_proj<span class="token punctuation">(</span>hidden_states<span class="token punctuation">)</span>            <span class="token punctuation">.</span>view<span class="token punctuation">(</span>bsz<span class="token punctuation">,</span> q_len<span class="token punctuation">,</span> self<span class="token punctuation">.</span>num_heads<span class="token punctuation">,</span> self<span class="token punctuation">.</span>head_dim<span class="token punctuation">)</span>            <span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>        <span class="token punctuation">)</span>        <span class="token comment"># RoPE</span>        query_states <span class="token operator">=</span> apply_rope<span class="token punctuation">(</span>query_states<span class="token punctuation">)</span>        key_states <span class="token operator">=</span> apply_rope<span class="token punctuation">(</span>key_states<span class="token punctuation">)</span>        <span class="token keyword">if</span> self<span class="token punctuation">.</span>use_cache<span class="token punctuation">:</span>            <span class="token comment"># Update KV Cache and get full kv</span>            <span class="token comment"># k/v: (bsz, num_heads, kv_len, head_dim)</span>            key_states<span class="token punctuation">,</span> value_states <span class="token operator">=</span> update_kv_cache<span class="token punctuation">(</span>key_states<span class="token punctuation">,</span> value_states<span class="token punctuation">)</span>        kv_len <span class="token operator">=</span> key_states<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span>        <span class="token comment"># Softmax(Q @ K^T / sqrt(d_k))</span>        <span class="token comment"># attn: (bsz, num_heads, q_len, kv_len)</span>        attn_weights <span class="token operator">=</span> <span class="token punctuation">(</span>            torch<span class="token punctuation">.</span>einsum<span class="token punctuation">(</span><span class="token string">"bhld, bhnd -> bhln"</span><span class="token punctuation">,</span> query_states<span class="token punctuation">,</span> key_states<span class="token punctuation">)</span> <span class="token operator">*</span> self<span class="token punctuation">.</span>scale        <span class="token punctuation">)</span>        attn_weights <span class="token operator">=</span> attn_weights<span class="token punctuation">.</span>softmax<span class="token punctuation">(</span>dim<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>        <span class="token comment"># A @ V</span>        <span class="token comment"># attn: (bsz, num_heads, q_len, head_dim)</span>        attn_weights <span class="token operator">=</span> torch<span class="token punctuation">.</span>einsum<span class="token punctuation">(</span><span class="token string">"bhln, bhnd -> bhld"</span><span class="token punctuation">,</span> attn_weights<span class="token punctuation">,</span> value_states<span class="token punctuation">)</span>        <span class="token comment"># output: (bsz, q_len, hidden_size)</span>        attn_output <span class="token operator">=</span> self<span class="token punctuation">.</span>o_proj<span class="token punctuation">(</span>            attn_weights<span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>            <span class="token punctuation">.</span>contiguous<span class="token punctuation">(</span><span class="token punctuation">)</span>            <span class="token punctuation">.</span>view<span class="token punctuation">(</span>bsz<span class="token punctuation">,</span> q_len<span class="token punctuation">,</span> self<span class="token punctuation">.</span>num_heads <span class="token operator">*</span> self<span class="token punctuation">.</span>head_dim<span class="token punctuation">)</span>        <span class="token punctuation">)</span>        <span class="token keyword">return</span> attn_output<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="gqa-grouped-query-attention"><a class="markdownIt-Anchor" href="#gqa-grouped-query-attention"></a> GQA (Grouped-query Attention)</h2><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> torch<span class="token keyword">from</span> torch <span class="token keyword">import</span> nn<span class="token keyword">from</span> typing <span class="token keyword">import</span> Optional<span class="token keyword">def</span> <span class="token function">apply_rope</span><span class="token punctuation">(</span>x<span class="token punctuation">:</span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">,</span> <span class="token operator">*</span>args<span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">return</span> x<span class="token keyword">def</span> <span class="token function">update_kv_cache</span><span class="token punctuation">(</span>key_states<span class="token punctuation">:</span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">,</span> value_states<span class="token punctuation">:</span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">return</span> key_states<span class="token punctuation">.</span>repeat<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> value_states<span class="token punctuation">.</span>repeat<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token keyword">def</span> <span class="token function">repeat_kv</span><span class="token punctuation">(</span>hidden_states<span class="token punctuation">:</span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">,</span> n_rep<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span><span class="token builtin">int</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    batch<span class="token punctuation">,</span> num_key_value_heads<span class="token punctuation">,</span> slen<span class="token punctuation">,</span> head_dim <span class="token operator">=</span> hidden_states<span class="token punctuation">.</span>shape    <span class="token keyword">if</span> n_rep <span class="token operator">==</span> <span class="token number">1</span><span class="token punctuation">:</span>        <span class="token keyword">return</span> hidden_states    hidden_states <span class="token operator">=</span> hidden_states<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token boolean">None</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">.</span>expand<span class="token punctuation">(</span>        batch<span class="token punctuation">,</span> num_key_value_heads<span class="token punctuation">,</span> n_rep<span class="token punctuation">,</span> slen<span class="token punctuation">,</span> head_dim    <span class="token punctuation">)</span>    <span class="token keyword">return</span> hidden_states<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span>batch<span class="token punctuation">,</span> num_key_value_heads <span class="token operator">*</span> n_rep<span class="token punctuation">,</span> slen<span class="token punctuation">,</span> head_dim<span class="token punctuation">)</span><span class="token keyword">class</span> <span class="token class-name">GroupQueryAttention</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>        self<span class="token punctuation">,</span>        hidden_size<span class="token punctuation">:</span> <span class="token builtin">int</span><span class="token punctuation">,</span>        num_heads<span class="token punctuation">:</span> <span class="token builtin">int</span><span class="token punctuation">,</span>        num_kv_heads<span class="token punctuation">:</span> <span class="token builtin">int</span><span class="token punctuation">,</span>        head_dim<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span><span class="token builtin">int</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span>        use_cache<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span><span class="token builtin">bool</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">False</span><span class="token punctuation">,</span>    <span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>hidden_size <span class="token operator">=</span> hidden_size        self<span class="token punctuation">.</span>num_heads <span class="token operator">=</span> num_heads        self<span class="token punctuation">.</span>num_kv_heads <span class="token operator">=</span> num_kv_heads        self<span class="token punctuation">.</span>num_groups <span class="token operator">=</span> num_heads <span class="token operator">//</span> num_kv_heads        self<span class="token punctuation">.</span>head_dim <span class="token operator">=</span> head_dim <span class="token keyword">or</span> hidden_size <span class="token operator">//</span> num_heads        self<span class="token punctuation">.</span>scale <span class="token operator">=</span> self<span class="token punctuation">.</span>head_dim<span class="token operator">**</span><span class="token operator">-</span><span class="token number">0.5</span>        self<span class="token punctuation">.</span>use_cache <span class="token operator">=</span> use_cache        self<span class="token punctuation">.</span>q_proj <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>self<span class="token punctuation">.</span>hidden_size<span class="token punctuation">,</span> self<span class="token punctuation">.</span>num_heads <span class="token operator">*</span> self<span class="token punctuation">.</span>head_dim<span class="token punctuation">)</span>        <span class="token comment"># different from the MHA/MQA</span>        self<span class="token punctuation">.</span>k_proj <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>self<span class="token punctuation">.</span>hidden_size<span class="token punctuation">,</span> self<span class="token punctuation">.</span>num_kv_heads <span class="token operator">*</span> self<span class="token punctuation">.</span>head_dim<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>v_proj <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>self<span class="token punctuation">.</span>hidden_size<span class="token punctuation">,</span> self<span class="token punctuation">.</span>num_kv_heads <span class="token operator">*</span> self<span class="token punctuation">.</span>head_dim<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>o_proj <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>self<span class="token punctuation">.</span>num_heads <span class="token operator">*</span> self<span class="token punctuation">.</span>head_dim<span class="token punctuation">,</span> self<span class="token punctuation">.</span>hidden_size<span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>        self<span class="token punctuation">,</span>        hidden_states<span class="token punctuation">:</span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">,</span>        position_ids<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>torch<span class="token punctuation">.</span>LongTensor<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span>    <span class="token punctuation">)</span><span class="token punctuation">:</span>        bsz<span class="token punctuation">,</span> q_len<span class="token punctuation">,</span> _ <span class="token operator">=</span> hidden_states<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span>        <span class="token comment"># q: (bsz, num_heads, q_len, head_dim)</span>        query_states <span class="token operator">=</span> <span class="token punctuation">(</span>            self<span class="token punctuation">.</span>q_proj<span class="token punctuation">(</span>hidden_states<span class="token punctuation">)</span>            <span class="token punctuation">.</span>view<span class="token punctuation">(</span>bsz<span class="token punctuation">,</span> q_len<span class="token punctuation">,</span> self<span class="token punctuation">.</span>num_heads<span class="token punctuation">,</span> self<span class="token punctuation">.</span>head_dim<span class="token punctuation">)</span>            <span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>        <span class="token punctuation">)</span>        <span class="token comment"># k/v: (bsz, num_kv_heads, q_len, head_dim)</span>        key_states <span class="token operator">=</span> <span class="token punctuation">(</span>            self<span class="token punctuation">.</span>k_proj<span class="token punctuation">(</span>hidden_states<span class="token punctuation">)</span>            <span class="token punctuation">.</span>view<span class="token punctuation">(</span>bsz<span class="token punctuation">,</span> q_len<span class="token punctuation">,</span> self<span class="token punctuation">.</span>num_kv_heads<span class="token punctuation">,</span> self<span class="token punctuation">.</span>head_dim<span class="token punctuation">)</span>            <span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>        <span class="token punctuation">)</span>        value_states <span class="token operator">=</span> <span class="token punctuation">(</span>            self<span class="token punctuation">.</span>v_proj<span class="token punctuation">(</span>hidden_states<span class="token punctuation">)</span>            <span class="token punctuation">.</span>view<span class="token punctuation">(</span>bsz<span class="token punctuation">,</span> q_len<span class="token punctuation">,</span> self<span class="token punctuation">.</span>num_kv_heads<span class="token punctuation">,</span> self<span class="token punctuation">.</span>head_dim<span class="token punctuation">)</span>            <span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>        <span class="token punctuation">)</span>        <span class="token comment"># RoPE</span>        query_states <span class="token operator">=</span> apply_rope<span class="token punctuation">(</span>query_states<span class="token punctuation">)</span>        key_states <span class="token operator">=</span> apply_rope<span class="token punctuation">(</span>key_states<span class="token punctuation">)</span>        <span class="token keyword">if</span> self<span class="token punctuation">.</span>use_cache<span class="token punctuation">:</span>            <span class="token comment"># Update KV Cache and get full kv</span>            <span class="token comment"># k/v: (bsz, num_heads, kv_len, head_dim)</span>            key_states<span class="token punctuation">,</span> value_states <span class="token operator">=</span> update_kv_cache<span class="token punctuation">(</span>key_states<span class="token punctuation">,</span> value_states<span class="token punctuation">)</span>        kv_len <span class="token operator">=</span> key_states<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span>        <span class="token comment"># Repeat kv</span>        <span class="token comment"># k/v: (bsz, num_heads, q_len, head_dim)</span>        key_states <span class="token operator">=</span> repeat_kv<span class="token punctuation">(</span>key_states<span class="token punctuation">,</span> self<span class="token punctuation">.</span>num_groups<span class="token punctuation">)</span>        value_states <span class="token operator">=</span> repeat_kv<span class="token punctuation">(</span>value_states<span class="token punctuation">,</span> self<span class="token punctuation">.</span>num_groups<span class="token punctuation">)</span>        <span class="token comment"># Softmax(Q @ K^T / sqrt(d_k))</span>        <span class="token comment"># attn: (bsz, num_heads, q_len, kv_len)</span>        attn_weights <span class="token operator">=</span> <span class="token punctuation">(</span>            torch<span class="token punctuation">.</span>einsum<span class="token punctuation">(</span><span class="token string">"bhld, bhnd -> bhln"</span><span class="token punctuation">,</span> query_states<span class="token punctuation">,</span> key_states<span class="token punctuation">)</span> <span class="token operator">*</span> self<span class="token punctuation">.</span>scale        <span class="token punctuation">)</span>        attn_weights <span class="token operator">=</span> attn_weights<span class="token punctuation">.</span>softmax<span class="token punctuation">(</span>dim<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>        <span class="token comment"># A @ V</span>        <span class="token comment"># attn: (bsz, num_heads, q_len, head_dim)</span>        attn_weights <span class="token operator">=</span> torch<span class="token punctuation">.</span>einsum<span class="token punctuation">(</span><span class="token string">"bhln, bhnd -> bhld"</span><span class="token punctuation">,</span> attn_weights<span class="token punctuation">,</span> value_states<span class="token punctuation">)</span>        <span class="token comment"># output: (bsz, q_len, hidden_size)</span>        attn_output <span class="token operator">=</span> self<span class="token punctuation">.</span>o_proj<span class="token punctuation">(</span>            attn_weights<span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>            <span class="token punctuation">.</span>contiguous<span class="token punctuation">(</span><span class="token punctuation">)</span>            <span class="token punctuation">.</span>view<span class="token punctuation">(</span>bsz<span class="token punctuation">,</span> q_len<span class="token punctuation">,</span> self<span class="token punctuation">.</span>num_heads <span class="token operator">*</span> self<span class="token punctuation">.</span>head_dim<span class="token punctuation">)</span>        <span class="token punctuation">)</span>        <span class="token keyword">return</span> attn_output<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="mla-multi-head-latent-attention"><a class="markdownIt-Anchor" href="#mla-multi-head-latent-attention"></a> MLA (Multi-head Latent Attention)</h2><img src="https://raw.githubusercontent.com/weikangqi/picgo/main/mla_cc.png" alt="mla_cc.png" style="zoom: 67%;" /><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> torch<span class="token keyword">from</span> torch <span class="token keyword">import</span> nn<span class="token keyword">from</span> typing <span class="token keyword">import</span> Optional<span class="token keyword">def</span> <span class="token function">apply_rope</span><span class="token punctuation">(</span>x<span class="token punctuation">:</span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">,</span> <span class="token operator">*</span>args<span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">return</span> x<span class="token keyword">def</span> <span class="token function">update_compressed_kv_cache</span><span class="token punctuation">(</span>compressed_kv<span class="token punctuation">:</span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">return</span> compressed_kv<span class="token punctuation">.</span>repeat<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token keyword">def</span> <span class="token function">repeat_kv</span><span class="token punctuation">(</span>hidden_states<span class="token punctuation">:</span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">,</span> n_rep<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span><span class="token builtin">int</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    batch<span class="token punctuation">,</span> num_key_value_heads<span class="token punctuation">,</span> slen<span class="token punctuation">,</span> head_dim <span class="token operator">=</span> hidden_states<span class="token punctuation">.</span>shape    <span class="token keyword">if</span> n_rep <span class="token operator">==</span> <span class="token number">1</span><span class="token punctuation">:</span>        <span class="token keyword">return</span> hidden_states    hidden_states <span class="token operator">=</span> hidden_states<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token boolean">None</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">.</span>expand<span class="token punctuation">(</span>        batch<span class="token punctuation">,</span> num_key_value_heads<span class="token punctuation">,</span> n_rep<span class="token punctuation">,</span> slen<span class="token punctuation">,</span> head_dim    <span class="token punctuation">)</span>    <span class="token keyword">return</span> hidden_states<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span>batch<span class="token punctuation">,</span> num_key_value_heads <span class="token operator">*</span> n_rep<span class="token punctuation">,</span> slen<span class="token punctuation">,</span> head_dim<span class="token punctuation">)</span><span class="token keyword">class</span> <span class="token class-name">MultiHeadLatentAttention</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>        self<span class="token punctuation">,</span>        hidden_size<span class="token punctuation">:</span> <span class="token builtin">int</span><span class="token punctuation">,</span>        num_heads<span class="token punctuation">:</span> <span class="token builtin">int</span><span class="token punctuation">,</span>        q_lora_rank<span class="token punctuation">:</span> <span class="token builtin">int</span><span class="token punctuation">,</span>        qk_rope_head_dim<span class="token punctuation">:</span> <span class="token builtin">int</span><span class="token punctuation">,</span>        qk_nope_head_dim<span class="token punctuation">:</span> <span class="token builtin">int</span><span class="token punctuation">,</span>        kv_lora_rank<span class="token punctuation">:</span> <span class="token builtin">int</span><span class="token punctuation">,</span>        v_head_dim<span class="token punctuation">:</span> <span class="token builtin">int</span><span class="token punctuation">,</span>        use_cache<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span><span class="token builtin">bool</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">False</span><span class="token punctuation">,</span>    <span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>        qk_head_dim <span class="token operator">=</span> qk_nope_head_dim <span class="token operator">+</span> qk_rope_head_dim        self<span class="token punctuation">.</span>hidden_size <span class="token operator">=</span> hidden_size        self<span class="token punctuation">.</span>num_heads <span class="token operator">=</span> num_heads        self<span class="token punctuation">.</span>q_lora_rank <span class="token operator">=</span> q_lora_rank        self<span class="token punctuation">.</span>qk_rope_head_dim <span class="token operator">=</span> qk_rope_head_dim        self<span class="token punctuation">.</span>qk_nope_head_dim <span class="token operator">=</span> qk_nope_head_dim        self<span class="token punctuation">.</span>kv_lora_rank <span class="token operator">=</span> kv_lora_rank        self<span class="token punctuation">.</span>qk_head_dim <span class="token operator">=</span> qk_head_dim        self<span class="token punctuation">.</span>v_head_dim <span class="token operator">=</span> v_head_dim        self<span class="token punctuation">.</span>scale <span class="token operator">=</span> self<span class="token punctuation">.</span>qk_head_dim<span class="token operator">**</span><span class="token operator">-</span><span class="token number">0.5</span>        self<span class="token punctuation">.</span>use_cache <span class="token operator">=</span> use_cache        self<span class="token punctuation">.</span>q_a_proj <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>hidden_size<span class="token punctuation">,</span> q_lora_rank<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>q_b_proj <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>q_lora_rank<span class="token punctuation">,</span> num_heads <span class="token operator">*</span> qk_head_dim<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>kv_a_proj <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>hidden_size<span class="token punctuation">,</span> qk_rope_head_dim <span class="token operator">+</span> kv_lora_rank<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>kv_b_proj <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>            kv_lora_rank<span class="token punctuation">,</span> num_heads <span class="token operator">*</span> <span class="token punctuation">(</span>qk_nope_head_dim <span class="token operator">+</span> v_head_dim<span class="token punctuation">)</span>        <span class="token punctuation">)</span>        self<span class="token punctuation">.</span>o_proj <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>num_heads <span class="token operator">*</span> v_head_dim<span class="token punctuation">,</span> hidden_size<span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>        self<span class="token punctuation">,</span>        hidden_states<span class="token punctuation">:</span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">,</span>        position_ids<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>torch<span class="token punctuation">.</span>LongTensor<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span>    <span class="token punctuation">)</span><span class="token punctuation">:</span>        bsz<span class="token punctuation">,</span> q_len<span class="token punctuation">,</span> _ <span class="token operator">=</span> hidden_states<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span>        query_states <span class="token operator">=</span> self<span class="token punctuation">.</span>q_b_proj<span class="token punctuation">(</span>self<span class="token punctuation">.</span>q_a_proj<span class="token punctuation">(</span>hidden_states<span class="token punctuation">)</span><span class="token punctuation">)</span>        query_states <span class="token operator">=</span> query_states<span class="token punctuation">.</span>view<span class="token punctuation">(</span>            bsz<span class="token punctuation">,</span> q_len<span class="token punctuation">,</span> self<span class="token punctuation">.</span>num_heads<span class="token punctuation">,</span> self<span class="token punctuation">.</span>qk_head_dim        <span class="token punctuation">)</span><span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>        q_nope<span class="token punctuation">,</span> q_rope <span class="token operator">=</span> query_states<span class="token punctuation">.</span>split<span class="token punctuation">(</span>            <span class="token punctuation">[</span>self<span class="token punctuation">.</span>qk_nope_head_dim<span class="token punctuation">,</span> self<span class="token punctuation">.</span>qk_rope_head_dim<span class="token punctuation">]</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span>        <span class="token punctuation">)</span>        q_rope <span class="token operator">=</span> apply_rope<span class="token punctuation">(</span>q_rope<span class="token punctuation">)</span>        query_states <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">[</span>q_nope<span class="token punctuation">,</span> q_rope<span class="token punctuation">]</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>        <span class="token comment"># compressed_kv: (bsz, q_len, qk_rope_head_dim + kv_lora_rank)</span>        compressed_kv <span class="token operator">=</span> self<span class="token punctuation">.</span>kv_a_proj<span class="token punctuation">(</span>hidden_states<span class="token punctuation">)</span>        k_rope<span class="token punctuation">,</span> kv_nope <span class="token operator">=</span> compressed_kv<span class="token punctuation">.</span>split<span class="token punctuation">(</span>            <span class="token punctuation">[</span>self<span class="token punctuation">.</span>qk_rope_head_dim<span class="token punctuation">,</span> self<span class="token punctuation">.</span>kv_lora_rank<span class="token punctuation">]</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span>        <span class="token punctuation">)</span>        <span class="token comment"># k_rope: (bsz, num_heads, q_len, qk_rope_head_dim)</span>        k_rope <span class="token operator">=</span> repeat_kv<span class="token punctuation">(</span>k_rope<span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>num_heads<span class="token punctuation">)</span>        k_rope <span class="token operator">=</span> apply_rope<span class="token punctuation">(</span>k_rope<span class="token punctuation">)</span>        <span class="token comment"># Store compressed kv as a whole</span>        <span class="token keyword">if</span> self<span class="token punctuation">.</span>use_cache<span class="token punctuation">:</span>            compressed_kv <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">[</span>k_rope<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">,</span> kv_nope<span class="token punctuation">]</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>            compressed_kv <span class="token operator">=</span> update_compressed_kv_cache<span class="token punctuation">(</span>compressed_kv<span class="token punctuation">)</span>            k_rope<span class="token punctuation">,</span> kv_nope <span class="token operator">=</span> compressed_kv<span class="token punctuation">.</span>split<span class="token punctuation">(</span>                <span class="token punctuation">[</span>self<span class="token punctuation">.</span>qk_rope_head_dim<span class="token punctuation">,</span> self<span class="token punctuation">.</span>kv_lora_rank<span class="token punctuation">]</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span>            <span class="token punctuation">)</span>            k_rope <span class="token operator">=</span> repeat_kv<span class="token punctuation">(</span>k_rope<span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>num_heads<span class="token punctuation">)</span>        kv_len <span class="token operator">=</span> compressed_kv<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span>                kv <span class="token operator">=</span> <span class="token punctuation">(</span>            self<span class="token punctuation">.</span>kv_b_proj<span class="token punctuation">(</span>kv_nope<span class="token punctuation">)</span>            <span class="token punctuation">.</span>view<span class="token punctuation">(</span>bsz<span class="token punctuation">,</span> kv_len<span class="token punctuation">,</span> self<span class="token punctuation">.</span>num_heads<span class="token punctuation">,</span> self<span class="token punctuation">.</span>qk_nope_head_dim <span class="token operator">+</span> self<span class="token punctuation">.</span>v_head_dim<span class="token punctuation">)</span>            <span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>        <span class="token punctuation">)</span>        k_nope<span class="token punctuation">,</span> value_states <span class="token operator">=</span> kv<span class="token punctuation">.</span>split<span class="token punctuation">(</span>            <span class="token punctuation">[</span>self<span class="token punctuation">.</span>qk_nope_head_dim<span class="token punctuation">,</span> self<span class="token punctuation">.</span>v_head_dim<span class="token punctuation">]</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span>        <span class="token punctuation">)</span>        key_states <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">[</span>k_rope<span class="token punctuation">,</span> k_nope<span class="token punctuation">]</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>        <span class="token comment"># Softmax(Q @ K^T / sqrt(d_k))</span>        <span class="token comment"># attn: (bsz, num_heads, q_len, kv_len)</span>        attn_weights <span class="token operator">=</span> <span class="token punctuation">(</span>            torch<span class="token punctuation">.</span>einsum<span class="token punctuation">(</span><span class="token string">"bhld, bhnd -> bhln"</span><span class="token punctuation">,</span> query_states<span class="token punctuation">,</span> key_states<span class="token punctuation">)</span> <span class="token operator">*</span> self<span class="token punctuation">.</span>scale        <span class="token punctuation">)</span>        attn_weights <span class="token operator">=</span> attn_weights<span class="token punctuation">.</span>softmax<span class="token punctuation">(</span>dim<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>        <span class="token comment"># A @ V</span>        <span class="token comment"># attn: (bsz, num_heads, q_len, head_dim)</span>        attn_weights <span class="token operator">=</span> torch<span class="token punctuation">.</span>einsum<span class="token punctuation">(</span><span class="token string">"bhln, bhnd -> bhld"</span><span class="token punctuation">,</span> attn_weights<span class="token punctuation">,</span> value_states<span class="token punctuation">)</span>        <span class="token comment"># output: (bsz, q_len, hidden_size)</span>        attn_output <span class="token operator">=</span> self<span class="token punctuation">.</span>o_proj<span class="token punctuation">(</span>            attn_weights<span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>            <span class="token punctuation">.</span>contiguous<span class="token punctuation">(</span><span class="token punctuation">)</span>            <span class="token punctuation">.</span>view<span class="token punctuation">(</span>bsz<span class="token punctuation">,</span> q_len<span class="token punctuation">,</span> self<span class="token punctuation">.</span>num_heads <span class="token operator">*</span> self<span class="token punctuation">.</span>v_head_dim<span class="token punctuation">)</span>        <span class="token punctuation">)</span>        <span class="token keyword">return</span> attn_output<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="mqa"><a class="markdownIt-Anchor" href="#mqa"></a> MQA</h2><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> torch<span class="token keyword">from</span> torch <span class="token keyword">import</span> nn<span class="token keyword">from</span> typing <span class="token keyword">import</span> Optional<span class="token keyword">def</span> <span class="token function">apply_rope</span><span class="token punctuation">(</span>x<span class="token punctuation">:</span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">,</span> <span class="token operator">*</span>args<span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">return</span> x<span class="token keyword">def</span> <span class="token function">update_kv_cache</span><span class="token punctuation">(</span>key_states<span class="token punctuation">:</span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">,</span> value_states<span class="token punctuation">:</span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">return</span> key_states<span class="token punctuation">.</span>repeat<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> value_states<span class="token punctuation">.</span>repeat<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token keyword">def</span> <span class="token function">repeat_kv</span><span class="token punctuation">(</span>hidden_states<span class="token punctuation">:</span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">,</span> n_rep<span class="token punctuation">:</span> <span class="token builtin">int</span> <span class="token operator">=</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    batch<span class="token punctuation">,</span> num_key_value_heads<span class="token punctuation">,</span> slen<span class="token punctuation">,</span> head_dim <span class="token operator">=</span> hidden_states<span class="token punctuation">.</span>shape    <span class="token keyword">if</span> n_rep <span class="token operator">==</span> <span class="token number">1</span><span class="token punctuation">:</span>        <span class="token keyword">return</span> hidden_states    hidden_states <span class="token operator">=</span> hidden_states<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token boolean">None</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">.</span>expand<span class="token punctuation">(</span>        batch<span class="token punctuation">,</span> num_key_value_heads<span class="token punctuation">,</span> n_rep<span class="token punctuation">,</span> slen<span class="token punctuation">,</span> head_dim    <span class="token punctuation">)</span>    <span class="token keyword">return</span> hidden_states<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span>batch<span class="token punctuation">,</span> num_key_value_heads <span class="token operator">*</span> n_rep<span class="token punctuation">,</span> slen<span class="token punctuation">,</span> head_dim<span class="token punctuation">)</span><span class="token keyword">class</span> <span class="token class-name">MultiQueryAttention</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>        self<span class="token punctuation">,</span>        hidden_size<span class="token punctuation">:</span> <span class="token builtin">int</span><span class="token punctuation">,</span>        num_heads<span class="token punctuation">:</span> <span class="token builtin">int</span><span class="token punctuation">,</span>        head_dim<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span><span class="token builtin">int</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span>        use_cache<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span><span class="token builtin">bool</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">False</span><span class="token punctuation">,</span>    <span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>hidden_size <span class="token operator">=</span> hidden_size        self<span class="token punctuation">.</span>num_heads <span class="token operator">=</span> num_heads        self<span class="token punctuation">.</span>head_dim <span class="token operator">=</span> head_dim <span class="token keyword">or</span> hidden_size <span class="token operator">//</span> num_heads        self<span class="token punctuation">.</span>scale <span class="token operator">=</span> self<span class="token punctuation">.</span>head_dim<span class="token operator">**</span><span class="token operator">-</span><span class="token number">0.5</span>        self<span class="token punctuation">.</span>use_cache <span class="token operator">=</span> use_cache        self<span class="token punctuation">.</span>q_proj <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>self<span class="token punctuation">.</span>hidden_size<span class="token punctuation">,</span> self<span class="token punctuation">.</span>num_heads <span class="token operator">*</span> self<span class="token punctuation">.</span>head_dim<span class="token punctuation">)</span>        <span class="token comment"># different from the MHA</span>        self<span class="token punctuation">.</span>k_proj <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>self<span class="token punctuation">.</span>hidden_size<span class="token punctuation">,</span> self<span class="token punctuation">.</span>head_dim<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>v_proj <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>self<span class="token punctuation">.</span>hidden_size<span class="token punctuation">,</span> self<span class="token punctuation">.</span>head_dim<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>o_proj <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>self<span class="token punctuation">.</span>num_heads <span class="token operator">*</span> self<span class="token punctuation">.</span>head_dim<span class="token punctuation">,</span> self<span class="token punctuation">.</span>hidden_size<span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>        self<span class="token punctuation">,</span>        hidden_states<span class="token punctuation">:</span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">,</span>        position_ids<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>torch<span class="token punctuation">.</span>LongTensor<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span>    <span class="token punctuation">)</span><span class="token punctuation">:</span>        bsz<span class="token punctuation">,</span> q_len<span class="token punctuation">,</span> _ <span class="token operator">=</span> hidden_states<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span>        <span class="token comment"># q: (bsz, num_heads, q_len, head_dim)</span>        query_states <span class="token operator">=</span> <span class="token punctuation">(</span>            self<span class="token punctuation">.</span>q_proj<span class="token punctuation">(</span>hidden_states<span class="token punctuation">)</span>            <span class="token punctuation">.</span>view<span class="token punctuation">(</span>bsz<span class="token punctuation">,</span> q_len<span class="token punctuation">,</span> self<span class="token punctuation">.</span>num_heads<span class="token punctuation">,</span> self<span class="token punctuation">.</span>head_dim<span class="token punctuation">)</span>            <span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>        <span class="token punctuation">)</span>        <span class="token comment"># k/v: (bsz, 1, q_len, head_dim)</span>        key_states <span class="token operator">=</span> self<span class="token punctuation">.</span>k_proj<span class="token punctuation">(</span>hidden_states<span class="token punctuation">)</span><span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span>        value_states <span class="token operator">=</span> self<span class="token punctuation">.</span>v_proj<span class="token punctuation">(</span>hidden_states<span class="token punctuation">)</span><span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span>        <span class="token comment"># RoPE</span>        query_states <span class="token operator">=</span> apply_rope<span class="token punctuation">(</span>query_states<span class="token punctuation">)</span>        key_states <span class="token operator">=</span> apply_rope<span class="token punctuation">(</span>key_states<span class="token punctuation">)</span>        <span class="token keyword">if</span> self<span class="token punctuation">.</span>use_cache<span class="token punctuation">:</span>            <span class="token comment"># Update KV Cache and get full kv</span>            <span class="token comment"># k/v: (bsz, num_heads, kv_len, head_dim)</span>            key_states<span class="token punctuation">,</span> value_states <span class="token operator">=</span> update_kv_cache<span class="token punctuation">(</span>key_states<span class="token punctuation">,</span> value_states<span class="token punctuation">)</span>        kv_len <span class="token operator">=</span> key_states<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span>        <span class="token comment"># Repeat kv</span>        <span class="token comment"># k/v: (bsz, num_heads, q_len, head_dim)</span>        key_states <span class="token operator">=</span> repeat_kv<span class="token punctuation">(</span>key_states<span class="token punctuation">,</span> self<span class="token punctuation">.</span>num_heads<span class="token punctuation">)</span>        value_states <span class="token operator">=</span> repeat_kv<span class="token punctuation">(</span>value_states<span class="token punctuation">,</span> self<span class="token punctuation">.</span>num_heads<span class="token punctuation">)</span>        <span class="token comment"># Softmax(Q @ K^T / sqrt(d_k))</span>        <span class="token comment"># attn: (bsz, num_heads, q_len, kv_len)</span>        attn_weights <span class="token operator">=</span> <span class="token punctuation">(</span>            torch<span class="token punctuation">.</span>einsum<span class="token punctuation">(</span><span class="token string">"bhld, bhnd -> bhln"</span><span class="token punctuation">,</span> query_states<span class="token punctuation">,</span> key_states<span class="token punctuation">)</span> <span class="token operator">*</span> self<span class="token punctuation">.</span>scale        <span class="token punctuation">)</span>        attn_weights <span class="token operator">=</span> attn_weights<span class="token punctuation">.</span>softmax<span class="token punctuation">(</span>dim<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>        <span class="token comment"># A @ V</span>        <span class="token comment"># attn: (bsz, num_heads, q_len, head_dim)</span>        attn_weights <span class="token operator">=</span> torch<span class="token punctuation">.</span>einsum<span class="token punctuation">(</span><span class="token string">"bhln, bhnd -> bhld"</span><span class="token punctuation">,</span> attn_weights<span class="token punctuation">,</span> value_states<span class="token punctuation">)</span>        <span class="token comment"># output: (bsz, q_len, hidden_size)</span>        attn_output <span class="token operator">=</span> self<span class="token punctuation">.</span>o_proj<span class="token punctuation">(</span>            attn_weights<span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>            <span class="token punctuation">.</span>contiguous<span class="token punctuation">(</span><span class="token punctuation">)</span>            <span class="token punctuation">.</span>view<span class="token punctuation">(</span>bsz<span class="token punctuation">,</span> q_len<span class="token punctuation">,</span> self<span class="token punctuation">.</span>num_heads <span class="token operator">*</span> self<span class="token punctuation">.</span>head_dim<span class="token punctuation">)</span>        <span class="token punctuation">)</span>        <span class="token keyword">return</span> attn_output<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;attention-mha-mla-gqamqa&quot;&gt;&lt;a class=&quot;markdownIt-Anchor&quot; href=&quot;#attention-mha-mla-gqamqa&quot;&gt;&lt;/a&gt; Attention: MHA, MLA, GQA,MQA&lt;/h1&gt;
&lt;p&gt;&lt;i</summary>
      
    
    
    
    
    <category term="LLM" scheme="https://weikangqi.github.io/tags/LLM/"/>
    
  </entry>
  
  <entry>
    <title>ARM 中常用的乘累加（MAC）指令总结</title>
    <link href="https://weikangqi.github.io/2024/12/ARM%20%E7%B4%AF%E5%8A%A0%E4%B9%98/"/>
    <id>https://weikangqi.github.io/2024/12/ARM%20%E7%B4%AF%E5%8A%A0%E4%B9%98/</id>
    <published>2024-12-01T15:30:16.000Z</published>
    <updated>2025-09-11T03:36:51.303Z</updated>
    
    <content type="html"><![CDATA[<hr /><h1 id="arm-中常用的乘累加mac指令总结"><a class="markdownIt-Anchor" href="#arm-中常用的乘累加mac指令总结"></a> ARM 中常用的乘累加（MAC）指令总结</h1><p>在现代 CPU 和 AI 加速器的指令集中，<strong>乘-累加（Multiply-Accumulate, MAC）</strong> 是最核心的计算操作之一。<br />无论是 <strong>卷积</strong>、<strong>矩阵乘法</strong> 还是 <strong>深度学习中的 GEMM</strong>，其本质都是大量的 MAC 运算。</p><p>在 ARM 架构下，针对不同的数据类型和应用场景，演进出了多种 <strong>乘累加指令</strong>。本文将对常见的几类进行梳理，并配合伪代码展示它们的计算方式。</p><hr /><h2 id="1-fmla-基础乘累加"><a class="markdownIt-Anchor" href="#1-fmla-基础乘累加"></a> 1. FMLA —— 基础乘累加</h2><p><strong>FMLA (Floating-point Multiply-Accumulate)</strong> 是最基础的向量乘累加指令。</p><ul><li><p><strong>指令集</strong>：<code>&gt;= ARMv7 (float32)</code>，<code>&gt;= ARMv8 (float16)</code></p></li><li><p><strong>常见 NEON 接口</strong>：</p><ul><li><code>float32x4_t vfmaq_f32(float32x4_t a, float32x4_t b, float32x4_t c)</code></li><li><code>float16x8_t vfmaq_f16(float16x8_t a, float16x8_t b, float16x8_t c)</code></li></ul></li><li><p><strong>运算逻辑</strong>：逐元素乘加</p><pre class="line-numbers language-c" data-language="c"><code class="language-c"><span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> i <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> i <span class="token operator">&lt;</span> N<span class="token punctuation">;</span> i<span class="token operator">++</span><span class="token punctuation">)</span> <span class="token punctuation">&#123;</span>    dst<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">+=</span> src1<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">*</span> src2<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">;</span><span class="token punctuation">&#125;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre></li></ul><blockquote><p>典型用途：浮点向量加速，如卷积核点积、向量化计算。</p></blockquote><hr /><h2 id="2-dot-向量点积"><a class="markdownIt-Anchor" href="#2-dot-向量点积"></a> 2. DOT —— 向量点积</h2><p>为了更高效地处理 <strong>int8 量化推理</strong>，ARM 在 <strong>ARMv8.2</strong> 中引入了 <strong>DOT (Dot Product)</strong> 指令。</p><ul><li><p><strong>指令集</strong>：<code>&gt;= ARMv8.2</code></p></li><li><p><strong>常见 NEON 接口</strong>：</p><ul><li><code>int32x4_t vdotq_s32(int32x4_t r, int8x16_t a, int8x16_t b)</code></li><li><code>int32x4_t vusdotq_s32(int32x4_t r, uint8x16_t a, int8x16_t b)</code></li></ul></li><li><p><strong>运算逻辑</strong>：分块点积 (4 × 4 → int32)</p><pre class="line-numbers language-c" data-language="c"><code class="language-c"><span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> i <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> i <span class="token operator">&lt;</span> <span class="token number">4</span><span class="token punctuation">;</span> i<span class="token operator">++</span><span class="token punctuation">)</span> <span class="token punctuation">&#123;</span>    <span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> j <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> j <span class="token operator">&lt;</span> <span class="token number">4</span><span class="token punctuation">;</span> j<span class="token operator">++</span><span class="token punctuation">)</span> <span class="token punctuation">&#123;</span>        dst<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">+=</span> src1<span class="token punctuation">[</span>i <span class="token operator">*</span> <span class="token number">4</span> <span class="token operator">+</span> j<span class="token punctuation">]</span> <span class="token operator">*</span> src2<span class="token punctuation">[</span>i <span class="token operator">*</span> <span class="token number">4</span> <span class="token operator">+</span> j<span class="token punctuation">]</span><span class="token punctuation">;</span>    <span class="token punctuation">&#125;</span><span class="token punctuation">&#125;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre></li></ul><blockquote><p>典型用途：卷积、点积，特别适合 int8 量化模型。</p></blockquote><hr /><h2 id="3-mmla-矩阵乘累加"><a class="markdownIt-Anchor" href="#3-mmla-矩阵乘累加"></a> 3. MMLA —— 矩阵乘累加</h2><p>在 <strong>ARMv8.6</strong> 之后，进一步推出了 <strong>MMLA (Matrix Multiply-Accumulate)</strong> 指令，可以直接把 <strong>小块向量视为矩阵</strong>，一次性完成 <strong>2×8 × 8×2</strong> 的矩阵乘法。</p><ul><li><p><strong>指令集</strong>：<code>&gt;= ARMv8.6</code></p></li><li><p><strong>常见 NEON 接口</strong>：</p><ul><li><code>int32x4_t vmmlaq_s32(int32x4_t r, int8x16_t a, int8x16_t b)</code></li><li><code>int32x4_t vusmmlaq_s32(int32x4_t r, uint8x16_t a, int8x16_t b)</code></li></ul></li><li><p><strong>运算逻辑</strong>：小矩阵乘法</p><pre class="line-numbers language-c" data-language="c"><code class="language-c"><span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> i <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> i <span class="token operator">&lt;</span> <span class="token number">2</span><span class="token punctuation">;</span> i<span class="token operator">++</span><span class="token punctuation">)</span> <span class="token punctuation">&#123;</span>    <span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> j <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> j <span class="token operator">&lt;</span> <span class="token number">2</span><span class="token punctuation">;</span> j<span class="token operator">++</span><span class="token punctuation">)</span> <span class="token punctuation">&#123;</span>        <span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> k <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> k <span class="token operator">&lt;</span> <span class="token number">8</span><span class="token punctuation">;</span> k<span class="token operator">++</span><span class="token punctuation">)</span> <span class="token punctuation">&#123;</span>            dst<span class="token punctuation">[</span>i <span class="token operator">*</span> <span class="token number">2</span> <span class="token operator">+</span> j<span class="token punctuation">]</span> <span class="token operator">+=</span> src1<span class="token punctuation">[</span>i <span class="token operator">*</span> <span class="token number">8</span> <span class="token operator">+</span> k<span class="token punctuation">]</span> <span class="token operator">*</span> src2<span class="token punctuation">[</span>j <span class="token operator">*</span> <span class="token number">8</span> <span class="token operator">+</span> k<span class="token punctuation">]</span><span class="token punctuation">;</span>        <span class="token punctuation">&#125;</span>    <span class="token punctuation">&#125;</span><span class="token punctuation">&#125;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li></ul><blockquote><p>典型用途：高效的 int8 GEMM，加速深度学习推理。</p></blockquote><hr /><h2 id="4-bfmmla-bfloat16-矩阵乘累加"><a class="markdownIt-Anchor" href="#4-bfmmla-bfloat16-矩阵乘累加"></a> 4. BFMMLA —— BFloat16 矩阵乘累加</h2><p>为了更好地支持 <strong>AI 训练与推理</strong>，ARMv8.6 又引入了 <strong>BFMMLA (BFloat16 Matrix Multiply-Accumulate)</strong>，专门处理 <strong>bfloat16 × bfloat16 → float32</strong>。</p><ul><li><p><strong>指令集</strong>：<code>&gt;= ARMv8.6</code></p></li><li><p><strong>常见 NEON 接口</strong>：</p><ul><li><code>float32x4_t vbfmmlaq_f32(float32x4_t r, bfloat16x8_t a, bfloat16x8_t b)</code></li></ul></li><li><p><strong>运算逻辑</strong>：小矩阵乘法</p><pre class="line-numbers language-c" data-language="c"><code class="language-c"><span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> i <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> i <span class="token operator">&lt;</span> <span class="token number">2</span><span class="token punctuation">;</span> i<span class="token operator">++</span><span class="token punctuation">)</span> <span class="token punctuation">&#123;</span>    <span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> j <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> j <span class="token operator">&lt;</span> <span class="token number">2</span><span class="token punctuation">;</span> j<span class="token operator">++</span><span class="token punctuation">)</span> <span class="token punctuation">&#123;</span>        <span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> k <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> k <span class="token operator">&lt;</span> <span class="token number">4</span><span class="token punctuation">;</span> k<span class="token operator">++</span><span class="token punctuation">)</span> <span class="token punctuation">&#123;</span>            dst<span class="token punctuation">[</span>i <span class="token operator">*</span> <span class="token number">2</span> <span class="token operator">+</span> j<span class="token punctuation">]</span> <span class="token operator">+=</span> src1<span class="token punctuation">[</span>i <span class="token operator">*</span> <span class="token number">4</span> <span class="token operator">+</span> k<span class="token punctuation">]</span> <span class="token operator">*</span> src2<span class="token punctuation">[</span>j <span class="token operator">*</span> <span class="token number">4</span> <span class="token operator">+</span> k<span class="token punctuation">]</span><span class="token punctuation">;</span>        <span class="token punctuation">&#125;</span>    <span class="token punctuation">&#125;</span><span class="token punctuation">&#125;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li></ul><blockquote><p>典型用途：深度学习训练（bfloat16 是目前主流 AI 训练精度格式）。</p></blockquote><table><thead><tr><th>指令</th><th>指令集</th><th>数据类型</th><th>运算规模</th><th>主要用途</th></tr></thead><tbody><tr><td><strong>FMLA</strong></td><td>ARMv7/ARMv8</td><td>FP16/FP32</td><td>逐元素</td><td>浮点向量运算</td></tr><tr><td><strong>SDOT/USDOT</strong></td><td>ARMv8.2</td><td>int8/uint8 → int32</td><td>4×4 点积</td><td>int8 卷积、点积</td></tr><tr><td><strong>SMMLA/USMMLA</strong></td><td>ARMv8.6</td><td>int8/uint8 → int32</td><td>2×2×8 矩阵</td><td>int8 GEMM</td></tr><tr><td><strong>BFMMLA</strong></td><td>ARMv8.6</td><td>bfloat16 → float32</td><td>2×2×4 矩阵</td><td>深度学习 (bfloat16)</td></tr></tbody></table><hr />]]></content>
    
    
      
      
    <summary type="html">&lt;hr /&gt;
&lt;h1 id=&quot;arm-中常用的乘累加mac指令总结&quot;&gt;&lt;a class=&quot;markdownIt-Anchor&quot; href=&quot;#arm-中常用的乘累加mac指令总结&quot;&gt;&lt;/a&gt; ARM 中常用的乘累加（MAC）指令总结&lt;/h1&gt;
&lt;p&gt;在现代 CPU 和 AI 加速</summary>
      
    
    
    
    
    <category term="ARM" scheme="https://weikangqi.github.io/tags/ARM/"/>
    
  </entry>
  
  <entry>
    <title>flash attention</title>
    <link href="https://weikangqi.github.io/2024/12/flashattention/"/>
    <id>https://weikangqi.github.io/2024/12/flashattention/</id>
    <published>2024-12-01T15:30:16.000Z</published>
    <updated>2025-09-11T03:36:51.303Z</updated>
    
    <content type="html"><![CDATA[<h1 id="flashattention"><a class="markdownIt-Anchor" href="#flashattention"></a> FlashAttention</h1><p><a href="https://courses.cs.washington.edu/courses/cse599m/23sp/notes/flashattn.pdf">https://courses.cs.washington.edu/courses/cse599m/23sp/notes/flashattn.pdf</a></p><h2 id="self-attention"><a class="markdownIt-Anchor" href="#self-attention"></a> self-Attention</h2><p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mtable width="100%"><mtr><mtd width="50%"></mtd><mtd><mrow><mi>O</mi><mo>=</mo><mi>s</mi><mi>o</mi><mi>f</mi><mi>t</mi><mi>m</mi><mi>a</mi><mi>x</mi><mo stretchy="false">(</mo><mi>Q</mi><msup><mi>K</mi><mi>T</mi></msup><mo stretchy="false">)</mo><mi>V</mi></mrow></mtd><mtd width="50%"></mtd><mtd><mtext>(1)</mtext></mtd></mtr></mtable><annotation encoding="application/x-tex">O = softmax(QK^T)V \tag{1}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.02778em;">O</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1.1413309999999999em;vertical-align:-0.25em;"></span><span class="mord mathdefault">s</span><span class="mord mathdefault">o</span><span class="mord mathdefault" style="margin-right:0.10764em;">f</span><span class="mord mathdefault">t</span><span class="mord mathdefault">m</span><span class="mord mathdefault">a</span><span class="mord mathdefault">x</span><span class="mopen">(</span><span class="mord mathdefault">Q</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.07153em;">K</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8913309999999999em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.13889em;">T</span></span></span></span></span></span></span></span><span class="mclose">)</span><span class="mord mathdefault" style="margin-right:0.22222em;">V</span></span><span class="tag"><span class="strut" style="height:1.1413309999999999em;vertical-align:-0.25em;"></span><span class="mord text"><span class="mord">(</span><span class="mord"><span class="mord">1</span></span><span class="mord">)</span></span></span></span></span></span></p><h2 id="softmax"><a class="markdownIt-Anchor" href="#softmax"></a> softmax</h2><img src="https://raw.githubusercontent.com/weikangqi/picgo/main/image-20250904152025097.png" alt="image-20250904152025097" style="zoom: 50%;" /><p><span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>x</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">x_{i}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">i</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 如果比较大，<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msup><mi>e</mi><msub><mi>x</mi><mi>i</mi></msub></msup></mrow><annotation encoding="application/x-tex">e^{x_{i}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.664392em;vertical-align:0em;"></span><span class="mord"><span class="mord mathdefault">e</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.664392em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathdefault mtight">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3280857142857143em;"><span style="top:-2.357em;margin-left:0em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">i</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em;"><span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span> 可能会溢出，比如float16 最大支持65536，当x&gt;11, <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msup><mi>e</mi><mi>x</mi></msup></mrow><annotation encoding="application/x-tex">e^{x}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.664392em;vertical-align:0em;"></span><span class="mord"><span class="mord mathdefault">e</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.664392em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">x</span></span></span></span></span></span></span></span></span></span></span></span> 就超出了float16的表示范围。</p><p>为了解决这个问题，可以上下除以最大值，来解决溢出问题。</p><img src="https://raw.githubusercontent.com/weikangqi/picgo/main/image-20250904152357842.png" alt="image-20250904152357842" style="zoom: 50%;" /><h2 id="safe-softmax-3-pass"><a class="markdownIt-Anchor" href="#safe-softmax-3-pass"></a> Safe softmax (3-Pass)</h2><ul><li>第一步 计算m 最大值</li><li>第二步 计算d 也就是分母，求和</li><li>第三步 分子/分母求每个值</li></ul><p>需要3个循环，来访问[1,N]。</p><h2 id="online-sofamax-2-pass"><a class="markdownIt-Anchor" href="#online-sofamax-2-pass"></a> Online sofamax (2-Pass)</h2><p><strong>将第一步和第二步合成一个pass</strong></p><p>依赖当前最大值<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>m</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">m_{i}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault">m</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">i</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>和当前sum值<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>d</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">d_{i}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.84444em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">i</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span></p><img src="https://raw.githubusercontent.com/weikangqi/picgo/main/image-20250904154033846.png" alt="image-20250904154033846" style="zoom:50%;" /><img src="https://raw.githubusercontent.com/weikangqi/picgo/main/image-20250904153909833.png" alt="image-20250904153909833" style="zoom: 50%;" /><h2 id="flashattention-1-pass"><a class="markdownIt-Anchor" href="#flashattention-1-pass"></a> FlashAttention (1-Pass)</h2><img src="https://raw.githubusercontent.com/weikangqi/picgo/main/image-20250904160034426.png" alt="image-20250904160034426" style="zoom: 50%;" /><p><span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>x</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">x_{i}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">i</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 等于<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>Q</mi></mrow><annotation encoding="application/x-tex">Q</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8777699999999999em;vertical-align:-0.19444em;"></span><span class="mord mathdefault">Q</span></span></span></span>的<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>k</mi></mrow><annotation encoding="application/x-tex">k</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.03148em;">k</span></span></span></span>行乘以<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msup><mi>K</mi><mi>T</mi></msup></mrow><annotation encoding="application/x-tex">K^{T}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8413309999999999em;vertical-align:0em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.07153em;">K</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8413309999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.13889em;">T</span></span></span></span></span></span></span></span></span></span></span></span>的<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>i</mi></mrow><annotation encoding="application/x-tex">i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.65952em;vertical-align:0em;"></span><span class="mord mathdefault">i</span></span></span></span>列。</p><img src="https://raw.githubusercontent.com/weikangqi/picgo/main/image-20250904160413940.png" alt="image-20250904160413940" style="zoom:50%;" /><img src="https://raw.githubusercontent.com/weikangqi/picgo/main/image-20250904160509203.png" alt="image-20250904160509203" style="zoom: 50%;" /><img src="https://raw.githubusercontent.com/weikangqi/picgo/main/image-20250904161026687.png" alt="image-20250904161026687" style="zoom:50%;" /><p>总结：</p><p>softmax 可以做成流式计算，把softmax的分母计算，也就是求和计算和求最大值计算融入到一个pass中，不依赖全局的最大值，而是局部的最大值。借助sharememory来存储中间值，这样就2pass。但是flashattention 可以。softmax之后和v相乘累加，满足加法结合率。对于MQA，GQA通过index的方法来加载KVcache计算，而不是copy一份。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;flashattention&quot;&gt;&lt;a class=&quot;markdownIt-Anchor&quot; href=&quot;#flashattention&quot;&gt;&lt;/a&gt; FlashAttention&lt;/h1&gt;
&lt;p&gt;&lt;a href=&quot;https://courses.cs.washingt</summary>
      
    
    
    
    
    <category term="ARM" scheme="https://weikangqi.github.io/tags/ARM/"/>
    
  </entry>
  
  <entry>
    <title>frp 内网穿透配置</title>
    <link href="https://weikangqi.github.io/2024/07/%E6%9D%82%E9%A1%B9/frp%E4%BB%A3%E7%90%86/"/>
    <id>https://weikangqi.github.io/2024/07/%E6%9D%82%E9%A1%B9/frp%E4%BB%A3%E7%90%86/</id>
    <published>2024-07-30T18:42:30.000Z</published>
    <updated>2025-09-11T03:36:51.304Z</updated>
    
    <content type="html"><![CDATA[<h1 id="frp-内网穿透配置"><a class="markdownIt-Anchor" href="#frp-内网穿透配置"></a> frp 内网穿透配置</h1><blockquote><p>要放暑假了，在家需要连接实验室服务器，之前通过云主机搭建frp中转，但是云主机比较贵，所以来白嫖Sakura Frp</p></blockquote><h2 id="1-注册账户"><a class="markdownIt-Anchor" href="#1-注册账户"></a> 1. 注册账户</h2><p>普通用户有两个隧道，限速10M,一个月有10G流量，可以每日签到获取额外的流量，不过对于ssh来讲，基本上是够用的，代码文本也消耗不了多少流量。</p><h2 id="2-建立隧道"><a class="markdownIt-Anchor" href="#2-建立隧道"></a> 2. 建立隧道</h2><p>实名制后，可以点击服务来创建一个隧道，来中转服务。</p><p><img src="https://s2.loli.net/2024/07/30/uOVJQl7mUSLnt2C.png" alt="QQ_1722343688110" /></p><p>这样就创建了一个隧道</p><p>frp本质上做的事情就是，将两端的流量进行转发，因为两端没有公网IP，也可以使用<code>zerotier</code></p><p>来打洞，但是如果NAT层比较多，延时还是比较大，在学校里面使用延时比较低，如果在家延时比较高，体验不好。</p><h2 id="3-服务端"><a class="markdownIt-Anchor" href="#3-服务端"></a> 3. 服务端</h2><p>我把你要访问的电脑称为服务端，也就是实验室的服务器</p><p>服务端需要下载frp</p><p><img src="https://s2.loli.net/2024/07/30/LdeWwjQl34Tn9r7.png" alt="QQ_1722342725796" /></p><ul><li>点击复制链接</li></ul><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash"><span class="token function">wget</span> 链接<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><ul><li><p>将frp下载下来，使用<code>tar -xvf natfrp-service_linux_amd64.tar.zst</code>解压缩</p></li><li><p><code>chmod +x ./frpc</code>来赋予可执行权限</p></li><li><p>获取加密参数</p><p><img src="https://s2.loli.net/2024/07/30/7W1t2ougqnmKOTQ.png" alt="QQ_1722342962407" /></p></li><li><p>启动</p><p><code>./frpc -f 参数</code></p><ul><li>可以使用<code>systemctl</code>来管理frp的服务，包括开机自动启动，开始，暂停等</li><li>nohup开控制frpc后端执行 <code>nohup ./frpc -f 参数 &gt; runoob.log 2&gt;&amp;1 &amp;</code></li></ul><p><img src="https://s2.loli.net/2024/07/30/IgwdNfDqC9jY7sR.png" alt="QQ_1722343190952" /></p></li></ul><p>可以获得访问服务器的<code>ip:port</code></p><h2 id="4-客户端"><a class="markdownIt-Anchor" href="#4-客户端"></a> 4. 客户端</h2><ul><li><p>下载认证</p><p><img src="https://s2.loli.net/2024/07/30/EhS36gJaFVGfLle.png" alt="QQ_1722343257332" /></p></li></ul><p><img src="https://s2.loli.net/2024/07/30/LRFC9bnuv1jlDZc.png" alt="QQ_1722343281279" /></p><p>会生成一个exe，点击运行即可</p><ul><li><p>ssh 连接</p><p>注意这里ssh端口不是22</p><p>而是上面获得的那个IP和端口</p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash"><span class="token function">ssh</span> <span class="token parameter variable">-p</span> port yourname@ip<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>这样就连接到服务器了。</p></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;frp-内网穿透配置&quot;&gt;&lt;a class=&quot;markdownIt-Anchor&quot; href=&quot;#frp-内网穿透配置&quot;&gt;&lt;/a&gt; frp 内网穿透配置&lt;/h1&gt;
&lt;blockquote&gt;
&lt;p&gt;要放暑假了，在家需要连接实验室服务器，之前通过云主机搭建frp中转，但</summary>
      
    
    
    
    <category term="工具配置" scheme="https://weikangqi.github.io/categories/%E5%B7%A5%E5%85%B7%E9%85%8D%E7%BD%AE/"/>
    
    
    <category term="frp" scheme="https://weikangqi.github.io/tags/frp/"/>
    
    <category term="内网穿透" scheme="https://weikangqi.github.io/tags/%E5%86%85%E7%BD%91%E7%A9%BF%E9%80%8F/"/>
    
  </entry>
  
  <entry>
    <title>vscode 调试coredump 文件</title>
    <link href="https://weikangqi.github.io/2024/07/%E8%B0%83%E8%AF%95/vscode%20%E8%B0%83%E8%AF%95coredump%E6%96%87%E4%BB%B6/"/>
    <id>https://weikangqi.github.io/2024/07/%E8%B0%83%E8%AF%95/vscode%20%E8%B0%83%E8%AF%95coredump%E6%96%87%E4%BB%B6/</id>
    <published>2024-07-30T18:42:30.000Z</published>
    <updated>2025-09-11T03:36:51.304Z</updated>
    
    <content type="html"><![CDATA[<ol><li><p>设置<code>ulimit -c unlimited</code>，如果<code>ulimit -c</code>结果是0的话产生不了coredump文件</p> <pre class="line-numbers language-bash" data-language="bash"><code class="language-bash"><span class="token builtin class-name">ulimit</span> <span class="token parameter variable">-c</span> <span class="token punctuation">[</span>size<span class="token punctuation">]</span>  //这里size一般修改为unlimited,或者是其他数字：2048<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>​上修改只对当前的shell有效，一旦关闭，则恢复原来的值</p></li><li><p><code>cat /var/log/apport.log</code> 可以看到生成的日志信息</p></li><li><p>core文件路径</p><ul><li><p>ubuntu20的生成coredump路径不在可执行路径下，而是在<code>/var/lib/apport/coredump</code>，因为没有写入权限，所以产生不了coredump文件，需要sudo，或者修改产生路径。</p></li><li><p>修改core文件产生位置在可执行文件目录下</p><p><code>sudo bash -c 'echo core.%e.%p &gt; /proc/sys/kernel/core_pattern'</code></p></li></ul></li><li><p>vscode 配置</p><ul><li><p>gdb</p><pre class="line-numbers language-json" data-language="json"><code class="language-json"><span class="token punctuation">&#123;</span>    <span class="token property">"name"</span><span class="token operator">:</span> <span class="token string">"(gdb) Launch"</span><span class="token punctuation">,</span>    <span class="token property">"type"</span><span class="token operator">:</span> <span class="token string">"cppdbg"</span><span class="token punctuation">,</span>    <span class="token property">"request"</span><span class="token operator">:</span> <span class="token string">"launch"</span><span class="token punctuation">,</span>    <span class="token property">"program"</span><span class="token operator">:</span> <span class="token string">"$&#123;workspaceFolder&#125;/a.out"</span><span class="token punctuation">,</span>    <span class="token property">"args"</span><span class="token operator">:</span> <span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">,</span>    <span class="token property">"stopAtEntry"</span><span class="token operator">:</span> <span class="token boolean">false</span><span class="token punctuation">,</span>    <span class="token property">"cwd"</span><span class="token operator">:</span> <span class="token string">"$&#123;workspaceFolder&#125;"</span><span class="token punctuation">,</span>    <span class="token property">"environment"</span><span class="token operator">:</span> <span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">,</span>    <span class="token property">"externalConsole"</span><span class="token operator">:</span> <span class="token boolean">false</span><span class="token punctuation">,</span>    <span class="token property">"MIMode"</span><span class="token operator">:</span> <span class="token string">"gdb"</span><span class="token punctuation">,</span>    <span class="token property">"setupCommands"</span><span class="token operator">:</span> <span class="token punctuation">[</span>        <span class="token punctuation">&#123;</span>            <span class="token property">"description"</span><span class="token operator">:</span> <span class="token string">"Enable pretty-printing for gdb"</span><span class="token punctuation">,</span>            <span class="token property">"text"</span><span class="token operator">:</span> <span class="token string">"-enable-pretty-printing"</span><span class="token punctuation">,</span>            <span class="token property">"ignoreFailures"</span><span class="token operator">:</span> <span class="token boolean">true</span>        <span class="token punctuation">&#125;</span>    <span class="token punctuation">]</span><span class="token punctuation">,</span>    <span class="token property">"coreDumpPath"</span><span class="token operator">:</span> <span class="token string">"$&#123;workspaceFolder&#125;/core.a.out.372125"</span><span class="token punctuation">&#125;</span><span class="token punctuation">,</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li></ul><p>虽然填了program，但是实际上是从coredump启动的</p><ul><li><p>codelldb</p><pre class="line-numbers language-json" data-language="json"><code class="language-json"><span class="token punctuation">&#123;</span>           <span class="token property">"type"</span><span class="token operator">:</span> <span class="token string">"lldb"</span><span class="token punctuation">,</span>           <span class="token property">"request"</span><span class="token operator">:</span> <span class="token string">"custom"</span><span class="token punctuation">,</span>           <span class="token property">"name"</span><span class="token operator">:</span> <span class="token string">"Open a core dump"</span><span class="token punctuation">,</span>           <span class="token property">"initCommands"</span><span class="token operator">:</span> <span class="token punctuation">[</span>               <span class="token string">"target create -c $&#123;workspaceFolder&#125;/core.a.out.372125"</span>           <span class="token punctuation">]</span><span class="token punctuation">&#125;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li></ul></li></ol><p>​</p>]]></content>
    
    
      
      
    <summary type="html">&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;设置&lt;code&gt;ulimit -c unlimited&lt;/code&gt;，如果&lt;code&gt;ulimit -c&lt;/code&gt;结果是0的话产生不了coredump文件&lt;/p&gt;
 &lt;pre class=&quot;line-numbers language-bash&quot; da</summary>
      
    
    
    
    <category term="工具配置" scheme="https://weikangqi.github.io/categories/%E5%B7%A5%E5%85%B7%E9%85%8D%E7%BD%AE/"/>
    
    
    <category term="frp" scheme="https://weikangqi.github.io/tags/frp/"/>
    
    <category term="内网穿透" scheme="https://weikangqi.github.io/tags/%E5%86%85%E7%BD%91%E7%A9%BF%E9%80%8F/"/>
    
  </entry>
  
  <entry>
    <title>Linux 中使用adb 连接手机</title>
    <link href="https://weikangqi.github.io/2024/03/%E8%B0%83%E8%AF%95/android%20gdb%20%E8%B0%83%E8%AF%95/"/>
    <id>https://weikangqi.github.io/2024/03/%E8%B0%83%E8%AF%95/android%20gdb%20%E8%B0%83%E8%AF%95/</id>
    <published>2024-03-05T10:00:00.000Z</published>
    <updated>2025-09-11T03:36:51.304Z</updated>
    
    <content type="html"><![CDATA[<h1 id="linux-中使用adb-连接手机"><a class="markdownIt-Anchor" href="#linux-中使用adb-连接手机"></a> Linux 中使用adb 连接手机</h1><p>在做实验时，使用linux服务器连接手机，一直出现 connect 后，手机直接 offline 的情况。<br />解决： 不能使用<code>sudo apt install adb</code> 安装adb，来连接手机。<br />解决方法：</p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash"><span class="token function">mkdir</span> cli-tools<span class="token function">wget</span> <span class="token parameter variable">-c</span> https://dl.google.com/android/repository/platform-tools-latest-linux.zip<span class="token function">unzip</span> platform-tools-latest-linux.zip <span class="token builtin class-name">cd</span> platform-tools/./adb connect yourIP:Port<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;linux-中使用adb-连接手机&quot;&gt;&lt;a class=&quot;markdownIt-Anchor&quot; href=&quot;#linux-中使用adb-连接手机&quot;&gt;&lt;/a&gt; Linux 中使用adb 连接手机&lt;/h1&gt;
&lt;p&gt;在做实验时，使用linux服务器连接手机，一直出现 c</summary>
      
    
    
    
    
    <category term="linux" scheme="https://weikangqi.github.io/tags/linux/"/>
    
    <category term="adb" scheme="https://weikangqi.github.io/tags/adb/"/>
    
  </entry>
  
  <entry>
    <title>使用LLDB 远程调试 安卓native C++ 程序的vscode 配置</title>
    <link href="https://weikangqi.github.io/2024/03/%E8%B0%83%E8%AF%95/%E4%BD%BF%E7%94%A8LLDB%20%E8%BF%9C%E7%A8%8B%E8%B0%83%E8%AF%95android/"/>
    <id>https://weikangqi.github.io/2024/03/%E8%B0%83%E8%AF%95/%E4%BD%BF%E7%94%A8LLDB%20%E8%BF%9C%E7%A8%8B%E8%B0%83%E8%AF%95android/</id>
    <published>2024-03-05T10:00:00.000Z</published>
    <updated>2025-09-11T03:36:51.304Z</updated>
    
    <content type="html"><![CDATA[<h1 id="使用lldb-远程调试-安卓native-c-程序的vscode-配置"><a class="markdownIt-Anchor" href="#使用lldb-远程调试-安卓native-c-程序的vscode-配置"></a> 使用LLDB 远程调试 安卓native C++ 程序的vscode 配置</h1><h2 id="背景"><a class="markdownIt-Anchor" href="#背景"></a> 背景</h2><p>我目前使用windows ssh连接到linux 服务器上,对android native C++代码进行编写,编译(因为服务器核心数量多,编译速度快). 目前高版本的NDK如r26等,已经不再对gdbserver 提供支撑, 所以迁移到LLDB调试,也是主流的技术方向.</p><h2 id="相关配置"><a class="markdownIt-Anchor" href="#相关配置"></a> 相关配置</h2><ol><li>安装vscode 插件<br />CodeLLDB<br /><img src="https://s2.loli.net/2024/03/11/pFxMIkRtHvYUyBL.png" alt="20240311140633" /></li><li>下载NDK<br /><a href="https://developer.android.com/ndk/downloads?hl=zh-cn">NDK下载网站链接</a><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash"><span class="token function">wget</span> https://dl.google.com/android/repository/android-ndk-r26c-linux.zip?hl<span class="token operator">=</span>zh-cn<span class="token function">unzip</span> android-ndk-r26c-linux.zip<span class="token punctuation">\</span>?hl<span class="token punctuation">\</span><span class="token operator">=</span>zh-cn<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre></li><li>adb 上传lldb-server <pre class="line-numbers language-bash" data-language="bash"><code class="language-bash"><span class="token builtin class-name">cd</span>  android-ndk-r26c<span class="token function">find</span> ./ <span class="token parameter variable">-name</span> <span class="token string">"lldb-server"</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><img src="https://s2.loli.net/2024/03/11/1X3Hesi5EPVQ9oy.png" alt="20240311141315" /><br />选取aarch64 版本的lldb-server <pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">adb push ./toolchains/llvm/prebuilt/linux-x86_64/lib/clang/17/lib/linux/aarch64/lldb-server /data/local/tmpadb shell <span class="token string">"chmod +xrw /data/local/tmp/lldb-server"</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre></li><li>启动lldb-server<br /><code>lldb-server platform --server --listen *:9999</code> 端口可以自己选,不冲突就行</li><li>配置vscode 的launch.json 文件<pre class="line-numbers language-json" data-language="json"><code class="language-json"><span class="token punctuation">&#123;</span>    <span class="token property">"name"</span><span class="token operator">:</span> <span class="token string">"Remote launch"</span><span class="token punctuation">,</span>    <span class="token property">"type"</span><span class="token operator">:</span> <span class="token string">"lldb"</span><span class="token punctuation">,</span>    <span class="token property">"request"</span><span class="token operator">:</span> <span class="token string">"launch"</span><span class="token punctuation">,</span>    <span class="token property">"program"</span><span class="token operator">:</span> <span class="token string">"$&#123;workspaceFolder&#125;/build/debuggee"</span><span class="token punctuation">,</span> <span class="token comment">// Local path. </span>    <span class="token property">"initCommands"</span><span class="token operator">:</span> <span class="token punctuation">[</span>        <span class="token string">"platform select &lt;platform>"</span><span class="token punctuation">,</span> <span class="token comment">// For example: 'remote-linux', 'remote-macosx', 'remote-android', etc.</span>        <span class="token string">"platform connect connect://&lt;remote_host>:&lt;port>"</span><span class="token punctuation">,</span>        <span class="token string">"settings set target.inherit-env false"</span><span class="token punctuation">,</span> <span class="token comment">// See note below.</span>    <span class="token punctuation">]</span><span class="token punctuation">,</span>    <span class="token property">"env"</span><span class="token operator">:</span> <span class="token punctuation">&#123;</span>        <span class="token property">"PATH"</span><span class="token operator">:</span> <span class="token string">"xxx"</span><span class="token punctuation">,</span> <span class="token comment">// remote 的path</span>        <span class="token property">"LD_LIBRARY_PATH"</span><span class="token operator">:</span> <span class="token string">"/data/local/tmp/code"</span>    <span class="token punctuation">&#125;</span><span class="token punctuation">,</span>    <span class="token property">"args"</span><span class="token operator">:</span><span class="token punctuation">[</span>        <span class="token string">"/data/local/tmp/code/pose.mnn"</span><span class="token punctuation">,</span>        <span class="token string">"/data/local/tmp/code/input.png"</span><span class="token punctuation">,</span>        <span class="token string">"/data/local/tmp/code/out.png"</span>    <span class="token punctuation">]</span><span class="token punctuation">,</span>    <span class="token property">"breakpointMode"</span><span class="token operator">:</span> <span class="token string">"file"</span><span class="token punctuation">&#125;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>有几个点需要注意<ul><li>program 是linux本地的路径,不是在remote手机上的路径</li><li>args 是程序运行时的参数,如果需要路径,是remote上的路径</li><li>env 配置的是remote上运行的env,比如链接库地址,PATH等</li><li>breakpointMode 设置为file就行</li></ul></li></ol><h2 id="建议"><a class="markdownIt-Anchor" href="#建议"></a> 建议</h2><ul><li>如果配置不成功,首先不使用vscode插件,在自己的电脑上命令行能够启动LLDB,LLDB-server和调试native c++程序.</li><li>codeLLDB 使用的是自己的lldb,需要检查该lldb能够正常运行和调试<br /><img src="https://s2.loli.net/2024/03/11/i1LOAze2saZJ6VK.png" alt="20240311142552" /></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;使用lldb-远程调试-安卓native-c-程序的vscode-配置&quot;&gt;&lt;a class=&quot;markdownIt-Anchor&quot; href=&quot;#使用lldb-远程调试-安卓native-c-程序的vscode-配置&quot;&gt;&lt;/a&gt; 使用LLDB 远程调试 安卓nat</summary>
      
    
    
    
    
    <category term="linux" scheme="https://weikangqi.github.io/tags/linux/"/>
    
    <category term="adb" scheme="https://weikangqi.github.io/tags/adb/"/>
    
    <category term="LLDB" scheme="https://weikangqi.github.io/tags/LLDB/"/>
    
    <category term="android" scheme="https://weikangqi.github.io/tags/android/"/>
    
    <category term="Debug" scheme="https://weikangqi.github.io/tags/Debug/"/>
    
  </entry>
  
  <entry>
    <title>Hello World</title>
    <link href="https://weikangqi.github.io/2013/12/hello-world/"/>
    <id>https://weikangqi.github.io/2013/12/hello-world/</id>
    <published>2013-12-02T15:30:16.000Z</published>
    <updated>2025-09-11T03:36:51.303Z</updated>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="quick-start"><a class="markdownIt-Anchor" href="#quick-start"></a> Quick Start</h2><h3 id="create-a-new-post"><a class="markdownIt-Anchor" href="#create-a-new-post"></a> Create a new post</h3><pre class="line-numbers language-cpp" data-language="cpp"><code class="language-cpp"><span class="token macro property"><span class="token directive-hash">#</span><span class="token directive keyword">include</span> <span class="token string">&lt;iostream></span></span><span class="token keyword">using</span> <span class="token keyword">namespace</span> std<span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="run-server"><a class="markdownIt-Anchor" href="#run-server"></a> Run server</h3><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">$ hexo server<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="generate-static-files"><a class="markdownIt-Anchor" href="#generate-static-files"></a> Generate static files</h3><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">$ hexo generate<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="deploy-to-remote-sites"><a class="markdownIt-Anchor" href="#deploy-to-remote-sites"></a> Deploy to remote sites</h3><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">$ hexo deploy<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><img src="https://raw.githubusercontent.com/weikangqi/picgo/main/lookahead-decoding.gif" alt="lookahead-decoding" style="zoom:50%;" /><p>![image-20250822002821998](C:</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;Welcome to &lt;a href=&quot;https://hexo.io/&quot;&gt;Hexo&lt;/a&gt;! This is your very first post. Check &lt;a href=&quot;https://hexo.io/docs/&quot;&gt;documentation&lt;/a&gt; for</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>大模型困惑度计算</title>
    <link href="https://weikangqi.github.io/2013/12/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%9F%BA%E7%A1%80/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BB%B7%E6%8C%87%E6%A0%87%E5%9B%B0%E6%83%91%E5%BA%A6/"/>
    <id>https://weikangqi.github.io/2013/12/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%9F%BA%E7%A1%80/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BB%B7%E6%8C%87%E6%A0%87%E5%9B%B0%E6%83%91%E5%BA%A6/</id>
    <published>2013-12-02T15:30:16.000Z</published>
    <updated>2025-09-11T03:36:51.303Z</updated>
    
    <content type="html"><![CDATA[<h2 id="困惑度perplexity"><a class="markdownIt-Anchor" href="#困惑度perplexity"></a> 困惑度（Perplexity）</h2><p><strong>困惑度（Perplexity, PPL）</strong> 是衡量语言模型质量的常用指标。它表示模型对下一词的预测不确定性，数值越低，表示模型的预测越准确。困惑度越高，表示模型对语言的理解越“困惑”。</p><h3 id="困惑度的计算公式"><a class="markdownIt-Anchor" href="#困惑度的计算公式"></a> 困惑度的计算公式</h3><p>给定一个词序列 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>w</mi><mn>1</mn></msub><mo separator="true">,</mo><msub><mi>w</mi><mn>2</mn></msub><mo separator="true">,</mo><mo>…</mo><mo separator="true">,</mo><msub><mi>w</mi><mi>N</mi></msub></mrow><annotation encoding="application/x-tex">w_1, w_2, \dots, w_N</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="minner">…</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.32833099999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.10903em;">N</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>，困惑度的计算公式为：</p><p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mtext>Perplexity</mtext><mo>=</mo><mi>exp</mi><mo>⁡</mo><mrow><mo fence="true">(</mo><mo>−</mo><mfrac><mn>1</mn><mi>N</mi></mfrac><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></munderover><mi>log</mi><mo>⁡</mo><mi>P</mi><mo stretchy="false">(</mo><msub><mi>w</mi><mi>i</mi></msub><mo>∣</mo><msub><mi>w</mi><mn>1</mn></msub><mo separator="true">,</mo><mo>…</mo><mo separator="true">,</mo><msub><mi>w</mi><mrow><mi>i</mi><mo>−</mo><mn>1</mn></mrow></msub><mo stretchy="false">)</mo><mo fence="true">)</mo></mrow></mrow><annotation encoding="application/x-tex">\text{Perplexity} = \exp \left( -\frac{1}{N} \sum_{i=1}^{N} \log P(w_i \mid w_1, \dots, w_{i-1}) \right)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="mord text"><span class="mord">Perplexity</span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:3.106005em;vertical-align:-1.277669em;"></span><span class="mop">exp</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="minner"><span class="mopen delimcenter" style="top:0em;"><span class="delimsizing size4">(</span></span><span class="mord">−</span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.32144em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.10903em;">N</span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.686em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.8283360000000002em;"><span style="top:-1.872331em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">i</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span style="top:-3.050005em;"><span class="pstrut" style="height:3.05em;"></span><span><span class="mop op-symbol large-op">∑</span></span></span><span style="top:-4.3000050000000005em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.10903em;">N</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.277669em;"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mop">lo<span style="margin-right:0.01389em;">g</span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">∣</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="minner">…</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.311664em;"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">i</span><span class="mbin mtight">−</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.208331em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mclose delimcenter" style="top:0em;"><span class="delimsizing size4">)</span></span></span></span></span></span></span></p><p>其中：</p><ul><li><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>N</mi></mrow><annotation encoding="application/x-tex">N</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.10903em;">N</span></span></span></span>表示句子中的词数。</li><li><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>P</mi><mo stretchy="false">(</mo><msub><mi>w</mi><mi>i</mi></msub><mo>∣</mo><msub><mi>w</mi><mn>1</mn></msub><mo separator="true">,</mo><mo>…</mo><mo separator="true">,</mo><msub><mi>w</mi><mrow><mi>i</mi><mo>−</mo><mn>1</mn></mrow></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">P(w_i \mid w_1, \dots, w_{i-1})</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">∣</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="minner">…</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.311664em;"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">i</span><span class="mbin mtight">−</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.208331em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span>是模型在给定上下文的条件下预测下一个词的概率。</li></ul><h3 id="例子计算困惑度"><a class="markdownIt-Anchor" href="#例子计算困惑度"></a> 例子：计算困惑度</h3><p>假设我们有一个句子：<br /><strong>&quot;The cat sat on the mat.&quot;</strong></p><p>语言模型对每个词的预测概率如下：</p><p><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>P</mi><mo stretchy="false">(</mo><mtext>&quot;The&quot;</mtext><mo stretchy="false">)</mo><mo>=</mo><mn>0.4</mn></mrow><annotation encoding="application/x-tex">P(\text{&quot;The&quot;}) = 0.4</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord text"><span class="mord">&quot;The&quot;</span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">0</span><span class="mord">.</span><span class="mord">4</span></span></span></span><br /><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>P</mi><mo stretchy="false">(</mo><mtext>&quot;cat&quot;</mtext><mo>∣</mo><mtext>&quot;The&quot;</mtext><mo stretchy="false">)</mo><mo>=</mo><mn>0.3</mn></mrow><annotation encoding="application/x-tex">P(\text{&quot;cat&quot;} \mid \text{&quot;The&quot;}) = 0.3</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord text"><span class="mord">&quot;cat&quot;</span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">∣</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord text"><span class="mord">&quot;The&quot;</span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">0</span><span class="mord">.</span><span class="mord">3</span></span></span></span><br /><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>P</mi><mo stretchy="false">(</mo><mtext>&quot;sat&quot;</mtext><mo>∣</mo><mtext>&quot;The cat&quot;</mtext><mo stretchy="false">)</mo><mo>=</mo><mn>0.2</mn></mrow><annotation encoding="application/x-tex">P(\text{&quot;sat&quot;} \mid \text{&quot;The cat&quot;}) = 0.2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord text"><span class="mord">&quot;sat&quot;</span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">∣</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord text"><span class="mord">&quot;The cat&quot;</span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">0</span><span class="mord">.</span><span class="mord">2</span></span></span></span><br /><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>P</mi><mo stretchy="false">(</mo><mtext>&quot;on&quot;</mtext><mo>∣</mo><mtext>&quot;The cat sat&quot;</mtext><mo stretchy="false">)</mo><mo>=</mo><mn>0.5</mn></mrow><annotation encoding="application/x-tex">P(\text{&quot;on&quot;} \mid \text{&quot;The cat sat&quot;}) = 0.5</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord text"><span class="mord">&quot;on&quot;</span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">∣</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord text"><span class="mord">&quot;The cat sat&quot;</span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">0</span><span class="mord">.</span><span class="mord">5</span></span></span></span><br /><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>P</mi><mo stretchy="false">(</mo><mtext>&quot;the&quot;</mtext><mo>∣</mo><mtext>&quot;The cat sat on&quot;</mtext><mo stretchy="false">)</mo><mo>=</mo><mn>0.7</mn></mrow><annotation encoding="application/x-tex">P(\text{&quot;the&quot;} \mid \text{&quot;The cat sat on&quot;}) = 0.7</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord text"><span class="mord">&quot;the&quot;</span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">∣</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord text"><span class="mord">&quot;The cat sat on&quot;</span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">0</span><span class="mord">.</span><span class="mord">7</span></span></span></span><br /><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>P</mi><mo stretchy="false">(</mo><mtext>&quot;mat&quot;</mtext><mo>∣</mo><mtext>&quot;The cat sat on the&quot;</mtext><mo stretchy="false">)</mo><mo>=</mo><mn>0.6</mn></mrow><annotation encoding="application/x-tex">P(\text{&quot;mat&quot;} \mid \text{&quot;The cat sat on the&quot;}) = 0.6</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord text"><span class="mord">&quot;mat&quot;</span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">∣</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord text"><span class="mord">&quot;The cat sat on the&quot;</span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">0</span><span class="mord">.</span><span class="mord">6</span></span></span></span></p><h4 id="第一步计算对数概率"><a class="markdownIt-Anchor" href="#第一步计算对数概率"></a> 第一步：计算对数概率</h4><p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>log</mi><mo>⁡</mo><mo stretchy="false">(</mo><mn>0.4</mn><mo stretchy="false">)</mo><mo>=</mo><mo>−</mo><mn>0.9163</mn></mrow><annotation encoding="application/x-tex">\log(0.4) = -0.9163</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mop">lo<span style="margin-right:0.01389em;">g</span></span><span class="mopen">(</span><span class="mord">0</span><span class="mord">.</span><span class="mord">4</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.72777em;vertical-align:-0.08333em;"></span><span class="mord">−</span><span class="mord">0</span><span class="mord">.</span><span class="mord">9</span><span class="mord">1</span><span class="mord">6</span><span class="mord">3</span></span></span></span></span></p><p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>log</mi><mo>⁡</mo><mo stretchy="false">(</mo><mn>0.3</mn><mo stretchy="false">)</mo><mo>=</mo><mo>−</mo><mn>1.204</mn></mrow><annotation encoding="application/x-tex">\log(0.3) = -1.204</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mop">lo<span style="margin-right:0.01389em;">g</span></span><span class="mopen">(</span><span class="mord">0</span><span class="mord">.</span><span class="mord">3</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.72777em;vertical-align:-0.08333em;"></span><span class="mord">−</span><span class="mord">1</span><span class="mord">.</span><span class="mord">2</span><span class="mord">0</span><span class="mord">4</span></span></span></span></span></p><p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>log</mi><mo>⁡</mo><mo stretchy="false">(</mo><mn>0.2</mn><mo stretchy="false">)</mo><mo>=</mo><mo>−</mo><mn>1.6094</mn></mrow><annotation encoding="application/x-tex">\log(0.2) = -1.6094</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mop">lo<span style="margin-right:0.01389em;">g</span></span><span class="mopen">(</span><span class="mord">0</span><span class="mord">.</span><span class="mord">2</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.72777em;vertical-align:-0.08333em;"></span><span class="mord">−</span><span class="mord">1</span><span class="mord">.</span><span class="mord">6</span><span class="mord">0</span><span class="mord">9</span><span class="mord">4</span></span></span></span></span></p><p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>log</mi><mo>⁡</mo><mo stretchy="false">(</mo><mn>0.5</mn><mo stretchy="false">)</mo><mo>=</mo><mo>−</mo><mn>0.6931</mn></mrow><annotation encoding="application/x-tex">\log(0.5) = -0.6931</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mop">lo<span style="margin-right:0.01389em;">g</span></span><span class="mopen">(</span><span class="mord">0</span><span class="mord">.</span><span class="mord">5</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.72777em;vertical-align:-0.08333em;"></span><span class="mord">−</span><span class="mord">0</span><span class="mord">.</span><span class="mord">6</span><span class="mord">9</span><span class="mord">3</span><span class="mord">1</span></span></span></span></span></p><p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>log</mi><mo>⁡</mo><mo stretchy="false">(</mo><mn>0.7</mn><mo stretchy="false">)</mo><mo>=</mo><mo>−</mo><mn>0.3567</mn></mrow><annotation encoding="application/x-tex">\log(0.7) = -0.3567</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mop">lo<span style="margin-right:0.01389em;">g</span></span><span class="mopen">(</span><span class="mord">0</span><span class="mord">.</span><span class="mord">7</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.72777em;vertical-align:-0.08333em;"></span><span class="mord">−</span><span class="mord">0</span><span class="mord">.</span><span class="mord">3</span><span class="mord">5</span><span class="mord">6</span><span class="mord">7</span></span></span></span></span></p><p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>log</mi><mo>⁡</mo><mo stretchy="false">(</mo><mn>0.6</mn><mo stretchy="false">)</mo><mo>=</mo><mo>−</mo><mn>0.5108</mn></mrow><annotation encoding="application/x-tex">\log(0.6) = -0.5108</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mop">lo<span style="margin-right:0.01389em;">g</span></span><span class="mopen">(</span><span class="mord">0</span><span class="mord">.</span><span class="mord">6</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.72777em;vertical-align:-0.08333em;"></span><span class="mord">−</span><span class="mord">0</span><span class="mord">.</span><span class="mord">5</span><span class="mord">1</span><span class="mord">0</span><span class="mord">8</span></span></span></span></span></p><h4 id="第二步计算总和并取平均值"><a class="markdownIt-Anchor" href="#第二步计算总和并取平均值"></a> 第二步：计算总和并取平均值</h4><p>将这些对数概率相加：</p><p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mo>−</mo><mn>0.9163</mn><mo>+</mo><mo>−</mo><mn>1.204</mn><mo>+</mo><mo>−</mo><mn>1.6094</mn><mo>+</mo><mo>−</mo><mn>0.6931</mn><mo>+</mo><mo>−</mo><mn>0.3567</mn><mo>+</mo><mo>−</mo><mn>0.5108</mn><mo>=</mo><mo>−</mo><mn>5.2903</mn></mrow><annotation encoding="application/x-tex">-0.9163 + -1.204 + -1.6094 + -0.6931 + -0.3567 + -0.5108 = -5.2903</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.72777em;vertical-align:-0.08333em;"></span><span class="mord">−</span><span class="mord">0</span><span class="mord">.</span><span class="mord">9</span><span class="mord">1</span><span class="mord">6</span><span class="mord">3</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.72777em;vertical-align:-0.08333em;"></span><span class="mord">−</span><span class="mord">1</span><span class="mord">.</span><span class="mord">2</span><span class="mord">0</span><span class="mord">4</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.72777em;vertical-align:-0.08333em;"></span><span class="mord">−</span><span class="mord">1</span><span class="mord">.</span><span class="mord">6</span><span class="mord">0</span><span class="mord">9</span><span class="mord">4</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.72777em;vertical-align:-0.08333em;"></span><span class="mord">−</span><span class="mord">0</span><span class="mord">.</span><span class="mord">6</span><span class="mord">9</span><span class="mord">3</span><span class="mord">1</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.72777em;vertical-align:-0.08333em;"></span><span class="mord">−</span><span class="mord">0</span><span class="mord">.</span><span class="mord">3</span><span class="mord">5</span><span class="mord">6</span><span class="mord">7</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.72777em;vertical-align:-0.08333em;"></span><span class="mord">−</span><span class="mord">0</span><span class="mord">.</span><span class="mord">5</span><span class="mord">1</span><span class="mord">0</span><span class="mord">8</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.72777em;vertical-align:-0.08333em;"></span><span class="mord">−</span><span class="mord">5</span><span class="mord">.</span><span class="mord">2</span><span class="mord">9</span><span class="mord">0</span><span class="mord">3</span></span></span></span></span></p><p>平均值为：</p><p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mfrac><mrow><mo>−</mo><mn>5.2903</mn></mrow><mn>6</mn></mfrac><mo>=</mo><mo>−</mo><mn>0.8817</mn></mrow><annotation encoding="application/x-tex">\frac{-5.2903}{6} = -0.8817</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:2.00744em;vertical-align:-0.686em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.32144em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">6</span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">−</span><span class="mord">5</span><span class="mord">.</span><span class="mord">2</span><span class="mord">9</span><span class="mord">0</span><span class="mord">3</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.686em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.72777em;vertical-align:-0.08333em;"></span><span class="mord">−</span><span class="mord">0</span><span class="mord">.</span><span class="mord">8</span><span class="mord">8</span><span class="mord">1</span><span class="mord">7</span></span></span></span></span></p><h4 id="第三步计算困惑度"><a class="markdownIt-Anchor" href="#第三步计算困惑度"></a> 第三步：计算困惑度</h4><p>最后，取平均值的指数：</p><p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mtext>Perplexity</mtext><mo>=</mo><mi>exp</mi><mo>⁡</mo><mo stretchy="false">(</mo><mn>0.8817</mn><mo stretchy="false">)</mo><mo>≈</mo><mn>2.414</mn></mrow><annotation encoding="application/x-tex">\text{Perplexity} = \exp(0.8817) \approx 2.414</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="mord text"><span class="mord">Perplexity</span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mop">exp</span><span class="mopen">(</span><span class="mord">0</span><span class="mord">.</span><span class="mord">8</span><span class="mord">8</span><span class="mord">1</span><span class="mord">7</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">≈</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">2</span><span class="mord">.</span><span class="mord">4</span><span class="mord">1</span><span class="mord">4</span></span></span></span></span></p><h3 id="结果分析"><a class="markdownIt-Anchor" href="#结果分析"></a> 结果分析</h3><p>困惑度为 <strong>2.414</strong>，表示该模型在预测下一个词时，平均有 2.4 个可能的词可以选择。数值越小，说明模型对词的预测越有把握。</p><h3 id="另一个模型的对比"><a class="markdownIt-Anchor" href="#另一个模型的对比"></a> 另一个模型的对比</h3><p>假设另一个模型给出的预测概率如下：<br /><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>P</mi><mo stretchy="false">(</mo><mtext>&quot;The&quot;</mtext><mo stretchy="false">)</mo><mo>=</mo><mn>0.6</mn></mrow><annotation encoding="application/x-tex">P(\text{&quot;The&quot;}) = 0.6</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord text"><span class="mord">&quot;The&quot;</span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">0</span><span class="mord">.</span><span class="mord">6</span></span></span></span><br /><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>P</mi><mo stretchy="false">(</mo><mtext>&quot;cat&quot;</mtext><mo>∣</mo><mtext>&quot;The&quot;</mtext><mo stretchy="false">)</mo><mo>=</mo><mn>0.5</mn></mrow><annotation encoding="application/x-tex">P(\text{&quot;cat&quot;} \mid \text{&quot;The&quot;}) = 0.5</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord text"><span class="mord">&quot;cat&quot;</span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">∣</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord text"><span class="mord">&quot;The&quot;</span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">0</span><span class="mord">.</span><span class="mord">5</span></span></span></span><br /><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>P</mi><mo stretchy="false">(</mo><mtext>&quot;sat&quot;</mtext><mo>∣</mo><mtext>&quot;The cat&quot;</mtext><mo stretchy="false">)</mo><mo>=</mo><mn>0.4</mn></mrow><annotation encoding="application/x-tex">P(\text{&quot;sat&quot;} \mid \text{&quot;The cat&quot;}) = 0.4</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord text"><span class="mord">&quot;sat&quot;</span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">∣</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord text"><span class="mord">&quot;The cat&quot;</span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">0</span><span class="mord">.</span><span class="mord">4</span></span></span></span><br /><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>P</mi><mo stretchy="false">(</mo><mtext>&quot;on&quot;</mtext><mo>∣</mo><mtext>&quot;The cat sat&quot;</mtext><mo stretchy="false">)</mo><mo>=</mo><mn>0.8</mn></mrow><annotation encoding="application/x-tex">P(\text{&quot;on&quot;} \mid \text{&quot;The cat sat&quot;}) = 0.8</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord text"><span class="mord">&quot;on&quot;</span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">∣</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord text"><span class="mord">&quot;The cat sat&quot;</span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">0</span><span class="mord">.</span><span class="mord">8</span></span></span></span><br /><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>P</mi><mo stretchy="false">(</mo><mtext>&quot;the&quot;</mtext><mo>∣</mo><mtext>&quot;The cat sat on&quot;</mtext><mo stretchy="false">)</mo><mo>=</mo><mn>0.9</mn></mrow><annotation encoding="application/x-tex">P(\text{&quot;the&quot;} \mid \text{&quot;The cat sat on&quot;}) = 0.9</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord text"><span class="mord">&quot;the&quot;</span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">∣</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord text"><span class="mord">&quot;The cat sat on&quot;</span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">0</span><span class="mord">.</span><span class="mord">9</span></span></span></span><br /><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>P</mi><mo stretchy="false">(</mo><mtext>&quot;mat&quot;</mtext><mo>∣</mo><mtext>&quot;The cat sat on the&quot;</mtext><mo stretchy="false">)</mo><mo>=</mo><mn>0.9</mn></mrow><annotation encoding="application/x-tex">P(\text{&quot;mat&quot;} \mid \text{&quot;The cat sat on the&quot;}) = 0.9</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord text"><span class="mord">&quot;mat&quot;</span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">∣</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord text"><span class="mord">&quot;The cat sat on the&quot;</span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">0</span><span class="mord">.</span><span class="mord">9</span></span></span></span></p><p>我们重新计算困惑度：</p><h4 id="第一步计算对数概率-2"><a class="markdownIt-Anchor" href="#第一步计算对数概率-2"></a> 第一步：计算对数概率</h4><p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>log</mi><mo>⁡</mo><mo stretchy="false">(</mo><mn>0.6</mn><mo stretchy="false">)</mo><mo>=</mo><mo>−</mo><mn>0.5108</mn></mrow><annotation encoding="application/x-tex">\log(0.6) = -0.5108</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mop">lo<span style="margin-right:0.01389em;">g</span></span><span class="mopen">(</span><span class="mord">0</span><span class="mord">.</span><span class="mord">6</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.72777em;vertical-align:-0.08333em;"></span><span class="mord">−</span><span class="mord">0</span><span class="mord">.</span><span class="mord">5</span><span class="mord">1</span><span class="mord">0</span><span class="mord">8</span></span></span></span></span></p><p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>log</mi><mo>⁡</mo><mo stretchy="false">(</mo><mn>0.5</mn><mo stretchy="false">)</mo><mo>=</mo><mo>−</mo><mn>0.6931</mn></mrow><annotation encoding="application/x-tex">\log(0.5) = -0.6931</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mop">lo<span style="margin-right:0.01389em;">g</span></span><span class="mopen">(</span><span class="mord">0</span><span class="mord">.</span><span class="mord">5</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.72777em;vertical-align:-0.08333em;"></span><span class="mord">−</span><span class="mord">0</span><span class="mord">.</span><span class="mord">6</span><span class="mord">9</span><span class="mord">3</span><span class="mord">1</span></span></span></span></span></p><p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>log</mi><mo>⁡</mo><mo stretchy="false">(</mo><mn>0.4</mn><mo stretchy="false">)</mo><mo>=</mo><mo>−</mo><mn>0.9163</mn></mrow><annotation encoding="application/x-tex">\log(0.4) = -0.9163</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mop">lo<span style="margin-right:0.01389em;">g</span></span><span class="mopen">(</span><span class="mord">0</span><span class="mord">.</span><span class="mord">4</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.72777em;vertical-align:-0.08333em;"></span><span class="mord">−</span><span class="mord">0</span><span class="mord">.</span><span class="mord">9</span><span class="mord">1</span><span class="mord">6</span><span class="mord">3</span></span></span></span></span></p><p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>log</mi><mo>⁡</mo><mo stretchy="false">(</mo><mn>0.8</mn><mo stretchy="false">)</mo><mo>=</mo><mo>−</mo><mn>0.2231</mn></mrow><annotation encoding="application/x-tex">\log(0.8) = -0.2231</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mop">lo<span style="margin-right:0.01389em;">g</span></span><span class="mopen">(</span><span class="mord">0</span><span class="mord">.</span><span class="mord">8</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.72777em;vertical-align:-0.08333em;"></span><span class="mord">−</span><span class="mord">0</span><span class="mord">.</span><span class="mord">2</span><span class="mord">2</span><span class="mord">3</span><span class="mord">1</span></span></span></span></span></p><p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>log</mi><mo>⁡</mo><mo stretchy="false">(</mo><mn>0.9</mn><mo stretchy="false">)</mo><mo>=</mo><mo>−</mo><mn>0.1054</mn></mrow><annotation encoding="application/x-tex">\log(0.9) = -0.1054</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mop">lo<span style="margin-right:0.01389em;">g</span></span><span class="mopen">(</span><span class="mord">0</span><span class="mord">.</span><span class="mord">9</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.72777em;vertical-align:-0.08333em;"></span><span class="mord">−</span><span class="mord">0</span><span class="mord">.</span><span class="mord">1</span><span class="mord">0</span><span class="mord">5</span><span class="mord">4</span></span></span></span></span></p><p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>log</mi><mo>⁡</mo><mo stretchy="false">(</mo><mn>0.95</mn><mo stretchy="false">)</mo><mo>=</mo><mo>−</mo><mn>0.0513</mn></mrow><annotation encoding="application/x-tex">\log(0.95) = -0.0513</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mop">lo<span style="margin-right:0.01389em;">g</span></span><span class="mopen">(</span><span class="mord">0</span><span class="mord">.</span><span class="mord">9</span><span class="mord">5</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.72777em;vertical-align:-0.08333em;"></span><span class="mord">−</span><span class="mord">0</span><span class="mord">.</span><span class="mord">0</span><span class="mord">5</span><span class="mord">1</span><span class="mord">3</span></span></span></span></span></p><h4 id="第二步计算总和并取平均值-2"><a class="markdownIt-Anchor" href="#第二步计算总和并取平均值-2"></a> 第二步：计算总和并取平均值</h4><p>相加：</p><p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mo>−</mo><mn>0.5108</mn><mo>+</mo><mo>−</mo><mn>0.6931</mn><mo>+</mo><mo>−</mo><mn>0.9163</mn><mo>+</mo><mo>−</mo><mn>0.2231</mn><mo>+</mo><mo>−</mo><mn>0.1054</mn><mo>+</mo><mo>−</mo><mn>0.0513</mn><mo>=</mo><mo>−</mo><mn>2.499</mn></mrow><annotation encoding="application/x-tex">-0.5108 + -0.6931 + -0.9163 + -0.2231 + -0.1054 + -0.0513 = -2.499</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.72777em;vertical-align:-0.08333em;"></span><span class="mord">−</span><span class="mord">0</span><span class="mord">.</span><span class="mord">5</span><span class="mord">1</span><span class="mord">0</span><span class="mord">8</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.72777em;vertical-align:-0.08333em;"></span><span class="mord">−</span><span class="mord">0</span><span class="mord">.</span><span class="mord">6</span><span class="mord">9</span><span class="mord">3</span><span class="mord">1</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.72777em;vertical-align:-0.08333em;"></span><span class="mord">−</span><span class="mord">0</span><span class="mord">.</span><span class="mord">9</span><span class="mord">1</span><span class="mord">6</span><span class="mord">3</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.72777em;vertical-align:-0.08333em;"></span><span class="mord">−</span><span class="mord">0</span><span class="mord">.</span><span class="mord">2</span><span class="mord">2</span><span class="mord">3</span><span class="mord">1</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.72777em;vertical-align:-0.08333em;"></span><span class="mord">−</span><span class="mord">0</span><span class="mord">.</span><span class="mord">1</span><span class="mord">0</span><span class="mord">5</span><span class="mord">4</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.72777em;vertical-align:-0.08333em;"></span><span class="mord">−</span><span class="mord">0</span><span class="mord">.</span><span class="mord">0</span><span class="mord">5</span><span class="mord">1</span><span class="mord">3</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.72777em;vertical-align:-0.08333em;"></span><span class="mord">−</span><span class="mord">2</span><span class="mord">.</span><span class="mord">4</span><span class="mord">9</span><span class="mord">9</span></span></span></span></span></p><p>平均值为：</p><p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mfrac><mrow><mo>−</mo><mn>2.499</mn></mrow><mn>6</mn></mfrac><mo>=</mo><mo>−</mo><mn>0.4165</mn></mrow><annotation encoding="application/x-tex">\frac{-2.499}{6} = -0.4165</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:2.00744em;vertical-align:-0.686em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.32144em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">6</span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">−</span><span class="mord">2</span><span class="mord">.</span><span class="mord">4</span><span class="mord">9</span><span class="mord">9</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.686em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.72777em;vertical-align:-0.08333em;"></span><span class="mord">−</span><span class="mord">0</span><span class="mord">.</span><span class="mord">4</span><span class="mord">1</span><span class="mord">6</span><span class="mord">5</span></span></span></span></span></p><h4 id="第三步计算困惑度-2"><a class="markdownIt-Anchor" href="#第三步计算困惑度-2"></a> 第三步：计算困惑度</h4><p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mtext>Perplexity</mtext><mo>=</mo><mi>exp</mi><mo>⁡</mo><mo stretchy="false">(</mo><mn>0.4165</mn><mo stretchy="false">)</mo><mo>≈</mo><mn>1.516</mn></mrow><annotation encoding="application/x-tex">\text{Perplexity} = \exp(0.4165) \approx 1.516</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="mord text"><span class="mord">Perplexity</span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mop">exp</span><span class="mopen">(</span><span class="mord">0</span><span class="mord">.</span><span class="mord">4</span><span class="mord">1</span><span class="mord">6</span><span class="mord">5</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">≈</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">1</span><span class="mord">.</span><span class="mord">5</span><span class="mord">1</span><span class="mord">6</span></span></span></span></span></p><h3 id="结果对比"><a class="markdownIt-Anchor" href="#结果对比"></a> 结果对比</h3><p>新模型的困惑度为 <strong>1.516</strong>，比之前的模型 <strong>2.414</strong> 更低，说明新模型对词序列的预测更加准确。</p><hr /><p>通过这个例子，可以看到困惑度越低，说明模型对语言的理解和预测越好。在语言模型的评估中，困惑度是衡量其预测能力的重要指标。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;困惑度perplexity&quot;&gt;&lt;a class=&quot;markdownIt-Anchor&quot; href=&quot;#困惑度perplexity&quot;&gt;&lt;/a&gt; 困惑度（Perplexity）&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;困惑度（Perplexity, PPL）&lt;/strong&gt;</summary>
      
    
    
    
    
    <category term="LLM" scheme="https://weikangqi.github.io/tags/LLM/"/>
    
  </entry>
  
</feed>
