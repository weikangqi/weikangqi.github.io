<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title></title>
      <link href="/2025/09/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%8E%A8%E7%90%86/MNN-LLM/"/>
      <url>/2025/09/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%8E%A8%E7%90%86/MNN-LLM/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>cuda 优化的性能的目标</title>
      <link href="/2025/09/cuda%20%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%E6%96%B9%E6%B3%95/"/>
      <url>/2025/09/cuda%20%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%E6%96%B9%E6%B3%95/</url>
      
        <content type="html"><![CDATA[<h1 id="优化的性能的目标"><a class="markdownIt-Anchor" href="#优化的性能的目标"></a> 优化的性能的目标</h1><ol><li>足够的并行度： thread 内部的 instructions 并行， warp 的并行（occupancy），SM 并行（更多的 block）</li><li>合并（coalesce）memory 访问：针对 global memory 的访问 pattern，warp 内线程访问要在一个 cache line（32 byte）内</li><li>warp 内的 thread 具有一致的 branch 分支。</li></ol><h1 id="衡量维度"><a class="markdownIt-Anchor" href="#衡量维度"></a> 衡量维度</h1><ol><li>memory bandwidth</li><li>instructions bandwidth</li><li>latency</li></ol><p><strong>Memory 维度</strong></p><p>Global memory： 每次以 32byte 的粒度读取到 cache line。warp 内针对 global memory 的 read 会合并。</p><p>可以关注 transaction per request，表示每次 warp 请求产生的 transaction 个数。主要是考虑到如果 warp 内访问 pattern 不合理会导致产生额外的 transaction，从而读取额外的数据，造成资源浪费。</p><p>shared memory： 比 global memory 快 10 倍，但是空间比较小。一般是 64K 32-bit。shared memory 在 block 内共享，所以一般用在 thread 可以复用的场景。对于需要在 thread 内做内存 transpose 的情况，也可以用 shared memory 缓存，提升访问速度。</p><p>使用 vector 数据类型，vector 数据类型（例如 float4, int4）等，可以使用一条指令读取或写入多个数。能够减少指令数，提升读写速度。但是在保证 access pattern 的情况下，使用 vector 也不会有质的提升。</p><h2 id="instructions-维度"><a class="markdownIt-Anchor" href="#instructions-维度"></a> Instructions 维度</h2><p>这里主要是单个 thread 内部的逻辑：</p><ol><li>增加指令的并行度，减小指令之间的依赖。通常这样会需要更多的 register。</li><li>另外 thread 内部尽量使用确定的 branch 条件，因为 branch 分支会导致 warp 分裂，使 warp 内的线程串行执行。降低并行度。</li><li>减小耗时的指令使用，例如使用 intrinsic 替换具体函数（精度会差一些，但是速度更快），使用单精度替换双精度，使用 flush to zero。</li><li>减小 thread 内部的 sync</li></ol><p>指令的 throughput 指的是单个 SM 内的 per cycle 指令数。是 num of operation per cycle 除以 32，因为 warp 执行一个指令相当于执行了 32 个 operation。</p><h2 id="latency-维度"><a class="markdownIt-Anchor" href="#latency-维度"></a> Latency 维度</h2><p>这里主要是 occupancy。thread 内的指令操作延迟可以通过更多的 thread 并行来隐藏掉。这样的指令延迟包括内存读取，所以如果是带宽瓶颈型 kernel，提高 occupancy 也可以提高性能。</p><p>occupancy 是一个静态值，根据 kernel 的 thread/block，register/thread，sharedmemory/block 来计算的，但实际上不会跑到理论最高值。可以把它看作是评价 kernel 静态配置的并行度指标。 Occupancy 则代表了, GPU 的 SM 上能驻留的线程数量(我今天在干的活的数量), 和该 SM 的最大能驻留的线程数量的(我累死最大能同时干的活的数量)的比值。这是 occupancy 的定义(实际上略微有差异, 特别是涉及到 achieved occupancy 的时候)。</p><p>另外提高并行度，除了把 register，thread 个数，sharedmemory 控制在 SM 的限制之内，还需要启动足够多的 block，从而来占用更多的 SM 计算资源。</p>]]></content>
      
      
      
        <tags>
            
            <tag> cuda </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>GGUF格式</title>
      <link href="/2025/08/GGUF/"/>
      <url>/2025/08/GGUF/</url>
      
        <content type="html"><![CDATA[<h1 id=""><a class="markdownIt-Anchor" href="#"></a> </h1><p>Hugging Face Hub 支持所有文件格式，但具有 GGUF 格式的内置功能，GGUF 是一种二进制格式，针对快速加载和保存模型进行了优化，使其能够高效地用于推理目的。 GGUF 设计用于与 GGML 和其他执行器一起使用。 GGUF 由 @ggerganov 开发，他也是流行的 C/C++ LLM 推理框架 llama.cpp 的开发者。最初在 PyTorch 等框架中开发的模型可以转换为 GGUF 格式，以便与这些引擎一起使用。</p><p><img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/gguf-spec.png" alt="来自huggin face" /></p><p>正如我们在此图中看到的，与仅张量的文件格式（例如 safetensors）不同（这也是 Hub 的推荐模型格式），GGUF 对张量和一组标准化元数据进行编码。</p><h2 id="quantization-types"><a class="markdownIt-Anchor" href="#quantization-types"></a> Quantization Types</h2><table><thead><tr><th>type</th><th>source</th><th style="text-align:left">description</th></tr></thead><tbody><tr><td>F64</td><td><a href="https://en.wikipedia.org/wiki/Double-precision_floating-point_format">Wikipedia</a></td><td style="text-align:left">64-bit standard IEEE 754 double-precision floating-point number.</td></tr><tr><td>I64</td><td><a href="https://github.com/ggerganov/llama.cpp/pull/6062">GH</a></td><td style="text-align:left">64-bit fixed-width integer number.</td></tr><tr><td>F32</td><td><a href="https://en.wikipedia.org/wiki/Single-precision_floating-point_format">Wikipedia</a></td><td style="text-align:left">32-bit standard IEEE 754 single-precision floating-point number.</td></tr><tr><td>I32</td><td><a href="https://github.com/ggerganov/llama.cpp/pull/6045">GH</a></td><td style="text-align:left">32-bit fixed-width integer number.</td></tr><tr><td>F16</td><td><a href="https://en.wikipedia.org/wiki/Half-precision_floating-point_format">Wikipedia</a></td><td style="text-align:left">16-bit standard IEEE 754 half-precision floating-point number.</td></tr><tr><td>BF16</td><td><a href="https://en.wikipedia.org/wiki/Bfloat16_floating-point_format">Wikipedia</a></td><td style="text-align:left">16-bit shortened version of the 32-bit IEEE 754 single-precision floating-point number.</td></tr><tr><td>I16</td><td><a href="https://github.com/ggerganov/llama.cpp/pull/6045">GH</a></td><td style="text-align:left">16-bit fixed-width integer number.</td></tr><tr><td>Q8_0</td><td><a href="https://github.com/huggingface/huggingface.js/pull/615#discussion_r1557654249">GH</a></td><td style="text-align:left">8-bit round-to-nearest quantization (<code>q</code>). Each block has 32 weights. Weight formula: <code>w = q * block_scale</code>. Legacy quantization method (not used widely as of today).</td></tr><tr><td>Q8_1</td><td><a href="https://github.com/huggingface/huggingface.js/pull/615#discussion_r1557682290">GH</a></td><td style="text-align:left">8-bit round-to-nearest quantization (<code>q</code>). Each block has 32 weights. Weight formula: <code>w = q * block_scale + block_minimum</code>. Legacy quantization method (not used widely as of today)</td></tr><tr><td>Q8_K</td><td><a href="https://github.com/ggerganov/llama.cpp/pull/1684#issue-1739619305">GH</a></td><td style="text-align:left">8-bit quantization (<code>q</code>). Each block has 256 weights. Only used for quantizing intermediate results. All 2-6 bit dot products are implemented for this quantization type. Weight formula: <code>w = q * block_scale</code>.</td></tr><tr><td>I8</td><td><a href="https://github.com/ggerganov/llama.cpp/pull/6045">GH</a></td><td style="text-align:left">8-bit fixed-width integer number.</td></tr><tr><td>Q6_K</td><td><a href="https://github.com/ggerganov/llama.cpp/pull/1684#issue-1739619305">GH</a></td><td style="text-align:left">6-bit quantization (<code>q</code>). Super-blocks with 16 blocks, each block has 16 weights. Weight formula: <code>w = q * block_scale(8-bit)</code>, resulting in 6.5625 bits-per-weight.</td></tr><tr><td>Q5_0</td><td><a href="https://github.com/huggingface/huggingface.js/pull/615#discussion_r1557654249">GH</a></td><td style="text-align:left">5-bit round-to-nearest quantization (<code>q</code>). Each block has 32 weights. Weight formula: <code>w = q * block_scale</code>. Legacy quantization method (not used widely as of today).</td></tr><tr><td>Q5_1</td><td><a href="https://github.com/huggingface/huggingface.js/pull/615#discussion_r1557682290">GH</a></td><td style="text-align:left">5-bit round-to-nearest quantization (<code>q</code>). Each block has 32 weights. Weight formula: <code>w = q * block_scale + block_minimum</code>. Legacy quantization method (not used widely as of today).</td></tr><tr><td>Q5_K</td><td><a href="https://github.com/ggerganov/llama.cpp/pull/1684#issue-1739619305">GH</a></td><td style="text-align:left">5-bit quantization (<code>q</code>). Super-blocks with 8 blocks, each block has 32 weights. Weight formula: <code>w = q * block_scale(6-bit) + block_min(6-bit)</code>, resulting in 5.5 bits-per-weight.</td></tr><tr><td>Q4_0</td><td><a href="https://github.com/huggingface/huggingface.js/pull/615#discussion_r1557654249">GH</a></td><td style="text-align:left">4-bit round-to-nearest quantization (<code>q</code>). Each block has 32 weights. Weight formula: <code>w = q * block_scale</code>. Legacy quantization method (not used widely as of today).</td></tr><tr><td>Q4_1</td><td><a href="https://github.com/huggingface/huggingface.js/pull/615#discussion_r1557682290">GH</a></td><td style="text-align:left">4-bit round-to-nearest quantization (<code>q</code>). Each block has 32 weights. Weight formula: <code>w = q * block_scale + block_minimum</code>. Legacy quantization method (not used widely as of today).</td></tr><tr><td>Q4_K</td><td><a href="https://github.com/ggerganov/llama.cpp/pull/1684#issue-1739619305">GH</a></td><td style="text-align:left">4-bit quantization (<code>q</code>). Super-blocks with 8 blocks, each block has 32 weights. Weight formula: <code>w = q * block_scale(6-bit) + block_min(6-bit)</code>, resulting in 4.5 bits-per-weight.</td></tr><tr><td>Q3_K</td><td><a href="https://github.com/ggerganov/llama.cpp/pull/1684#issue-1739619305">GH</a></td><td style="text-align:left">3-bit quantization (<code>q</code>). Super-blocks with 16 blocks, each block has 16 weights. Weight formula: <code>w = q * block_scale(6-bit)</code>, resulting. 3.4375 bits-per-weight.</td></tr><tr><td>Q2_K</td><td><a href="https://github.com/ggerganov/llama.cpp/pull/1684#issue-1739619305">GH</a></td><td style="text-align:left">2-bit quantization (<code>q</code>). Super-blocks with 16 blocks, each block has 16 weight. Weight formula: <code>w = q * block_scale(4-bit) + block_min(4-bit)</code>, resulting in 2.5625 bits-per-weight.</td></tr><tr><td>IQ4_NL</td><td><a href="https://github.com/ggerganov/llama.cpp/pull/5590">GH</a></td><td style="text-align:left">4-bit quantization (<code>q</code>). Super-blocks with 256 weights. Weight <code>w</code> is obtained using <code>super_block_scale</code> &amp; <code>importance matrix</code>.</td></tr><tr><td>IQ4_XS</td><td><a href="https://huggingface.co/CISCai/OpenCodeInterpreter-DS-6.7B-SOTA-GGUF/blob/main/README.md?code=true#L59-L70">HF</a></td><td style="text-align:left">4-bit quantization (<code>q</code>). Super-blocks with 256 weights. Weight <code>w</code> is obtained using <code>super_block_scale</code> &amp; <code>importance matrix</code>, resulting in 4.25 bits-per-weight.</td></tr><tr><td>IQ3_S</td><td><a href="https://huggingface.co/CISCai/OpenCodeInterpreter-DS-6.7B-SOTA-GGUF/blob/main/README.md?code=true#L59-L70">HF</a></td><td style="text-align:left">3-bit quantization (<code>q</code>). Super-blocks with 256 weights. Weight <code>w</code> is obtained using <code>super_block_scale</code> &amp; <code>importance matrix</code>, resulting in 3.44 bits-per-weight.</td></tr><tr><td>IQ3_XXS</td><td><a href="https://huggingface.co/CISCai/OpenCodeInterpreter-DS-6.7B-SOTA-GGUF/blob/main/README.md?code=true#L59-L70">HF</a></td><td style="text-align:left">3-bit quantization (<code>q</code>). Super-blocks with 256 weights. Weight <code>w</code> is obtained using <code>super_block_scale</code> &amp; <code>importance matrix</code>, resulting in 3.06 bits-per-weight.</td></tr><tr><td>IQ2_XXS</td><td><a href="https://huggingface.co/CISCai/OpenCodeInterpreter-DS-6.7B-SOTA-GGUF/blob/main/README.md?code=true#L59-L70">HF</a></td><td style="text-align:left">2-bit quantization (<code>q</code>). Super-blocks with 256 weights. Weight <code>w</code> is obtained using <code>super_block_scale</code> &amp; <code>importance matrix</code>, resulting in 2.06 bits-per-weight.</td></tr><tr><td>IQ2_S</td><td><a href="https://huggingface.co/CISCai/OpenCodeInterpreter-DS-6.7B-SOTA-GGUF/blob/main/README.md?code=true#L59-L70">HF</a></td><td style="text-align:left">2-bit quantization (<code>q</code>). Super-blocks with 256 weights. Weight <code>w</code> is obtained using <code>super_block_scale</code> &amp; <code>importance matrix</code>, resulting in 2.5 bits-per-weight.</td></tr><tr><td>IQ2_XS</td><td><a href="https://huggingface.co/CISCai/OpenCodeInterpreter-DS-6.7B-SOTA-GGUF/blob/main/README.md?code=true#L59-L70">HF</a></td><td style="text-align:left">2-bit quantization (<code>q</code>). Super-blocks with 256 weights. Weight <code>w</code> is obtained using <code>super_block_scale</code> &amp; <code>importance matrix</code>, resulting in 2.31 bits-per-weight.</td></tr><tr><td>IQ1_S</td><td><a href="https://huggingface.co/CISCai/OpenCodeInterpreter-DS-6.7B-SOTA-GGUF/blob/main/README.md?code=true#L59-L70">HF</a></td><td style="text-align:left">1-bit quantization (<code>q</code>). Super-blocks with 256 weights. Weight <code>w</code> is obtained using <code>super_block_scale</code> &amp; <code>importance matrix</code>, resulting in 1.56 bits-per-weight.</td></tr><tr><td>IQ1_M</td><td><a href="https://github.com/ggerganov/llama.cpp/pull/6302">GH</a></td><td style="text-align:left">1-bit quantization (<code>q</code>). Super-blocks with 256 weights. Weight <code>w</code> is obtained using <code>super_block_scale</code> &amp; <code>importance matrix</code>, resulting in 1.75 bits-per-weight.</td></tr></tbody></table><h2 id="provided-files"><a class="markdownIt-Anchor" href="#provided-files"></a> Provided files</h2><table><thead><tr><th>Name</th><th>Quant method</th><th>Bits</th><th>Size</th><th>Max RAM required</th><th>Use case</th></tr></thead><tbody><tr><td><a href="https://huggingface.co/TheBloke/Llama-2-13B-chat-GGML/blob/main/llama-2-13b-chat.ggmlv3.q2_K.bin">llama-2-13b-chat.ggmlv3.q2_K.bin</a></td><td>q2_K</td><td>2</td><td>5.51 GB</td><td>8.01 GB</td><td>New k-quant method. Uses GGML_TYPE_Q4_K for the attention.vw and feed_forward.w2 tensors, GGML_TYPE_Q2_K for the other tensors.</td></tr><tr><td><a href="https://huggingface.co/TheBloke/Llama-2-13B-chat-GGML/blob/main/llama-2-13b-chat.ggmlv3.q3_K_S.bin">llama-2-13b-chat.ggmlv3.q3_K_S.bin</a></td><td>q3_K_S</td><td>3</td><td>5.66 GB</td><td>8.16 GB</td><td>New k-quant method. Uses GGML_TYPE_Q3_K for all tensors</td></tr><tr><td><a href="https://huggingface.co/TheBloke/Llama-2-13B-chat-GGML/blob/main/llama-2-13b-chat.ggmlv3.q3_K_M.bin">llama-2-13b-chat.ggmlv3.q3_K_M.bin</a></td><td>q3_K_M</td><td>3</td><td>6.31 GB</td><td>8.81 GB</td><td>New k-quant method. Uses GGML_TYPE_Q4_K for the attention.wv, attention.wo, and feed_forward.w2 tensors, else GGML_TYPE_Q3_K</td></tr><tr><td><a href="https://huggingface.co/TheBloke/Llama-2-13B-chat-GGML/blob/main/llama-2-13b-chat.ggmlv3.q3_K_L.bin">llama-2-13b-chat.ggmlv3.q3_K_L.bin</a></td><td>q3_K_L</td><td>3</td><td>6.93 GB</td><td>9.43 GB</td><td>New k-quant method. Uses GGML_TYPE_Q5_K for the attention.wv, attention.wo, and feed_forward.w2 tensors, else GGML_TYPE_Q3_K</td></tr><tr><td><a href="https://huggingface.co/TheBloke/Llama-2-13B-chat-GGML/blob/main/llama-2-13b-chat.ggmlv3.q4_0.bin">llama-2-13b-chat.ggmlv3.q4_0.bin</a></td><td>q4_0</td><td>4</td><td>7.32 GB</td><td>9.82 GB</td><td>Original quant method, 4-bit.</td></tr><tr><td><a href="https://huggingface.co/TheBloke/Llama-2-13B-chat-GGML/blob/main/llama-2-13b-chat.ggmlv3.q4_K_S.bin">llama-2-13b-chat.ggmlv3.q4_K_S.bin</a></td><td>q4_K_S</td><td>4</td><td>7.37 GB</td><td>9.87 GB</td><td>New k-quant method. Uses GGML_TYPE_Q4_K for all tensors</td></tr><tr><td><a href="https://huggingface.co/TheBloke/Llama-2-13B-chat-GGML/blob/main/llama-2-13b-chat.ggmlv3.q4_K_M.bin">llama-2-13b-chat.ggmlv3.q4_K_M.bin</a></td><td>q4_K_M</td><td>4</td><td>7.87 GB</td><td>10.37 GB</td><td>New k-quant method. Uses GGML_TYPE_Q6_K for half of the attention.wv and feed_forward.w2 tensors, else GGML_TYPE_Q4_K</td></tr><tr><td><a href="https://huggingface.co/TheBloke/Llama-2-13B-chat-GGML/blob/main/llama-2-13b-chat.ggmlv3.q4_1.bin">llama-2-13b-chat.ggmlv3.q4_1.bin</a></td><td>q4_1</td><td>4</td><td>8.14 GB</td><td>10.64 GB</td><td>Original quant method, 4-bit. Higher accuracy than q4_0 but not as high as q5_0. However has quicker inference than q5 models.</td></tr><tr><td><a href="https://huggingface.co/TheBloke/Llama-2-13B-chat-GGML/blob/main/llama-2-13b-chat.ggmlv3.q5_0.bin">llama-2-13b-chat.ggmlv3.q5_0.bin</a></td><td>q5_0</td><td>5</td><td>8.95 GB</td><td>11.45 GB</td><td>Original quant method, 5-bit. Higher accuracy, higher resource usage and slower inference.</td></tr><tr><td><a href="https://huggingface.co/TheBloke/Llama-2-13B-chat-GGML/blob/main/llama-2-13b-chat.ggmlv3.q5_K_S.bin">llama-2-13b-chat.ggmlv3.q5_K_S.bin</a></td><td>q5_K_S</td><td>5</td><td>8.97 GB</td><td>11.47 GB</td><td>New k-quant method. Uses GGML_TYPE_Q5_K for all tensors</td></tr><tr><td><a href="https://huggingface.co/TheBloke/Llama-2-13B-chat-GGML/blob/main/llama-2-13b-chat.ggmlv3.q5_K_M.bin">llama-2-13b-chat.ggmlv3.q5_K_M.bin</a></td><td>q5_K_M</td><td>5</td><td>9.23 GB</td><td>11.73 GB</td><td>New k-quant method. Uses GGML_TYPE_Q6_K for half of the attention.wv and feed_forward.w2 tensors, else GGML_TYPE_Q5_K</td></tr><tr><td><a href="https://huggingface.co/TheBloke/Llama-2-13B-chat-GGML/blob/main/llama-2-13b-chat.ggmlv3.q5_1.bin">llama-2-13b-chat.ggmlv3.q5_1.bin</a></td><td>q5_1</td><td>5</td><td>9.76 GB</td><td>12.26 GB</td><td>Original quant method, 5-bit. Even higher accuracy, resource usage and slower inference.</td></tr><tr><td><a href="https://huggingface.co/TheBloke/Llama-2-13B-chat-GGML/blob/main/llama-2-13b-chat.ggmlv3.q6_K.bin">llama-2-13b-chat.ggmlv3.q6_K.bin</a></td><td>q6_K</td><td>6</td><td>10.68 GB</td><td>13.18 GB</td><td>New k-quant method. Uses GGML_TYPE_Q8_K for all tensors - 6-bit quantization</td></tr><tr><td><a href="https://huggingface.co/TheBloke/Llama-2-13B-chat-GGML/blob/main/llama-2-13b-chat.ggmlv3.q8_0.bin">llama-2-13b-chat.ggmlv3.q8_0.bin</a></td><td>q8_0</td><td>8</td><td>13.83 GB</td><td>16.33 GB</td><td>Original quant method, 8-bit. Almost indistinguishable from float16. High resource use and slow. Not recommended for most users.</td></tr></tbody></table><p><strong>Note</strong>: the above RAM figures assume no GPU offloading. If layers are offloaded to the GPU, this will reduce RAM usage and use VRAM instead.</p>]]></content>
      
      
      
        <tags>
            
            <tag> LLM </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>MNN fp32 &lt;--&gt; fp8</title>
      <link href="/2025/08/MNN%20fp32-fp8/"/>
      <url>/2025/08/MNN%20fp32-fp8/</url>
      
        <content type="html"><![CDATA[<pre class="line-numbers language-Cpp" data-language="Cpp"><code class="language-Cpp">static const int FP32_EXP_BIAS &#x3D; 127;static const int FP8_EXP_BIAS &#x3D; 24;   &#x2F;&#x2F; [0, 31] --&gt; [-24, 7] --&gt; [1 &#x2F; 2^24, 2^7]void MNNFp32ToFp8(uint8_t* dst, const float* src, size_t size) &#123;    for (int i &#x3D; 0; i &lt; size; i++) &#123;        &#x2F;&#x2F; 1. 获取 FP32 的二进制表示（IEEE 754 32-bit）        &#x2F;&#x2F; float 在内存中占 4 字节，将其 reinterpret 成 uint32_t 方便位操作        uint32_t rawData &#x3D; *((uint32_t *)(&amp;src[i]));        &#x2F;&#x2F; 2. 提取符号位 S（最高位 31）        &#x2F;&#x2F; 0 表示正数，1 表示负数        uint32_t sign &#x3D; (rawData &gt;&gt; 31) &amp; 1U;        &#x2F;&#x2F; 3. 提取指数位 E (8 bit)        &#x2F;&#x2F; FP32 的指数在 bit 23~30        uint32_t exp &#x3D; (rawData &gt;&gt; 23) &amp; 0xFFU;        &#x2F;&#x2F; 4. 提取尾数位 M        &#x2F;&#x2F; FP32 尾数有 23 bit，这里只取高 2 bit 来适配 FP8(s1e5m2)        uint32_t mant &#x3D; (rawData &gt;&gt; 21) &amp; 0x3U;        &#x2F;&#x2F; 5. 计算 FP32 的真实指数        &#x2F;&#x2F; FP32 的 Bias &#x3D; 127，减去 Bias 得到真实指数        int realExp &#x3D; (int)exp - FP32_EXP_BIAS;        &#x2F;&#x2F; 6. 指数范围截断（clamp）        &#x2F;&#x2F; FP8(s1e5m2) 指数范围 [-15,16]，Bias &#x3D; 15        &#x2F;&#x2F; 下限截断：如果 realExp 小于 -15，就设置为 -15        realExp &#x3D; ALIMAX(realExp, 0 - FP8_EXP_BIAS);        &#x2F;&#x2F; 上限截断：如果 realExp 大于 16，就设置为 16        realExp &#x3D; ALIMIN(realExp, 31 - FP8_EXP_BIAS);        &#x2F;&#x2F; 7. 加回 FP8 Bias，得到 FP8 的指数 E&#39;        exp &#x3D; (uint32_t)(realExp + FP8_EXP_BIAS);        &#x2F;&#x2F; 8. 拼接 FP8 (8 bit)        &#x2F;&#x2F; FP8 格式： [1bit sign | 5bit exponent | 2bit mantissa]        dst[i] &#x3D; (int8_t)((sign &lt;&lt; 7) | (exp &lt;&lt; 2) | mant);       &#125;&#125;void MNNFp8ToFp32(float* dst, const uint8_t* src, size_t size) &#123;    for (int i &#x3D; 0; i &lt; size; i++) &#123;        uint32_t sign &#x3D; (src[i] &gt;&gt; 7) &amp; 1U;        uint32_t exp &#x3D; (int)((src[i] &gt;&gt; 2) &amp; 0x1fU);        uint32_t mant &#x3D; (src[i] &amp; 3U) &lt;&lt; 21;        int realExp &#x3D; (int)exp - FP8_EXP_BIAS;        exp &#x3D; (uint32_t)(realExp + FP32_EXP_BIAS);        uint32_t rawData &#x3D; (sign &lt;&lt; 31) | (exp &lt;&lt; 23) | mant;        dst[i] &#x3D; *((float *)(&amp;rawData));    &#125;&#125;&#x2F;&#x2F; fp16 &lt;--&gt; fp8void MNNFp16ToFp8(uint8_t* dst, const uint16_t* src, size_t size) &#123;#ifdef MNN_USE_NEON#ifdef __aarch64__    int loopN &#x3D; size &#x2F; 16;    for (int i &#x3D; 0; i &lt; loopN; i++) &#123;        uint8x16_t v1 &#x3D; vld1q_u8((uint8_t*)(src + i * 16));        uint8x16_t v2 &#x3D; vld1q_u8((uint8_t*)(src + i * 16 + 8));        uint8x16_t res &#x3D; vuzp2q_u8(v1, v2);        vst1q_u8(dst + i * 16, res);    &#125;    for (int i &#x3D; loopN * 16; i &lt; size; i++) &#123;        dst[i] &#x3D; static_cast&lt;int8_t&gt;(src[i] &gt;&gt; 8);    &#125;#else    int loopN &#x3D; size &#x2F; 8;    for (int i &#x3D; 0; i &lt; loopN; i++) &#123;        uint16x8_t vec &#x3D; vld1q_u16(src + i * 8);        uint8x8_t  res &#x3D; vshrn_n_u16(vec, 8);        vst1_u8(dst + i * 8, res);    &#125;    for (int i &#x3D; loopN * 8; i &lt; size; i++) &#123;        dst[i] &#x3D; static_cast&lt;int8_t&gt;(src[i] &gt;&gt; 8);    &#125;#endif &#x2F;&#x2F; ARM64#else    for (int i &#x3D; 0; i &lt; size; i++) &#123;        dst[i] &#x3D; static_cast&lt;int8_t&gt;(src[i] &gt;&gt; 8);    &#125;#endif &#x2F;&#x2F; USE_NEON&#125;void MNNFp8ToFp16(uint16_t* dst, const uint8_t* src, size_t size) &#123;#ifdef MNN_USE_NEON    int loopN &#x3D; size &#x2F; 8;    for (int i &#x3D; 0; i &lt; loopN; i++) &#123;        uint8x8_t vec8x8 &#x3D; vld1_u8(src + i * 8);        uint16x8_t vec16x8 &#x3D; vshll_n_u8(vec8x8, 8);        vst1q_u16(dst + i * 8, vec16x8);    &#125;    for (int i &#x3D; loopN * 8; i &lt; size; i++) &#123;        dst[i] &#x3D; static_cast&lt;int16_t&gt;(src[i]) &lt;&lt; 8;    &#125;#else    for (int i &#x3D; 0; i &lt; size; i++) &#123;        dst[i] &#x3D; static_cast&lt;int16_t&gt;(src[i]) &lt;&lt; 8;    &#125;#endif &#x2F;&#x2F; USE_NEON&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>]]></content>
      
      
      
        <tags>
            
            <tag> ARM </tag>
            
            <tag> MNN </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>gdb 调试多线程</title>
      <link href="/2025/08/gdb%E8%B0%83%E8%AF%95%E5%A4%9A%E7%BA%BF%E7%A8%8B%E6%96%87%E4%BB%B6/"/>
      <url>/2025/08/gdb%E8%B0%83%E8%AF%95%E5%A4%9A%E7%BA%BF%E7%A8%8B%E6%96%87%E4%BB%B6/</url>
      
        <content type="html"><![CDATA[<h2 id="多线程程序"><a class="markdownIt-Anchor" href="#多线程程序"></a> 多线程程序</h2><pre class="line-numbers language-Cpp" data-language="Cpp"><code class="language-Cpp">#include &lt;stdio.h&gt;#include &lt;unistd.h&gt;#include &lt;pthread.h&gt;void* thread_entry_funcA(void* arg)&#123;    int i &#x3D; 0;    for(; i &lt; 10; i++)&#123;        printf(&quot;[thread_entry_funcA]: %d\n&quot;, i);        sleep(1);    &#125;    return NULL;&#125;void* thread_entry_funcB(void* arg)&#123;    int i &#x3D; 0;    for(; i &lt; 10; i++)&#123;        printf(&quot;[thread_entry_funcB]: %d\n&quot;, i);        sleep(1);    &#125;    return NULL;&#125;int main()&#123;    pthread_t tidA, tidB;    int ret &#x3D; pthread_create(&amp;tidA, NULL, thread_entry_funcA, NULL);    if(ret &lt; 0)&#123;        perror(&quot;pthread_create&quot;);        return 0;    &#125;    ret &#x3D; pthread_create(&amp;tidB, NULL, thread_entry_funcB, NULL);    if(ret &lt; 0)&#123;        perror(&quot;pthread_create&quot;);        return 0;    &#125;    pthread_join(tidA, NULL);    pthread_join(tidB, NULL);    return 0;&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>编译</p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">gcc thread_gdb.c <span class="token parameter variable">-o</span> thread_gdb <span class="token parameter variable">-lpthread</span> <span class="token parameter variable">-g</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p><code>pstack</code>命令 观察多线程</p><h2 id="查看多线程信息"><a class="markdownIt-Anchor" href="#查看多线程信息"></a> 查看多线程信息</h2><pre class="line-numbers language-none"><code class="language-none">(gdb) info threads<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h2 id="切换线程"><a class="markdownIt-Anchor" href="#切换线程"></a> 切换线程</h2><pre class="line-numbers language-none"><code class="language-none">t [线程ID]<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p><img src="https://s2.loli.net/2024/05/20/oSielhwcb14jIms.png" alt="image-20240520213157159" /></p><h2 id="切换到每一层堆栈"><a class="markdownIt-Anchor" href="#切换到每一层堆栈"></a> 切换到每一层堆栈</h2><h4 id="查看调用堆栈"><a class="markdownIt-Anchor" href="#查看调用堆栈"></a> 查看调用堆栈</h4><p>打印堆栈信息</p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">bt <span class="token comment">#查看调用堆栈</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h4 id="切换堆栈"><a class="markdownIt-Anchor" href="#切换堆栈"></a> 切换堆栈</h4><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">f <span class="token number">0</span> <span class="token comment"># 切换到0号堆栈</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><img src="https://s2.loli.net/2024/05/20/L6JSKsy1nmHUfzZ.png" alt="image-20240520213456485" style="zoom:50%;" /><p>这时<code>(gdb) n</code>会让所有线程继续运行。我只想让一个线程继续运行，使用<em>调度器锁模式</em></p><h2 id="调度器锁模式"><a class="markdownIt-Anchor" href="#调度器锁模式"></a> 调度器锁模式</h2><p>调试时除了当前线程在运行，想要规定其他线程的运行情况， 可以使用**set scheduler-locking [mode]**命令来进行设置</p><ol><li><p><code>set scheduler-locking off</code> 不锁定任何线程，所有线程都可以继续执行。</p></li><li><p><code>set scheduler-locking on</code> 只有当前线程可以执行，其他线程暂停运行。</p></li><li><p><code>set scheduler-locking step</code></p><ul><li>当单步执行某一线程时，保证在调试过程中当前线程不会发生切换（B线程在调试，A线程也会运行，但是不会A运行就会切换到A）。其他线程也会随着被调试线程的单步执行而执行。</li><li>但如果该模式下执行 continue、until、finish 命令，则其它线程也会执行，并且如果某一线程执行过程遇到断点，则 GDB 调试器会将该线程作为当前线程。</li></ul><p>必须在线程创建之后，在一开始设置是不成功的</p></li></ol><pre class="line-numbers language-none"><code class="language-none">show scheduler-locking<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>]]></content>
      
      
      
        <tags>
            
            <tag> 多线程 </tag>
            
            <tag> gdb </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Lookahead</title>
      <link href="/2025/07/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%8E%A8%E7%90%86/Lookahead/"/>
      <url>/2025/07/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%8E%A8%E7%90%86/Lookahead/</url>
      
        <content type="html"><![CDATA[<p>Jacobi 迭代把自回归的N次迭代转换为N个方程，然后联合求解。而 <a href="https://zhida.zhihu.com/search?content_id=257559026&amp;content_type=Article&amp;match_order=1&amp;q=Jacobi+Decoding&amp;zhida_source=entity">Jacobi Decoding</a> 将每次迭代上一次的输出整体作为下一次的输入，其实就是把每一个 token 上的输出视作一个 2-gram，并以此作为Draft Model。论文“Break the Sequential Dependency of LLM Inference Using Lookahead Decoding”的作者想到，如果可以记录下更多的历史信息，就可以制造一个 N-gram 作为 Draft Model，这样就能提高 Speculative Decoding 的准确率。这就是Lookahead Decoding。简要来说，Lookahead=N-gram+Jacobi iteration+parallel verification，其利用 jacobi 迭代法同时提取和验证 n-grams，打破自回归解码的顺序依赖性，从而降低解码次数，实现推理加速。相比之前的并行解码，Lookahead Decoding即不需要草稿模型，也不需要像Medusa那样微调head。论文作者将 Jacobi Decoding 视为Lookahead Decoding在 2-gram 情况下的特例。</p><img src="https://github.com/hao-ai-lab/LookaheadDecoding/raw/main/media/lookahead-decoding.gif" alt="img" style="zoom: 33%;" /><p>为了加速解码过程，每个Lookahead Decoding步骤被分为两个并行分支：生成n-gram的lookahead分支和验证n-gram的verification分支，两者都在一个前向传播过程中执行。</p><ul><li>Lookahead（前瞻）分支：这是原始雅可比解码的过程。因为不一致性的问题，此过程不会被用作主要投机验证的机制，而是作为一种采样收集或者说生成 n-gram 的并行解码过程。Lookahead 分支的目的是生成新的 N-Grams，加上其中新生成的 Token 就可以用于构建下一次 Verify 分支的候选序列。</li><li>Verification（验证）分支：这个分支从n-gram集合中匹配的多个candidates作为投机验证输入，完成具体的投机采样过程。verification分支会选择并验证有希望的 n-gram ，并且会将其用于更新下一次 Lookahead 分支的序列。</li></ul><h2 id="推理"><a class="markdownIt-Anchor" href="#推理"></a> 推理</h2><img src="https://raw.githubusercontent.com/weikangqi/picgo/main/image-20250902163700561.png" alt="image-20250902163700561" style="zoom:50%;" /><p>推理时，由于 LLM 解码主要受内存带宽限制，因此我们在同一步骤中合并前瞻和验证分支，利用 GPU 的并行处理能力来隐藏开销。 mask就是并行解码的关键。本示例的mask具体如上图所示。</p><h2 id="prepare-for-next-iteration"><a class="markdownIt-Anchor" href="#prepare-for-next-iteration"></a> <strong>Prepare for next iteration</strong></h2><p>当前迭代结束之后，会为下一次迭代做好准备。具体是：</p><ul><li>更新 2D Window。用后一层替代前一层（最后一层由最新输出得到的logits填充），并根据被接受的 tokens 的数量截断每一层。在当前的序列中随机采样填充被截断的部分。</li><li>更新 n-gram。如何生成新的n-gram？其实就是就是在2d-window里面从上往下找。</li><li>更新下一次前向的 Attention Mask 和 KV Cache。假设接受了 k 个 tokens，就据此扩展 Attention Mask，并将这 k 个 tokens 的 cache 拼接到 KV Cache 上。</li><li>当满足退出条件，例如生成的 tokens 长度达到<code>max_length</code>时，返回结果。</li></ul><img src="https://raw.githubusercontent.com/weikangqi/picgo/main/lookahead-perf.png" alt="img" style="zoom: 33%;" />]]></content>
      
      
      
        <tags>
            
            <tag> LLM </tag>
            
            <tag> 投机解码 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Jacobi Decoding</title>
      <link href="/2025/07/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%8E%A8%E7%90%86/jacobi%20decoding/"/>
      <url>/2025/07/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%8E%A8%E7%90%86/jacobi%20decoding/</url>
      
        <content type="html"><![CDATA[<h1 id="jacobi-decoding"><a class="markdownIt-Anchor" href="#jacobi-decoding"></a> Jacobi Decoding</h1><p>在神经机器翻译（NMT）中，Transformer 模型已成为主流。然而，虽然 Transformer 在训练阶段可以高度并行化，但在推理阶段却依赖<strong>自回归解码</strong>（autoregressive decoding），即一次生成一个 token，每个新 token 又依赖前面已经生成的 token。这种<strong>顺序依赖</strong>导致推理速度成为瓶颈，特别是在需要低延迟的实际应用中。以往的研究多集中在 <strong>非自回归翻译（NAT）</strong>，它可以一次性并行生成整句翻译。但 NAT 方法往往需要重新设计模型结构、消耗大量训练资源，而且翻译质量通常会有所下降。</p><p>这篇 ACL 2023 的论文提出了一条<strong>全新的路径</strong>：与其重新训练模型，不如<strong>改变解码算法本身</strong>，让现有的自回归模型也能“并行解码”。</p><img src="https://raw.githubusercontent.com/weikangqi/picgo/main/equations.png" alt="img" style="zoom: 50%;" /><p>Jacobi Decoding 就是寻求更少的迭代，寻找方程组的解（fixed point）。Jacobi decoding 在生产上，目前还没有相对自回归获得比较大的加速比。可以查考下面的流程：</p><img src="https://raw.githubusercontent.com/weikangqi/picgo/main/jacobi-iteration.gif" alt="img" style="zoom: 33%;" /><p>从这个图可以看到，Jacobi decoding每次迭代更新多个token，每次计算输出步长是M，可以作为一个batch并行的推理，相对单步耗时是增加的，但是对于是访存受限型的来讲，如果accept率可以上升还是又加速效果的。</p><ol><li>PJ：以句子为单位，每次处理一个句子</li><li>PGJ：以字词为单位，每次处理一个字词</li><li>HGJ：PGJ 的基础上，增加EOS 字符中途快速退出的兼容</li></ol><img src="https://raw.githubusercontent.com/weikangqi/picgo/main/image-20250902112750988.png" alt="image-20250902112750988" style="zoom:50%;" /><h2 id="初始化guess-token"><a class="markdownIt-Anchor" href="#初始化guess-token"></a> 初始化Guess Token</h2><p>如果没有特别初始化，默认Guess Token填为&lt;pad&gt;</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">JacobiDecoder</span><span class="token punctuation">(</span>MTDecoder<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> tokenizer<span class="token punctuation">,</span> model<span class="token punctuation">,</span> initializer<span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span>tokenizer<span class="token punctuation">,</span> model<span class="token punctuation">,</span> initializer<span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>name <span class="token operator">=</span> <span class="token string">"jacobi"</span>        self<span class="token punctuation">.</span>acronym <span class="token operator">=</span> <span class="token string">"j"</span>    <span class="token decorator annotation punctuation">@torch<span class="token punctuation">.</span>no_grad</span><span class="token punctuation">(</span><span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">decode</span><span class="token punctuation">(</span>        self<span class="token punctuation">,</span>        input_ids<span class="token punctuation">,</span>        attention_mask<span class="token punctuation">,</span>        target_len<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span>        gold_target<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span>        init_tensor<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span>        compute_ddg<span class="token punctuation">:</span> <span class="token builtin">bool</span> <span class="token operator">=</span> <span class="token boolean">False</span><span class="token punctuation">,</span>        logits_preprocessor<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span>        <span class="token operator">*</span>args<span class="token punctuation">,</span>        <span class="token operator">**</span>kwargs    <span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">if</span> init_tensor <span class="token keyword">is</span> <span class="token boolean">None</span><span class="token punctuation">:</span>            init_tensor <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span>                <span class="token punctuation">[</span>self<span class="token punctuation">.</span>pad_token_id<span class="token punctuation">]</span> <span class="token operator">*</span> input_ids<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span> <span class="token operator">*</span> max_length<span class="token punctuation">,</span>                device<span class="token operator">=</span>self<span class="token punctuation">.</span>device<span class="token punctuation">,</span>            <span class="token punctuation">)</span><span class="token punctuation">.</span>reshape<span class="token punctuation">(</span>input_ids<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">,</span> max_length<span class="token punctuation">)</span>        <span class="token keyword">elif</span> self<span class="token punctuation">.</span>is_mbart<span class="token punctuation">:</span> <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>    <span class="token keyword">def</span> <span class="token function">initialize</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> init_transl<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">if</span> self<span class="token punctuation">.</span>initializer <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>            init_tensor<span class="token punctuation">,</span> _ <span class="token operator">=</span> self<span class="token punctuation">.</span>initializer<span class="token punctuation">.</span>init_translation<span class="token punctuation">(</span>init_transl<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span>        <span class="token keyword">else</span><span class="token punctuation">:</span>            init_tensor <span class="token operator">=</span> <span class="token boolean">None</span>        <span class="token keyword">return</span> init_tensor    <span class="token keyword">def</span> <span class="token function">compute_decode_kwargs</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> input_ids<span class="token punctuation">,</span> attention_mask<span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span><span class="token punctuation">:</span>        gold_autoregressive <span class="token operator">=</span> self<span class="token punctuation">.</span>generate_gold_autoregressive<span class="token punctuation">(</span>input_ids<span class="token punctuation">,</span> attention_mask<span class="token punctuation">)</span>        init_tensor <span class="token operator">=</span> self<span class="token punctuation">.</span>initialize<span class="token punctuation">(</span>init_transl<span class="token operator">=</span>gold_autoregressive<span class="token punctuation">)</span>        logits_preprocessor <span class="token operator">=</span> self<span class="token punctuation">.</span>generate_logits_preprocessor<span class="token punctuation">(</span>input_ids<span class="token punctuation">)</span>        <span class="token keyword">return</span> <span class="token punctuation">&#123;</span>            <span class="token string">"init_tensor"</span><span class="token punctuation">:</span> init_tensor<span class="token punctuation">.</span>clone<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            <span class="token string">"target_len"</span><span class="token punctuation">:</span> gold_autoregressive<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span>            <span class="token string">"gold_target"</span><span class="token punctuation">:</span> gold_autoregressive<span class="token punctuation">,</span>            <span class="token string">"logits_preprocessor"</span><span class="token punctuation">:</span> logits_preprocessor        <span class="token punctuation">&#125;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>]]></content>
      
      
      
        <tags>
            
            <tag> LLM </tag>
            
            <tag> 投机解码 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>QLora</title>
      <link href="/2025/07/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E9%87%8F%E5%8C%96/QLora/"/>
      <url>/2025/07/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E9%87%8F%E5%8C%96/QLora/</url>
      
        <content type="html"><![CDATA[<h1 id="qlora"><a class="markdownIt-Anchor" href="#qlora"></a> <strong>QLora</strong></h1><p><a href="https://zhuanlan.zhihu.com/p/666234324">zhuanlan.zhihu.com/…</a></p><p>QLora属于非线性量化，使用NF4量化策略。基于分块的分位数量化的量化策略；其次是双重量化，包含对普通参数的一次量化和对量化常数的再一次量化，可以进一步减小缓存占用；具有k-比特的有损最小熵编码具有如下特性：<strong>当将输入数据进行量化时，每个可能的k-bit的整数值出现的频率是相等的（k-bit 如4bit，16个整型数字出现的频率一样，所有数字被均匀分配到这16个数字，这样具有最小损失）</strong>。如果我们粗暴的使用round操作去低精度的更近的值，我们可能造成大量的数据都被量化到同一个数上，这样特征之间的差异性在量化过程中就被丢失了。</p><h3 id="分位数量化"><a class="markdownIt-Anchor" href="#分位数量化"></a> <strong>分位数量化</strong></h3><p>首先将参数进行分布统计，以下是正态分布图</p><p>​            <img src="https://raw.githubusercontent.com/weikangqi/picgo/main/MTMxMDI2NjQyOTU5MTk3NjM_330702_ND9XoBvHEZaNxz_U_1754895845" alt="img" style="zoom: 33%;" /></p><p>​</p><p>x轴代表了参数范围，y轴代表了出现的概率，如果我们要量化成4bit，那么x轴应该分成16份。每一份的面积相等。下一步需要确定分为数值。预训练模型的参数往往是符合正态分布的，累积分布函数的反函数来简化分位数计算。</p><p>以下是累计分布函数，以4bit为例，讲y轴分成16份，求出x轴坐标。</p><p>​            <img src="https://raw.githubusercontent.com/weikangqi/picgo/main/MTMxMDI2NjQyOTU5MTk3NjM_358076_FNsq37BJqEx0ESBM_1754898669" alt="img" style="zoom: 33%;" /></p><p>​</p><p>i是指bin的边界（0 - 1）有17，16个区间，使用16个点来代表。每个点的值是两个边界的平均值。于是可以得到qi。0和1的CDF的反函数的解分别是负无穷和正无穷，因此我们不能将0和1代入式(3)。为了解决这个问题，我们可以设置一个偏移（offset）位。使用偏移位后我们计算的区间不再是[0,1]，而是[1-offerset,offset]</p><p>量化步骤</p><ol><li>对参数进行归一化，x/x_max</li><li>对参数找到最接近的qi</li><li>将i 整型代表浮点数，存储到张量中</li></ol><h3 id="分块"><a class="markdownIt-Anchor" href="#分块"></a> 分块</h3><p>分块k位量化的策略是通过将张量分成若干个块，让每个块都有独立的量化常数c，从而解决模型参数的极大极小的异常值的问题。分块量化的另外一个好处是减少了核之间的通信，可以实现更好的并行性，并充分利用硬件的多核的能力。</p><h3 id="存储"><a class="markdownIt-Anchor" href="#存储"></a> 存储</h3><p>需要存储一个归一化参数C  =x_max，以及一张整型到浮点数的映射表。</p><h3 id="双重量化"><a class="markdownIt-Anchor" href="#双重量化"></a> 双重量化</h3><p>双重量化就是除了上面的量化再量化一次，归一化参数C，可以进行8bit量化减小存储。</p><p>累计分布函数CDF：x轴如下从小到大表示数据分布，y轴代表了累计分布的概率</p><p>​            <img src="https://raw.githubusercontent.com/weikangqi/picgo/main/MTMxMDI2NjQyOTU5MTk3NjM_220726_46YxVCtYclWW7bHO_1754896591" alt="img" style="zoom: 23%;" /></p><p>​</p><p>逆累计分布函数ICDF</p><p>所以“逆累积分布函数”的意思其实是“反累积分布函数”</p><p><strong>累积分布：分位点－&gt;概率，</strong></p><p><strong>逆累积分布：概率－&gt;分位点。</strong></p>]]></content>
      
      
      
        <tags>
            
            <tag> LLM </tag>
            
            <tag> 量化 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>LLama3 Python</title>
      <link href="/2025/05/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%9F%BA%E7%A1%80/LLama3/"/>
      <url>/2025/05/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%9F%BA%E7%A1%80/LLama3/</url>
      
        <content type="html"><![CDATA[<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token comment"># Copyright (c) Meta Platforms, Inc. and affiliates.</span><span class="token comment"># This software may be used and distributed in accordance with the terms of the Llama 3 Community License Agreement.</span><span class="token keyword">import</span> math<span class="token keyword">from</span> dataclasses <span class="token keyword">import</span> dataclass<span class="token keyword">from</span> typing <span class="token keyword">import</span> Optional<span class="token punctuation">,</span> Tuple<span class="token keyword">import</span> fairscale<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>model_parallel<span class="token punctuation">.</span>initialize <span class="token keyword">as</span> fs_init<span class="token keyword">import</span> torch<span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>functional <span class="token keyword">as</span> F<span class="token keyword">from</span> fairscale<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>model_parallel<span class="token punctuation">.</span>layers <span class="token keyword">import</span> <span class="token punctuation">(</span>    ColumnParallelLinear<span class="token punctuation">,</span>    RowParallelLinear<span class="token punctuation">,</span>    VocabParallelEmbedding<span class="token punctuation">,</span><span class="token punctuation">)</span><span class="token keyword">from</span> torch <span class="token keyword">import</span> nn<span class="token decorator annotation punctuation">@dataclass</span><span class="token keyword">class</span> <span class="token class-name">ModelArgs</span><span class="token punctuation">:</span>    dim<span class="token punctuation">:</span> <span class="token builtin">int</span> <span class="token operator">=</span> <span class="token number">4096</span>    n_layers<span class="token punctuation">:</span> <span class="token builtin">int</span> <span class="token operator">=</span> <span class="token number">32</span>    n_heads<span class="token punctuation">:</span> <span class="token builtin">int</span> <span class="token operator">=</span> <span class="token number">32</span>    n_kv_heads<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span><span class="token builtin">int</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span>    vocab_size<span class="token punctuation">:</span> <span class="token builtin">int</span> <span class="token operator">=</span> <span class="token operator">-</span><span class="token number">1</span>    multiple_of<span class="token punctuation">:</span> <span class="token builtin">int</span> <span class="token operator">=</span> <span class="token number">256</span>  <span class="token comment"># make SwiGLU hidden layer size multiple of large power of 2</span>    ffn_dim_multiplier<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span><span class="token builtin">float</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span>    norm_eps<span class="token punctuation">:</span> <span class="token builtin">float</span> <span class="token operator">=</span> <span class="token number">1e-5</span>    rope_theta<span class="token punctuation">:</span> <span class="token builtin">float</span> <span class="token operator">=</span> <span class="token number">500000</span>    max_batch_size<span class="token punctuation">:</span> <span class="token builtin">int</span> <span class="token operator">=</span> <span class="token number">32</span>    max_seq_len<span class="token punctuation">:</span> <span class="token builtin">int</span> <span class="token operator">=</span> <span class="token number">2048</span><span class="token keyword">class</span> <span class="token class-name">RMSNorm</span><span class="token punctuation">(</span>torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> dim<span class="token punctuation">:</span> <span class="token builtin">int</span><span class="token punctuation">,</span> eps<span class="token punctuation">:</span> <span class="token builtin">float</span> <span class="token operator">=</span> <span class="token number">1e-6</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>eps <span class="token operator">=</span> eps        self<span class="token punctuation">.</span>weight <span class="token operator">=</span> nn<span class="token punctuation">.</span>Parameter<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>ones<span class="token punctuation">(</span>dim<span class="token punctuation">)</span><span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">_norm</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">return</span> x <span class="token operator">*</span> torch<span class="token punctuation">.</span>rsqrt<span class="token punctuation">(</span>x<span class="token punctuation">.</span><span class="token builtin">pow</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">.</span>mean<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> keepdim<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span> <span class="token operator">+</span> self<span class="token punctuation">.</span>eps<span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>        output <span class="token operator">=</span> self<span class="token punctuation">.</span>_norm<span class="token punctuation">(</span>x<span class="token punctuation">.</span><span class="token builtin">float</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span>type_as<span class="token punctuation">(</span>x<span class="token punctuation">)</span>        <span class="token keyword">return</span> output <span class="token operator">*</span> self<span class="token punctuation">.</span>weight<span class="token keyword">def</span> <span class="token function">precompute_freqs_cis</span><span class="token punctuation">(</span>dim<span class="token punctuation">:</span> <span class="token builtin">int</span><span class="token punctuation">,</span> end<span class="token punctuation">:</span> <span class="token builtin">int</span><span class="token punctuation">,</span> theta<span class="token punctuation">:</span> <span class="token builtin">float</span> <span class="token operator">=</span> <span class="token number">10000.0</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    freqs <span class="token operator">=</span> <span class="token number">1.0</span> <span class="token operator">/</span> <span class="token punctuation">(</span>theta <span class="token operator">**</span> <span class="token punctuation">(</span>torch<span class="token punctuation">.</span>arange<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> dim<span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token punctuation">:</span> <span class="token punctuation">(</span>dim <span class="token operator">//</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">.</span><span class="token builtin">float</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">/</span> dim<span class="token punctuation">)</span><span class="token punctuation">)</span>    t <span class="token operator">=</span> torch<span class="token punctuation">.</span>arange<span class="token punctuation">(</span>end<span class="token punctuation">,</span> device<span class="token operator">=</span>freqs<span class="token punctuation">.</span>device<span class="token punctuation">,</span> dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span>float32<span class="token punctuation">)</span>    freqs <span class="token operator">=</span> torch<span class="token punctuation">.</span>outer<span class="token punctuation">(</span>t<span class="token punctuation">,</span> freqs<span class="token punctuation">)</span>    freqs_cis <span class="token operator">=</span> torch<span class="token punctuation">.</span>polar<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>ones_like<span class="token punctuation">(</span>freqs<span class="token punctuation">)</span><span class="token punctuation">,</span> freqs<span class="token punctuation">)</span>  <span class="token comment"># complex64</span>    <span class="token keyword">return</span> freqs_cis<span class="token keyword">def</span> <span class="token function">reshape_for_broadcast</span><span class="token punctuation">(</span>freqs_cis<span class="token punctuation">:</span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">,</span> x<span class="token punctuation">:</span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">)</span><span class="token punctuation">:</span>    ndim <span class="token operator">=</span> x<span class="token punctuation">.</span>ndim    <span class="token keyword">assert</span> <span class="token number">0</span> <span class="token operator">&lt;=</span> <span class="token number">1</span> <span class="token operator">&lt;</span> ndim    <span class="token keyword">assert</span> freqs_cis<span class="token punctuation">.</span>shape <span class="token operator">==</span> <span class="token punctuation">(</span>x<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> x<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span>    shape <span class="token operator">=</span> <span class="token punctuation">[</span>d <span class="token keyword">if</span> i <span class="token operator">==</span> <span class="token number">1</span> <span class="token keyword">or</span> i <span class="token operator">==</span> ndim <span class="token operator">-</span> <span class="token number">1</span> <span class="token keyword">else</span> <span class="token number">1</span> <span class="token keyword">for</span> i<span class="token punctuation">,</span> d <span class="token keyword">in</span> <span class="token builtin">enumerate</span><span class="token punctuation">(</span>x<span class="token punctuation">.</span>shape<span class="token punctuation">)</span><span class="token punctuation">]</span>    <span class="token keyword">return</span> freqs_cis<span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token operator">*</span>shape<span class="token punctuation">)</span><span class="token keyword">def</span> <span class="token function">apply_rotary_emb</span><span class="token punctuation">(</span>    xq<span class="token punctuation">:</span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">,</span>    xk<span class="token punctuation">:</span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">,</span>    freqs_cis<span class="token punctuation">:</span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">,</span><span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> Tuple<span class="token punctuation">[</span>torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">,</span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">]</span><span class="token punctuation">:</span>    xq_ <span class="token operator">=</span> torch<span class="token punctuation">.</span>view_as_complex<span class="token punctuation">(</span>xq<span class="token punctuation">.</span><span class="token builtin">float</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>reshape<span class="token punctuation">(</span><span class="token operator">*</span>xq<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span>    xk_ <span class="token operator">=</span> torch<span class="token punctuation">.</span>view_as_complex<span class="token punctuation">(</span>xk<span class="token punctuation">.</span><span class="token builtin">float</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>reshape<span class="token punctuation">(</span><span class="token operator">*</span>xk<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span>    freqs_cis <span class="token operator">=</span> reshape_for_broadcast<span class="token punctuation">(</span>freqs_cis<span class="token punctuation">,</span> xq_<span class="token punctuation">)</span>    xq_out <span class="token operator">=</span> torch<span class="token punctuation">.</span>view_as_real<span class="token punctuation">(</span>xq_ <span class="token operator">*</span> freqs_cis<span class="token punctuation">)</span><span class="token punctuation">.</span>flatten<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">)</span>    xk_out <span class="token operator">=</span> torch<span class="token punctuation">.</span>view_as_real<span class="token punctuation">(</span>xk_ <span class="token operator">*</span> freqs_cis<span class="token punctuation">)</span><span class="token punctuation">.</span>flatten<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">)</span>    <span class="token keyword">return</span> xq_out<span class="token punctuation">.</span>type_as<span class="token punctuation">(</span>xq<span class="token punctuation">)</span><span class="token punctuation">,</span> xk_out<span class="token punctuation">.</span>type_as<span class="token punctuation">(</span>xk<span class="token punctuation">)</span><span class="token keyword">def</span> <span class="token function">repeat_kv</span><span class="token punctuation">(</span>x<span class="token punctuation">:</span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">,</span> n_rep<span class="token punctuation">:</span> <span class="token builtin">int</span><span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">:</span>    <span class="token triple-quoted-string string">"""torch.repeat_interleave(x, dim=2, repeats=n_rep)"""</span>    bs<span class="token punctuation">,</span> slen<span class="token punctuation">,</span> n_kv_heads<span class="token punctuation">,</span> head_dim <span class="token operator">=</span> x<span class="token punctuation">.</span>shape    <span class="token keyword">if</span> n_rep <span class="token operator">==</span> <span class="token number">1</span><span class="token punctuation">:</span>        <span class="token keyword">return</span> x    <span class="token keyword">return</span> <span class="token punctuation">(</span>        x<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token boolean">None</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span>        <span class="token punctuation">.</span>expand<span class="token punctuation">(</span>bs<span class="token punctuation">,</span> slen<span class="token punctuation">,</span> n_kv_heads<span class="token punctuation">,</span> n_rep<span class="token punctuation">,</span> head_dim<span class="token punctuation">)</span>        <span class="token punctuation">.</span>reshape<span class="token punctuation">(</span>bs<span class="token punctuation">,</span> slen<span class="token punctuation">,</span> n_kv_heads <span class="token operator">*</span> n_rep<span class="token punctuation">,</span> head_dim<span class="token punctuation">)</span>    <span class="token punctuation">)</span><span class="token keyword">class</span> <span class="token class-name">Attention</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> args<span class="token punctuation">:</span> ModelArgs<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>n_kv_heads <span class="token operator">=</span> args<span class="token punctuation">.</span>n_heads <span class="token keyword">if</span> args<span class="token punctuation">.</span>n_kv_heads <span class="token keyword">is</span> <span class="token boolean">None</span> <span class="token keyword">else</span> args<span class="token punctuation">.</span>n_kv_heads        model_parallel_size <span class="token operator">=</span> fs_init<span class="token punctuation">.</span>get_model_parallel_world_size<span class="token punctuation">(</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>n_local_heads <span class="token operator">=</span> args<span class="token punctuation">.</span>n_heads <span class="token operator">//</span> model_parallel_size        self<span class="token punctuation">.</span>n_local_kv_heads <span class="token operator">=</span> self<span class="token punctuation">.</span>n_kv_heads <span class="token operator">//</span> model_parallel_size        self<span class="token punctuation">.</span>n_rep <span class="token operator">=</span> self<span class="token punctuation">.</span>n_local_heads <span class="token operator">//</span> self<span class="token punctuation">.</span>n_local_kv_heads        self<span class="token punctuation">.</span>head_dim <span class="token operator">=</span> args<span class="token punctuation">.</span>dim <span class="token operator">//</span> args<span class="token punctuation">.</span>n_heads        self<span class="token punctuation">.</span>wq <span class="token operator">=</span> ColumnParallelLinear<span class="token punctuation">(</span>            args<span class="token punctuation">.</span>dim<span class="token punctuation">,</span>            args<span class="token punctuation">.</span>n_heads <span class="token operator">*</span> self<span class="token punctuation">.</span>head_dim<span class="token punctuation">,</span>            bias<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span>            gather_output<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span>            init_method<span class="token operator">=</span><span class="token keyword">lambda</span> x<span class="token punctuation">:</span> x<span class="token punctuation">,</span>        <span class="token punctuation">)</span>        self<span class="token punctuation">.</span>wk <span class="token operator">=</span> ColumnParallelLinear<span class="token punctuation">(</span>            args<span class="token punctuation">.</span>dim<span class="token punctuation">,</span>            self<span class="token punctuation">.</span>n_kv_heads <span class="token operator">*</span> self<span class="token punctuation">.</span>head_dim<span class="token punctuation">,</span>            bias<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span>            gather_output<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span>            init_method<span class="token operator">=</span><span class="token keyword">lambda</span> x<span class="token punctuation">:</span> x<span class="token punctuation">,</span>        <span class="token punctuation">)</span>        self<span class="token punctuation">.</span>wv <span class="token operator">=</span> ColumnParallelLinear<span class="token punctuation">(</span>            args<span class="token punctuation">.</span>dim<span class="token punctuation">,</span>            self<span class="token punctuation">.</span>n_kv_heads <span class="token operator">*</span> self<span class="token punctuation">.</span>head_dim<span class="token punctuation">,</span>            bias<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span>            gather_output<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span>            init_method<span class="token operator">=</span><span class="token keyword">lambda</span> x<span class="token punctuation">:</span> x<span class="token punctuation">,</span>        <span class="token punctuation">)</span>        self<span class="token punctuation">.</span>wo <span class="token operator">=</span> RowParallelLinear<span class="token punctuation">(</span>            args<span class="token punctuation">.</span>n_heads <span class="token operator">*</span> self<span class="token punctuation">.</span>head_dim<span class="token punctuation">,</span>            args<span class="token punctuation">.</span>dim<span class="token punctuation">,</span>            bias<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span>            input_is_parallel<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>            init_method<span class="token operator">=</span><span class="token keyword">lambda</span> x<span class="token punctuation">:</span> x<span class="token punctuation">,</span>        <span class="token punctuation">)</span>        self<span class="token punctuation">.</span>cache_k <span class="token operator">=</span> torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span>            <span class="token punctuation">(</span>                args<span class="token punctuation">.</span>max_batch_size<span class="token punctuation">,</span>                args<span class="token punctuation">.</span>max_seq_len<span class="token punctuation">,</span>                self<span class="token punctuation">.</span>n_local_kv_heads<span class="token punctuation">,</span>                self<span class="token punctuation">.</span>head_dim<span class="token punctuation">,</span>            <span class="token punctuation">)</span>        <span class="token punctuation">)</span><span class="token punctuation">.</span>cuda<span class="token punctuation">(</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>cache_v <span class="token operator">=</span> torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span>            <span class="token punctuation">(</span>                args<span class="token punctuation">.</span>max_batch_size<span class="token punctuation">,</span>                args<span class="token punctuation">.</span>max_seq_len<span class="token punctuation">,</span>                self<span class="token punctuation">.</span>n_local_kv_heads<span class="token punctuation">,</span>                self<span class="token punctuation">.</span>head_dim<span class="token punctuation">,</span>            <span class="token punctuation">)</span>        <span class="token punctuation">)</span><span class="token punctuation">.</span>cuda<span class="token punctuation">(</span><span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>        self<span class="token punctuation">,</span>        x<span class="token punctuation">:</span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">,</span>        start_pos<span class="token punctuation">:</span> <span class="token builtin">int</span><span class="token punctuation">,</span>        freqs_cis<span class="token punctuation">:</span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">,</span>        mask<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">]</span><span class="token punctuation">,</span>    <span class="token punctuation">)</span><span class="token punctuation">:</span>        bsz<span class="token punctuation">,</span> seqlen<span class="token punctuation">,</span> _ <span class="token operator">=</span> x<span class="token punctuation">.</span>shape        xq<span class="token punctuation">,</span> xk<span class="token punctuation">,</span> xv <span class="token operator">=</span> self<span class="token punctuation">.</span>wq<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>wk<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>wv<span class="token punctuation">(</span>x<span class="token punctuation">)</span>        xq <span class="token operator">=</span> xq<span class="token punctuation">.</span>view<span class="token punctuation">(</span>bsz<span class="token punctuation">,</span> seqlen<span class="token punctuation">,</span> self<span class="token punctuation">.</span>n_local_heads<span class="token punctuation">,</span> self<span class="token punctuation">.</span>head_dim<span class="token punctuation">)</span>        xk <span class="token operator">=</span> xk<span class="token punctuation">.</span>view<span class="token punctuation">(</span>bsz<span class="token punctuation">,</span> seqlen<span class="token punctuation">,</span> self<span class="token punctuation">.</span>n_local_kv_heads<span class="token punctuation">,</span> self<span class="token punctuation">.</span>head_dim<span class="token punctuation">)</span>        xv <span class="token operator">=</span> xv<span class="token punctuation">.</span>view<span class="token punctuation">(</span>bsz<span class="token punctuation">,</span> seqlen<span class="token punctuation">,</span> self<span class="token punctuation">.</span>n_local_kv_heads<span class="token punctuation">,</span> self<span class="token punctuation">.</span>head_dim<span class="token punctuation">)</span>        xq<span class="token punctuation">,</span> xk <span class="token operator">=</span> apply_rotary_emb<span class="token punctuation">(</span>xq<span class="token punctuation">,</span> xk<span class="token punctuation">,</span> freqs_cis<span class="token operator">=</span>freqs_cis<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>cache_k <span class="token operator">=</span> self<span class="token punctuation">.</span>cache_k<span class="token punctuation">.</span>to<span class="token punctuation">(</span>xq<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>cache_v <span class="token operator">=</span> self<span class="token punctuation">.</span>cache_v<span class="token punctuation">.</span>to<span class="token punctuation">(</span>xq<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>cache_k<span class="token punctuation">[</span><span class="token punctuation">:</span>bsz<span class="token punctuation">,</span> start_pos <span class="token punctuation">:</span> start_pos <span class="token operator">+</span> seqlen<span class="token punctuation">]</span> <span class="token operator">=</span> xk        self<span class="token punctuation">.</span>cache_v<span class="token punctuation">[</span><span class="token punctuation">:</span>bsz<span class="token punctuation">,</span> start_pos <span class="token punctuation">:</span> start_pos <span class="token operator">+</span> seqlen<span class="token punctuation">]</span> <span class="token operator">=</span> xv        keys <span class="token operator">=</span> self<span class="token punctuation">.</span>cache_k<span class="token punctuation">[</span><span class="token punctuation">:</span>bsz<span class="token punctuation">,</span> <span class="token punctuation">:</span> start_pos <span class="token operator">+</span> seqlen<span class="token punctuation">]</span>        values <span class="token operator">=</span> self<span class="token punctuation">.</span>cache_v<span class="token punctuation">[</span><span class="token punctuation">:</span>bsz<span class="token punctuation">,</span> <span class="token punctuation">:</span> start_pos <span class="token operator">+</span> seqlen<span class="token punctuation">]</span>        <span class="token comment"># repeat k/v heads if n_kv_heads &lt; n_heads</span>        keys <span class="token operator">=</span> repeat_kv<span class="token punctuation">(</span>            keys<span class="token punctuation">,</span> self<span class="token punctuation">.</span>n_rep        <span class="token punctuation">)</span>  <span class="token comment"># (bs, cache_len + seqlen, n_local_heads, head_dim)</span>        values <span class="token operator">=</span> repeat_kv<span class="token punctuation">(</span>            values<span class="token punctuation">,</span> self<span class="token punctuation">.</span>n_rep        <span class="token punctuation">)</span>  <span class="token comment"># (bs, cache_len + seqlen, n_local_heads, head_dim)</span>        xq <span class="token operator">=</span> xq<span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>  <span class="token comment"># (bs, n_local_heads, seqlen, head_dim)</span>        keys <span class="token operator">=</span> keys<span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>  <span class="token comment"># (bs, n_local_heads, cache_len + seqlen, head_dim)</span>        values <span class="token operator">=</span> values<span class="token punctuation">.</span>transpose<span class="token punctuation">(</span>            <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span>        <span class="token punctuation">)</span>  <span class="token comment"># (bs, n_local_heads, cache_len + seqlen, head_dim)</span>        scores <span class="token operator">=</span> torch<span class="token punctuation">.</span>matmul<span class="token punctuation">(</span>xq<span class="token punctuation">,</span> keys<span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token operator">/</span> math<span class="token punctuation">.</span>sqrt<span class="token punctuation">(</span>self<span class="token punctuation">.</span>head_dim<span class="token punctuation">)</span>        <span class="token keyword">if</span> mask <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>            scores <span class="token operator">=</span> scores <span class="token operator">+</span> mask  <span class="token comment"># (bs, n_local_heads, seqlen, cache_len + seqlen)</span>        scores <span class="token operator">=</span> F<span class="token punctuation">.</span>softmax<span class="token punctuation">(</span>scores<span class="token punctuation">.</span><span class="token builtin">float</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span>type_as<span class="token punctuation">(</span>xq<span class="token punctuation">)</span>        output <span class="token operator">=</span> torch<span class="token punctuation">.</span>matmul<span class="token punctuation">(</span>scores<span class="token punctuation">,</span> values<span class="token punctuation">)</span>  <span class="token comment"># (bs, n_local_heads, seqlen, head_dim)</span>        output <span class="token operator">=</span> output<span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">.</span>contiguous<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>view<span class="token punctuation">(</span>bsz<span class="token punctuation">,</span> seqlen<span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>        <span class="token keyword">return</span> self<span class="token punctuation">.</span>wo<span class="token punctuation">(</span>output<span class="token punctuation">)</span><span class="token keyword">class</span> <span class="token class-name">FeedForward</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>        self<span class="token punctuation">,</span>        dim<span class="token punctuation">:</span> <span class="token builtin">int</span><span class="token punctuation">,</span>        hidden_dim<span class="token punctuation">:</span> <span class="token builtin">int</span><span class="token punctuation">,</span>        multiple_of<span class="token punctuation">:</span> <span class="token builtin">int</span><span class="token punctuation">,</span>        ffn_dim_multiplier<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span><span class="token builtin">float</span><span class="token punctuation">]</span><span class="token punctuation">,</span>    <span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>        hidden_dim <span class="token operator">=</span> <span class="token builtin">int</span><span class="token punctuation">(</span><span class="token number">2</span> <span class="token operator">*</span> hidden_dim <span class="token operator">/</span> <span class="token number">3</span><span class="token punctuation">)</span>        <span class="token comment"># custom dim factor multiplier</span>        <span class="token keyword">if</span> ffn_dim_multiplier <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>            hidden_dim <span class="token operator">=</span> <span class="token builtin">int</span><span class="token punctuation">(</span>ffn_dim_multiplier <span class="token operator">*</span> hidden_dim<span class="token punctuation">)</span>        hidden_dim <span class="token operator">=</span> multiple_of <span class="token operator">*</span> <span class="token punctuation">(</span><span class="token punctuation">(</span>hidden_dim <span class="token operator">+</span> multiple_of <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">)</span> <span class="token operator">//</span> multiple_of<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>w1 <span class="token operator">=</span> ColumnParallelLinear<span class="token punctuation">(</span>            dim<span class="token punctuation">,</span> hidden_dim<span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> gather_output<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> init_method<span class="token operator">=</span><span class="token keyword">lambda</span> x<span class="token punctuation">:</span> x        <span class="token punctuation">)</span>        self<span class="token punctuation">.</span>w2 <span class="token operator">=</span> RowParallelLinear<span class="token punctuation">(</span>            hidden_dim<span class="token punctuation">,</span> dim<span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> input_is_parallel<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> init_method<span class="token operator">=</span><span class="token keyword">lambda</span> x<span class="token punctuation">:</span> x        <span class="token punctuation">)</span>        self<span class="token punctuation">.</span>w3 <span class="token operator">=</span> ColumnParallelLinear<span class="token punctuation">(</span>            dim<span class="token punctuation">,</span> hidden_dim<span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> gather_output<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> init_method<span class="token operator">=</span><span class="token keyword">lambda</span> x<span class="token punctuation">:</span> x        <span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">return</span> self<span class="token punctuation">.</span>w2<span class="token punctuation">(</span>F<span class="token punctuation">.</span>silu<span class="token punctuation">(</span>self<span class="token punctuation">.</span>w1<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token operator">*</span> self<span class="token punctuation">.</span>w3<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token keyword">class</span> <span class="token class-name">TransformerBlock</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> layer_id<span class="token punctuation">:</span> <span class="token builtin">int</span><span class="token punctuation">,</span> args<span class="token punctuation">:</span> ModelArgs<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>n_heads <span class="token operator">=</span> args<span class="token punctuation">.</span>n_heads        self<span class="token punctuation">.</span>dim <span class="token operator">=</span> args<span class="token punctuation">.</span>dim        self<span class="token punctuation">.</span>head_dim <span class="token operator">=</span> args<span class="token punctuation">.</span>dim <span class="token operator">//</span> args<span class="token punctuation">.</span>n_heads        self<span class="token punctuation">.</span>attention <span class="token operator">=</span> Attention<span class="token punctuation">(</span>args<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>feed_forward <span class="token operator">=</span> FeedForward<span class="token punctuation">(</span>            dim<span class="token operator">=</span>args<span class="token punctuation">.</span>dim<span class="token punctuation">,</span>            hidden_dim<span class="token operator">=</span><span class="token number">4</span> <span class="token operator">*</span> args<span class="token punctuation">.</span>dim<span class="token punctuation">,</span>            multiple_of<span class="token operator">=</span>args<span class="token punctuation">.</span>multiple_of<span class="token punctuation">,</span>            ffn_dim_multiplier<span class="token operator">=</span>args<span class="token punctuation">.</span>ffn_dim_multiplier<span class="token punctuation">,</span>        <span class="token punctuation">)</span>        self<span class="token punctuation">.</span>layer_id <span class="token operator">=</span> layer_id        self<span class="token punctuation">.</span>attention_norm <span class="token operator">=</span> RMSNorm<span class="token punctuation">(</span>args<span class="token punctuation">.</span>dim<span class="token punctuation">,</span> eps<span class="token operator">=</span>args<span class="token punctuation">.</span>norm_eps<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>ffn_norm <span class="token operator">=</span> RMSNorm<span class="token punctuation">(</span>args<span class="token punctuation">.</span>dim<span class="token punctuation">,</span> eps<span class="token operator">=</span>args<span class="token punctuation">.</span>norm_eps<span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>        self<span class="token punctuation">,</span>        x<span class="token punctuation">:</span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">,</span>        start_pos<span class="token punctuation">:</span> <span class="token builtin">int</span><span class="token punctuation">,</span>        freqs_cis<span class="token punctuation">:</span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">,</span>        mask<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">]</span><span class="token punctuation">,</span>    <span class="token punctuation">)</span><span class="token punctuation">:</span>        h <span class="token operator">=</span> x <span class="token operator">+</span> self<span class="token punctuation">.</span>attention<span class="token punctuation">(</span>self<span class="token punctuation">.</span>attention_norm<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">,</span> start_pos<span class="token punctuation">,</span> freqs_cis<span class="token punctuation">,</span> mask<span class="token punctuation">)</span>        out <span class="token operator">=</span> h <span class="token operator">+</span> self<span class="token punctuation">.</span>feed_forward<span class="token punctuation">(</span>self<span class="token punctuation">.</span>ffn_norm<span class="token punctuation">(</span>h<span class="token punctuation">)</span><span class="token punctuation">)</span>        <span class="token keyword">return</span> out<span class="token keyword">class</span> <span class="token class-name">Transformer</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> params<span class="token punctuation">:</span> ModelArgs<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>params <span class="token operator">=</span> params        self<span class="token punctuation">.</span>vocab_size <span class="token operator">=</span> params<span class="token punctuation">.</span>vocab_size        self<span class="token punctuation">.</span>n_layers <span class="token operator">=</span> params<span class="token punctuation">.</span>n_layers        self<span class="token punctuation">.</span>tok_embeddings <span class="token operator">=</span> VocabParallelEmbedding<span class="token punctuation">(</span>            params<span class="token punctuation">.</span>vocab_size<span class="token punctuation">,</span> params<span class="token punctuation">.</span>dim<span class="token punctuation">,</span> init_method<span class="token operator">=</span><span class="token keyword">lambda</span> x<span class="token punctuation">:</span> x        <span class="token punctuation">)</span>        self<span class="token punctuation">.</span>layers <span class="token operator">=</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>ModuleList<span class="token punctuation">(</span><span class="token punctuation">)</span>        <span class="token keyword">for</span> layer_id <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>params<span class="token punctuation">.</span>n_layers<span class="token punctuation">)</span><span class="token punctuation">:</span>            self<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>append<span class="token punctuation">(</span>TransformerBlock<span class="token punctuation">(</span>layer_id<span class="token punctuation">,</span> params<span class="token punctuation">)</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>norm <span class="token operator">=</span> RMSNorm<span class="token punctuation">(</span>params<span class="token punctuation">.</span>dim<span class="token punctuation">,</span> eps<span class="token operator">=</span>params<span class="token punctuation">.</span>norm_eps<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>output <span class="token operator">=</span> ColumnParallelLinear<span class="token punctuation">(</span>            params<span class="token punctuation">.</span>dim<span class="token punctuation">,</span> params<span class="token punctuation">.</span>vocab_size<span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> init_method<span class="token operator">=</span><span class="token keyword">lambda</span> x<span class="token punctuation">:</span> x        <span class="token punctuation">)</span>        self<span class="token punctuation">.</span>freqs_cis <span class="token operator">=</span> precompute_freqs_cis<span class="token punctuation">(</span>            params<span class="token punctuation">.</span>dim <span class="token operator">//</span> params<span class="token punctuation">.</span>n_heads<span class="token punctuation">,</span>            params<span class="token punctuation">.</span>max_seq_len <span class="token operator">*</span> <span class="token number">2</span><span class="token punctuation">,</span>            params<span class="token punctuation">.</span>rope_theta<span class="token punctuation">,</span>        <span class="token punctuation">)</span>    <span class="token decorator annotation punctuation">@torch<span class="token punctuation">.</span>inference_mode</span><span class="token punctuation">(</span><span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> tokens<span class="token punctuation">:</span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">,</span> start_pos<span class="token punctuation">:</span> <span class="token builtin">int</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        _bsz<span class="token punctuation">,</span> seqlen <span class="token operator">=</span> tokens<span class="token punctuation">.</span>shape        h <span class="token operator">=</span> self<span class="token punctuation">.</span>tok_embeddings<span class="token punctuation">(</span>tokens<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>freqs_cis <span class="token operator">=</span> self<span class="token punctuation">.</span>freqs_cis<span class="token punctuation">.</span>to<span class="token punctuation">(</span>h<span class="token punctuation">.</span>device<span class="token punctuation">)</span>        freqs_cis <span class="token operator">=</span> self<span class="token punctuation">.</span>freqs_cis<span class="token punctuation">[</span>start_pos <span class="token punctuation">:</span> start_pos <span class="token operator">+</span> seqlen<span class="token punctuation">]</span>        mask <span class="token operator">=</span> <span class="token boolean">None</span>        <span class="token keyword">if</span> seqlen <span class="token operator">></span> <span class="token number">1</span><span class="token punctuation">:</span>            mask <span class="token operator">=</span> torch<span class="token punctuation">.</span>full<span class="token punctuation">(</span><span class="token punctuation">(</span>seqlen<span class="token punctuation">,</span> seqlen<span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token builtin">float</span><span class="token punctuation">(</span><span class="token string">"-inf"</span><span class="token punctuation">)</span><span class="token punctuation">,</span> device<span class="token operator">=</span>tokens<span class="token punctuation">.</span>device<span class="token punctuation">)</span>            mask <span class="token operator">=</span> torch<span class="token punctuation">.</span>triu<span class="token punctuation">(</span>mask<span class="token punctuation">,</span> diagonal<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>            <span class="token comment"># When performing key-value caching, we compute the attention scores</span>            <span class="token comment"># only for the new sequence. Thus, the matrix of scores is of size</span>            <span class="token comment"># (seqlen, cache_len + seqlen), and the only masked entries are (i, j) for</span>            <span class="token comment"># j > cache_len + i, since row i corresponds to token cache_len + i.</span>            mask <span class="token operator">=</span> torch<span class="token punctuation">.</span>hstack<span class="token punctuation">(</span>                <span class="token punctuation">[</span>torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span><span class="token punctuation">(</span>seqlen<span class="token punctuation">,</span> start_pos<span class="token punctuation">)</span><span class="token punctuation">,</span> device<span class="token operator">=</span>tokens<span class="token punctuation">.</span>device<span class="token punctuation">)</span><span class="token punctuation">,</span> mask<span class="token punctuation">]</span>            <span class="token punctuation">)</span><span class="token punctuation">.</span>type_as<span class="token punctuation">(</span>h<span class="token punctuation">)</span>        <span class="token keyword">for</span> layer <span class="token keyword">in</span> self<span class="token punctuation">.</span>layers<span class="token punctuation">:</span>            h <span class="token operator">=</span> layer<span class="token punctuation">(</span>h<span class="token punctuation">,</span> start_pos<span class="token punctuation">,</span> freqs_cis<span class="token punctuation">,</span> mask<span class="token punctuation">)</span>        h <span class="token operator">=</span> self<span class="token punctuation">.</span>norm<span class="token punctuation">(</span>h<span class="token punctuation">)</span>        output <span class="token operator">=</span> self<span class="token punctuation">.</span>output<span class="token punctuation">(</span>h<span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">float</span><span class="token punctuation">(</span><span class="token punctuation">)</span>        <span class="token keyword">return</span> output<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>]]></content>
      
      
      
        <tags>
            
            <tag> LLM </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Attention(MHA, MLA, GQA, MQA)</title>
      <link href="/2025/02/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%9F%BA%E7%A1%80/attention%20%E6%9C%BA%E5%88%B6/"/>
      <url>/2025/02/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%9F%BA%E7%A1%80/attention%20%E6%9C%BA%E5%88%B6/</url>
      
        <content type="html"><![CDATA[<h1 id="attention-mha-mla-gqamqa"><a class="markdownIt-Anchor" href="#attention-mha-mla-gqamqa"></a> Attention: MHA, MLA, GQA,MQA</h1><p><img src="https://github.com/haukzero/from-mha-to-mla/blob/master/img/mha_mqa_gqa.png?raw=true" alt="mha_mqa_gqa.png" /></p><h2 id="mha"><a class="markdownIt-Anchor" href="#mha"></a> MHA</h2><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> torch<span class="token keyword">from</span> torch <span class="token keyword">import</span> nn<span class="token keyword">from</span> typing <span class="token keyword">import</span> Optional<span class="token keyword">def</span> <span class="token function">apply_rope</span><span class="token punctuation">(</span>x<span class="token punctuation">:</span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">,</span> <span class="token operator">*</span>args<span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">return</span> x<span class="token keyword">def</span> <span class="token function">update_kv_cache</span><span class="token punctuation">(</span>key_states<span class="token punctuation">:</span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">,</span> value_states<span class="token punctuation">:</span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">return</span> key_states<span class="token punctuation">.</span>repeat<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> value_states<span class="token punctuation">.</span>repeat<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token keyword">class</span> <span class="token class-name">MultiHeadAttention</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>        self<span class="token punctuation">,</span>        hidden_size<span class="token punctuation">:</span> <span class="token builtin">int</span><span class="token punctuation">,</span>        num_heads<span class="token punctuation">:</span> <span class="token builtin">int</span><span class="token punctuation">,</span>        head_dim<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span><span class="token builtin">int</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span>        use_cache<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span><span class="token builtin">bool</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">False</span><span class="token punctuation">,</span>    <span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>hidden_size <span class="token operator">=</span> hidden_size        self<span class="token punctuation">.</span>num_heads <span class="token operator">=</span> num_heads        self<span class="token punctuation">.</span>head_dim <span class="token operator">=</span> head_dim <span class="token keyword">or</span> hidden_size <span class="token operator">//</span> num_heads        self<span class="token punctuation">.</span>scale <span class="token operator">=</span> self<span class="token punctuation">.</span>head_dim<span class="token operator">**</span><span class="token operator">-</span><span class="token number">0.5</span>        self<span class="token punctuation">.</span>use_cache <span class="token operator">=</span> use_cache        self<span class="token punctuation">.</span>q_proj <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>self<span class="token punctuation">.</span>hidden_size<span class="token punctuation">,</span> self<span class="token punctuation">.</span>num_heads <span class="token operator">*</span> self<span class="token punctuation">.</span>head_dim<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>k_proj <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>self<span class="token punctuation">.</span>hidden_size<span class="token punctuation">,</span> self<span class="token punctuation">.</span>num_heads <span class="token operator">*</span> self<span class="token punctuation">.</span>head_dim<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>v_proj <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>self<span class="token punctuation">.</span>hidden_size<span class="token punctuation">,</span> self<span class="token punctuation">.</span>num_heads <span class="token operator">*</span> self<span class="token punctuation">.</span>head_dim<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>o_proj <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>self<span class="token punctuation">.</span>num_heads <span class="token operator">*</span> self<span class="token punctuation">.</span>head_dim<span class="token punctuation">,</span> self<span class="token punctuation">.</span>hidden_size<span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>        self<span class="token punctuation">,</span>        hidden_states<span class="token punctuation">:</span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">,</span>        position_ids<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>torch<span class="token punctuation">.</span>LongTensor<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span>    <span class="token punctuation">)</span><span class="token punctuation">:</span>        bsz<span class="token punctuation">,</span> q_len<span class="token punctuation">,</span> _ <span class="token operator">=</span> hidden_states<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span>        <span class="token comment"># q: (bsz, num_heads, q_len, head_dim)</span>        query_states <span class="token operator">=</span> <span class="token punctuation">(</span>            self<span class="token punctuation">.</span>q_proj<span class="token punctuation">(</span>hidden_states<span class="token punctuation">)</span>            <span class="token punctuation">.</span>view<span class="token punctuation">(</span>bsz<span class="token punctuation">,</span> q_len<span class="token punctuation">,</span> self<span class="token punctuation">.</span>num_heads<span class="token punctuation">,</span> self<span class="token punctuation">.</span>head_dim<span class="token punctuation">)</span>            <span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>        <span class="token punctuation">)</span>        key_states <span class="token operator">=</span> <span class="token punctuation">(</span>            self<span class="token punctuation">.</span>k_proj<span class="token punctuation">(</span>hidden_states<span class="token punctuation">)</span>            <span class="token punctuation">.</span>view<span class="token punctuation">(</span>bsz<span class="token punctuation">,</span> q_len<span class="token punctuation">,</span> self<span class="token punctuation">.</span>num_heads<span class="token punctuation">,</span> self<span class="token punctuation">.</span>head_dim<span class="token punctuation">)</span>            <span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>        <span class="token punctuation">)</span>        value_states <span class="token operator">=</span> <span class="token punctuation">(</span>            self<span class="token punctuation">.</span>v_proj<span class="token punctuation">(</span>hidden_states<span class="token punctuation">)</span>            <span class="token punctuation">.</span>view<span class="token punctuation">(</span>bsz<span class="token punctuation">,</span> q_len<span class="token punctuation">,</span> self<span class="token punctuation">.</span>num_heads<span class="token punctuation">,</span> self<span class="token punctuation">.</span>head_dim<span class="token punctuation">)</span>            <span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>        <span class="token punctuation">)</span>        <span class="token comment"># RoPE</span>        query_states <span class="token operator">=</span> apply_rope<span class="token punctuation">(</span>query_states<span class="token punctuation">)</span>        key_states <span class="token operator">=</span> apply_rope<span class="token punctuation">(</span>key_states<span class="token punctuation">)</span>        <span class="token keyword">if</span> self<span class="token punctuation">.</span>use_cache<span class="token punctuation">:</span>            <span class="token comment"># Update KV Cache and get full kv</span>            <span class="token comment"># k/v: (bsz, num_heads, kv_len, head_dim)</span>            key_states<span class="token punctuation">,</span> value_states <span class="token operator">=</span> update_kv_cache<span class="token punctuation">(</span>key_states<span class="token punctuation">,</span> value_states<span class="token punctuation">)</span>        kv_len <span class="token operator">=</span> key_states<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span>        <span class="token comment"># Softmax(Q @ K^T / sqrt(d_k))</span>        <span class="token comment"># attn: (bsz, num_heads, q_len, kv_len)</span>        attn_weights <span class="token operator">=</span> <span class="token punctuation">(</span>            torch<span class="token punctuation">.</span>einsum<span class="token punctuation">(</span><span class="token string">"bhld, bhnd -> bhln"</span><span class="token punctuation">,</span> query_states<span class="token punctuation">,</span> key_states<span class="token punctuation">)</span> <span class="token operator">*</span> self<span class="token punctuation">.</span>scale        <span class="token punctuation">)</span>        attn_weights <span class="token operator">=</span> attn_weights<span class="token punctuation">.</span>softmax<span class="token punctuation">(</span>dim<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>        <span class="token comment"># A @ V</span>        <span class="token comment"># attn: (bsz, num_heads, q_len, head_dim)</span>        attn_weights <span class="token operator">=</span> torch<span class="token punctuation">.</span>einsum<span class="token punctuation">(</span><span class="token string">"bhln, bhnd -> bhld"</span><span class="token punctuation">,</span> attn_weights<span class="token punctuation">,</span> value_states<span class="token punctuation">)</span>        <span class="token comment"># output: (bsz, q_len, hidden_size)</span>        attn_output <span class="token operator">=</span> self<span class="token punctuation">.</span>o_proj<span class="token punctuation">(</span>            attn_weights<span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>            <span class="token punctuation">.</span>contiguous<span class="token punctuation">(</span><span class="token punctuation">)</span>            <span class="token punctuation">.</span>view<span class="token punctuation">(</span>bsz<span class="token punctuation">,</span> q_len<span class="token punctuation">,</span> self<span class="token punctuation">.</span>num_heads <span class="token operator">*</span> self<span class="token punctuation">.</span>head_dim<span class="token punctuation">)</span>        <span class="token punctuation">)</span>        <span class="token keyword">return</span> attn_output<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="gqa-grouped-query-attention"><a class="markdownIt-Anchor" href="#gqa-grouped-query-attention"></a> GQA (Grouped-query Attention)</h2><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> torch<span class="token keyword">from</span> torch <span class="token keyword">import</span> nn<span class="token keyword">from</span> typing <span class="token keyword">import</span> Optional<span class="token keyword">def</span> <span class="token function">apply_rope</span><span class="token punctuation">(</span>x<span class="token punctuation">:</span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">,</span> <span class="token operator">*</span>args<span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">return</span> x<span class="token keyword">def</span> <span class="token function">update_kv_cache</span><span class="token punctuation">(</span>key_states<span class="token punctuation">:</span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">,</span> value_states<span class="token punctuation">:</span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">return</span> key_states<span class="token punctuation">.</span>repeat<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> value_states<span class="token punctuation">.</span>repeat<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token keyword">def</span> <span class="token function">repeat_kv</span><span class="token punctuation">(</span>hidden_states<span class="token punctuation">:</span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">,</span> n_rep<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span><span class="token builtin">int</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    batch<span class="token punctuation">,</span> num_key_value_heads<span class="token punctuation">,</span> slen<span class="token punctuation">,</span> head_dim <span class="token operator">=</span> hidden_states<span class="token punctuation">.</span>shape    <span class="token keyword">if</span> n_rep <span class="token operator">==</span> <span class="token number">1</span><span class="token punctuation">:</span>        <span class="token keyword">return</span> hidden_states    hidden_states <span class="token operator">=</span> hidden_states<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token boolean">None</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">.</span>expand<span class="token punctuation">(</span>        batch<span class="token punctuation">,</span> num_key_value_heads<span class="token punctuation">,</span> n_rep<span class="token punctuation">,</span> slen<span class="token punctuation">,</span> head_dim    <span class="token punctuation">)</span>    <span class="token keyword">return</span> hidden_states<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span>batch<span class="token punctuation">,</span> num_key_value_heads <span class="token operator">*</span> n_rep<span class="token punctuation">,</span> slen<span class="token punctuation">,</span> head_dim<span class="token punctuation">)</span><span class="token keyword">class</span> <span class="token class-name">GroupQueryAttention</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>        self<span class="token punctuation">,</span>        hidden_size<span class="token punctuation">:</span> <span class="token builtin">int</span><span class="token punctuation">,</span>        num_heads<span class="token punctuation">:</span> <span class="token builtin">int</span><span class="token punctuation">,</span>        num_kv_heads<span class="token punctuation">:</span> <span class="token builtin">int</span><span class="token punctuation">,</span>        head_dim<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span><span class="token builtin">int</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span>        use_cache<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span><span class="token builtin">bool</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">False</span><span class="token punctuation">,</span>    <span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>hidden_size <span class="token operator">=</span> hidden_size        self<span class="token punctuation">.</span>num_heads <span class="token operator">=</span> num_heads        self<span class="token punctuation">.</span>num_kv_heads <span class="token operator">=</span> num_kv_heads        self<span class="token punctuation">.</span>num_groups <span class="token operator">=</span> num_heads <span class="token operator">//</span> num_kv_heads        self<span class="token punctuation">.</span>head_dim <span class="token operator">=</span> head_dim <span class="token keyword">or</span> hidden_size <span class="token operator">//</span> num_heads        self<span class="token punctuation">.</span>scale <span class="token operator">=</span> self<span class="token punctuation">.</span>head_dim<span class="token operator">**</span><span class="token operator">-</span><span class="token number">0.5</span>        self<span class="token punctuation">.</span>use_cache <span class="token operator">=</span> use_cache        self<span class="token punctuation">.</span>q_proj <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>self<span class="token punctuation">.</span>hidden_size<span class="token punctuation">,</span> self<span class="token punctuation">.</span>num_heads <span class="token operator">*</span> self<span class="token punctuation">.</span>head_dim<span class="token punctuation">)</span>        <span class="token comment"># different from the MHA/MQA</span>        self<span class="token punctuation">.</span>k_proj <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>self<span class="token punctuation">.</span>hidden_size<span class="token punctuation">,</span> self<span class="token punctuation">.</span>num_kv_heads <span class="token operator">*</span> self<span class="token punctuation">.</span>head_dim<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>v_proj <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>self<span class="token punctuation">.</span>hidden_size<span class="token punctuation">,</span> self<span class="token punctuation">.</span>num_kv_heads <span class="token operator">*</span> self<span class="token punctuation">.</span>head_dim<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>o_proj <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>self<span class="token punctuation">.</span>num_heads <span class="token operator">*</span> self<span class="token punctuation">.</span>head_dim<span class="token punctuation">,</span> self<span class="token punctuation">.</span>hidden_size<span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>        self<span class="token punctuation">,</span>        hidden_states<span class="token punctuation">:</span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">,</span>        position_ids<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>torch<span class="token punctuation">.</span>LongTensor<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span>    <span class="token punctuation">)</span><span class="token punctuation">:</span>        bsz<span class="token punctuation">,</span> q_len<span class="token punctuation">,</span> _ <span class="token operator">=</span> hidden_states<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span>        <span class="token comment"># q: (bsz, num_heads, q_len, head_dim)</span>        query_states <span class="token operator">=</span> <span class="token punctuation">(</span>            self<span class="token punctuation">.</span>q_proj<span class="token punctuation">(</span>hidden_states<span class="token punctuation">)</span>            <span class="token punctuation">.</span>view<span class="token punctuation">(</span>bsz<span class="token punctuation">,</span> q_len<span class="token punctuation">,</span> self<span class="token punctuation">.</span>num_heads<span class="token punctuation">,</span> self<span class="token punctuation">.</span>head_dim<span class="token punctuation">)</span>            <span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>        <span class="token punctuation">)</span>        <span class="token comment"># k/v: (bsz, num_kv_heads, q_len, head_dim)</span>        key_states <span class="token operator">=</span> <span class="token punctuation">(</span>            self<span class="token punctuation">.</span>k_proj<span class="token punctuation">(</span>hidden_states<span class="token punctuation">)</span>            <span class="token punctuation">.</span>view<span class="token punctuation">(</span>bsz<span class="token punctuation">,</span> q_len<span class="token punctuation">,</span> self<span class="token punctuation">.</span>num_kv_heads<span class="token punctuation">,</span> self<span class="token punctuation">.</span>head_dim<span class="token punctuation">)</span>            <span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>        <span class="token punctuation">)</span>        value_states <span class="token operator">=</span> <span class="token punctuation">(</span>            self<span class="token punctuation">.</span>v_proj<span class="token punctuation">(</span>hidden_states<span class="token punctuation">)</span>            <span class="token punctuation">.</span>view<span class="token punctuation">(</span>bsz<span class="token punctuation">,</span> q_len<span class="token punctuation">,</span> self<span class="token punctuation">.</span>num_kv_heads<span class="token punctuation">,</span> self<span class="token punctuation">.</span>head_dim<span class="token punctuation">)</span>            <span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>        <span class="token punctuation">)</span>        <span class="token comment"># RoPE</span>        query_states <span class="token operator">=</span> apply_rope<span class="token punctuation">(</span>query_states<span class="token punctuation">)</span>        key_states <span class="token operator">=</span> apply_rope<span class="token punctuation">(</span>key_states<span class="token punctuation">)</span>        <span class="token keyword">if</span> self<span class="token punctuation">.</span>use_cache<span class="token punctuation">:</span>            <span class="token comment"># Update KV Cache and get full kv</span>            <span class="token comment"># k/v: (bsz, num_heads, kv_len, head_dim)</span>            key_states<span class="token punctuation">,</span> value_states <span class="token operator">=</span> update_kv_cache<span class="token punctuation">(</span>key_states<span class="token punctuation">,</span> value_states<span class="token punctuation">)</span>        kv_len <span class="token operator">=</span> key_states<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span>        <span class="token comment"># Repeat kv</span>        <span class="token comment"># k/v: (bsz, num_heads, q_len, head_dim)</span>        key_states <span class="token operator">=</span> repeat_kv<span class="token punctuation">(</span>key_states<span class="token punctuation">,</span> self<span class="token punctuation">.</span>num_groups<span class="token punctuation">)</span>        value_states <span class="token operator">=</span> repeat_kv<span class="token punctuation">(</span>value_states<span class="token punctuation">,</span> self<span class="token punctuation">.</span>num_groups<span class="token punctuation">)</span>        <span class="token comment"># Softmax(Q @ K^T / sqrt(d_k))</span>        <span class="token comment"># attn: (bsz, num_heads, q_len, kv_len)</span>        attn_weights <span class="token operator">=</span> <span class="token punctuation">(</span>            torch<span class="token punctuation">.</span>einsum<span class="token punctuation">(</span><span class="token string">"bhld, bhnd -> bhln"</span><span class="token punctuation">,</span> query_states<span class="token punctuation">,</span> key_states<span class="token punctuation">)</span> <span class="token operator">*</span> self<span class="token punctuation">.</span>scale        <span class="token punctuation">)</span>        attn_weights <span class="token operator">=</span> attn_weights<span class="token punctuation">.</span>softmax<span class="token punctuation">(</span>dim<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>        <span class="token comment"># A @ V</span>        <span class="token comment"># attn: (bsz, num_heads, q_len, head_dim)</span>        attn_weights <span class="token operator">=</span> torch<span class="token punctuation">.</span>einsum<span class="token punctuation">(</span><span class="token string">"bhln, bhnd -> bhld"</span><span class="token punctuation">,</span> attn_weights<span class="token punctuation">,</span> value_states<span class="token punctuation">)</span>        <span class="token comment"># output: (bsz, q_len, hidden_size)</span>        attn_output <span class="token operator">=</span> self<span class="token punctuation">.</span>o_proj<span class="token punctuation">(</span>            attn_weights<span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>            <span class="token punctuation">.</span>contiguous<span class="token punctuation">(</span><span class="token punctuation">)</span>            <span class="token punctuation">.</span>view<span class="token punctuation">(</span>bsz<span class="token punctuation">,</span> q_len<span class="token punctuation">,</span> self<span class="token punctuation">.</span>num_heads <span class="token operator">*</span> self<span class="token punctuation">.</span>head_dim<span class="token punctuation">)</span>        <span class="token punctuation">)</span>        <span class="token keyword">return</span> attn_output<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="mla-multi-head-latent-attention"><a class="markdownIt-Anchor" href="#mla-multi-head-latent-attention"></a> MLA (Multi-head Latent Attention)</h2><img src="https://raw.githubusercontent.com/weikangqi/picgo/main/mla_cc.png" alt="mla_cc.png" style="zoom: 67%;" /><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> torch<span class="token keyword">from</span> torch <span class="token keyword">import</span> nn<span class="token keyword">from</span> typing <span class="token keyword">import</span> Optional<span class="token keyword">def</span> <span class="token function">apply_rope</span><span class="token punctuation">(</span>x<span class="token punctuation">:</span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">,</span> <span class="token operator">*</span>args<span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">return</span> x<span class="token keyword">def</span> <span class="token function">update_compressed_kv_cache</span><span class="token punctuation">(</span>compressed_kv<span class="token punctuation">:</span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">return</span> compressed_kv<span class="token punctuation">.</span>repeat<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token keyword">def</span> <span class="token function">repeat_kv</span><span class="token punctuation">(</span>hidden_states<span class="token punctuation">:</span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">,</span> n_rep<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span><span class="token builtin">int</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    batch<span class="token punctuation">,</span> num_key_value_heads<span class="token punctuation">,</span> slen<span class="token punctuation">,</span> head_dim <span class="token operator">=</span> hidden_states<span class="token punctuation">.</span>shape    <span class="token keyword">if</span> n_rep <span class="token operator">==</span> <span class="token number">1</span><span class="token punctuation">:</span>        <span class="token keyword">return</span> hidden_states    hidden_states <span class="token operator">=</span> hidden_states<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token boolean">None</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">.</span>expand<span class="token punctuation">(</span>        batch<span class="token punctuation">,</span> num_key_value_heads<span class="token punctuation">,</span> n_rep<span class="token punctuation">,</span> slen<span class="token punctuation">,</span> head_dim    <span class="token punctuation">)</span>    <span class="token keyword">return</span> hidden_states<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span>batch<span class="token punctuation">,</span> num_key_value_heads <span class="token operator">*</span> n_rep<span class="token punctuation">,</span> slen<span class="token punctuation">,</span> head_dim<span class="token punctuation">)</span><span class="token keyword">class</span> <span class="token class-name">MultiHeadLatentAttention</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>        self<span class="token punctuation">,</span>        hidden_size<span class="token punctuation">:</span> <span class="token builtin">int</span><span class="token punctuation">,</span>        num_heads<span class="token punctuation">:</span> <span class="token builtin">int</span><span class="token punctuation">,</span>        q_lora_rank<span class="token punctuation">:</span> <span class="token builtin">int</span><span class="token punctuation">,</span>        qk_rope_head_dim<span class="token punctuation">:</span> <span class="token builtin">int</span><span class="token punctuation">,</span>        qk_nope_head_dim<span class="token punctuation">:</span> <span class="token builtin">int</span><span class="token punctuation">,</span>        kv_lora_rank<span class="token punctuation">:</span> <span class="token builtin">int</span><span class="token punctuation">,</span>        v_head_dim<span class="token punctuation">:</span> <span class="token builtin">int</span><span class="token punctuation">,</span>        use_cache<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span><span class="token builtin">bool</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">False</span><span class="token punctuation">,</span>    <span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>        qk_head_dim <span class="token operator">=</span> qk_nope_head_dim <span class="token operator">+</span> qk_rope_head_dim        self<span class="token punctuation">.</span>hidden_size <span class="token operator">=</span> hidden_size        self<span class="token punctuation">.</span>num_heads <span class="token operator">=</span> num_heads        self<span class="token punctuation">.</span>q_lora_rank <span class="token operator">=</span> q_lora_rank        self<span class="token punctuation">.</span>qk_rope_head_dim <span class="token operator">=</span> qk_rope_head_dim        self<span class="token punctuation">.</span>qk_nope_head_dim <span class="token operator">=</span> qk_nope_head_dim        self<span class="token punctuation">.</span>kv_lora_rank <span class="token operator">=</span> kv_lora_rank        self<span class="token punctuation">.</span>qk_head_dim <span class="token operator">=</span> qk_head_dim        self<span class="token punctuation">.</span>v_head_dim <span class="token operator">=</span> v_head_dim        self<span class="token punctuation">.</span>scale <span class="token operator">=</span> self<span class="token punctuation">.</span>qk_head_dim<span class="token operator">**</span><span class="token operator">-</span><span class="token number">0.5</span>        self<span class="token punctuation">.</span>use_cache <span class="token operator">=</span> use_cache        self<span class="token punctuation">.</span>q_a_proj <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>hidden_size<span class="token punctuation">,</span> q_lora_rank<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>q_b_proj <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>q_lora_rank<span class="token punctuation">,</span> num_heads <span class="token operator">*</span> qk_head_dim<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>kv_a_proj <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>hidden_size<span class="token punctuation">,</span> qk_rope_head_dim <span class="token operator">+</span> kv_lora_rank<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>kv_b_proj <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>            kv_lora_rank<span class="token punctuation">,</span> num_heads <span class="token operator">*</span> <span class="token punctuation">(</span>qk_nope_head_dim <span class="token operator">+</span> v_head_dim<span class="token punctuation">)</span>        <span class="token punctuation">)</span>        self<span class="token punctuation">.</span>o_proj <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>num_heads <span class="token operator">*</span> v_head_dim<span class="token punctuation">,</span> hidden_size<span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>        self<span class="token punctuation">,</span>        hidden_states<span class="token punctuation">:</span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">,</span>        position_ids<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>torch<span class="token punctuation">.</span>LongTensor<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span>    <span class="token punctuation">)</span><span class="token punctuation">:</span>        bsz<span class="token punctuation">,</span> q_len<span class="token punctuation">,</span> _ <span class="token operator">=</span> hidden_states<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span>        query_states <span class="token operator">=</span> self<span class="token punctuation">.</span>q_b_proj<span class="token punctuation">(</span>self<span class="token punctuation">.</span>q_a_proj<span class="token punctuation">(</span>hidden_states<span class="token punctuation">)</span><span class="token punctuation">)</span>        query_states <span class="token operator">=</span> query_states<span class="token punctuation">.</span>view<span class="token punctuation">(</span>            bsz<span class="token punctuation">,</span> q_len<span class="token punctuation">,</span> self<span class="token punctuation">.</span>num_heads<span class="token punctuation">,</span> self<span class="token punctuation">.</span>qk_head_dim        <span class="token punctuation">)</span><span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>        q_nope<span class="token punctuation">,</span> q_rope <span class="token operator">=</span> query_states<span class="token punctuation">.</span>split<span class="token punctuation">(</span>            <span class="token punctuation">[</span>self<span class="token punctuation">.</span>qk_nope_head_dim<span class="token punctuation">,</span> self<span class="token punctuation">.</span>qk_rope_head_dim<span class="token punctuation">]</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span>        <span class="token punctuation">)</span>        q_rope <span class="token operator">=</span> apply_rope<span class="token punctuation">(</span>q_rope<span class="token punctuation">)</span>        query_states <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">[</span>q_nope<span class="token punctuation">,</span> q_rope<span class="token punctuation">]</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>        <span class="token comment"># compressed_kv: (bsz, q_len, qk_rope_head_dim + kv_lora_rank)</span>        compressed_kv <span class="token operator">=</span> self<span class="token punctuation">.</span>kv_a_proj<span class="token punctuation">(</span>hidden_states<span class="token punctuation">)</span>        k_rope<span class="token punctuation">,</span> kv_nope <span class="token operator">=</span> compressed_kv<span class="token punctuation">.</span>split<span class="token punctuation">(</span>            <span class="token punctuation">[</span>self<span class="token punctuation">.</span>qk_rope_head_dim<span class="token punctuation">,</span> self<span class="token punctuation">.</span>kv_lora_rank<span class="token punctuation">]</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span>        <span class="token punctuation">)</span>        <span class="token comment"># k_rope: (bsz, num_heads, q_len, qk_rope_head_dim)</span>        k_rope <span class="token operator">=</span> repeat_kv<span class="token punctuation">(</span>k_rope<span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>num_heads<span class="token punctuation">)</span>        k_rope <span class="token operator">=</span> apply_rope<span class="token punctuation">(</span>k_rope<span class="token punctuation">)</span>        <span class="token comment"># Store compressed kv as a whole</span>        <span class="token keyword">if</span> self<span class="token punctuation">.</span>use_cache<span class="token punctuation">:</span>            compressed_kv <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">[</span>k_rope<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">,</span> kv_nope<span class="token punctuation">]</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>            compressed_kv <span class="token operator">=</span> update_compressed_kv_cache<span class="token punctuation">(</span>compressed_kv<span class="token punctuation">)</span>            k_rope<span class="token punctuation">,</span> kv_nope <span class="token operator">=</span> compressed_kv<span class="token punctuation">.</span>split<span class="token punctuation">(</span>                <span class="token punctuation">[</span>self<span class="token punctuation">.</span>qk_rope_head_dim<span class="token punctuation">,</span> self<span class="token punctuation">.</span>kv_lora_rank<span class="token punctuation">]</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span>            <span class="token punctuation">)</span>            k_rope <span class="token operator">=</span> repeat_kv<span class="token punctuation">(</span>k_rope<span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>num_heads<span class="token punctuation">)</span>        kv_len <span class="token operator">=</span> compressed_kv<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span>                kv <span class="token operator">=</span> <span class="token punctuation">(</span>            self<span class="token punctuation">.</span>kv_b_proj<span class="token punctuation">(</span>kv_nope<span class="token punctuation">)</span>            <span class="token punctuation">.</span>view<span class="token punctuation">(</span>bsz<span class="token punctuation">,</span> kv_len<span class="token punctuation">,</span> self<span class="token punctuation">.</span>num_heads<span class="token punctuation">,</span> self<span class="token punctuation">.</span>qk_nope_head_dim <span class="token operator">+</span> self<span class="token punctuation">.</span>v_head_dim<span class="token punctuation">)</span>            <span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>        <span class="token punctuation">)</span>        k_nope<span class="token punctuation">,</span> value_states <span class="token operator">=</span> kv<span class="token punctuation">.</span>split<span class="token punctuation">(</span>            <span class="token punctuation">[</span>self<span class="token punctuation">.</span>qk_nope_head_dim<span class="token punctuation">,</span> self<span class="token punctuation">.</span>v_head_dim<span class="token punctuation">]</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span>        <span class="token punctuation">)</span>        key_states <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">[</span>k_rope<span class="token punctuation">,</span> k_nope<span class="token punctuation">]</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>        <span class="token comment"># Softmax(Q @ K^T / sqrt(d_k))</span>        <span class="token comment"># attn: (bsz, num_heads, q_len, kv_len)</span>        attn_weights <span class="token operator">=</span> <span class="token punctuation">(</span>            torch<span class="token punctuation">.</span>einsum<span class="token punctuation">(</span><span class="token string">"bhld, bhnd -> bhln"</span><span class="token punctuation">,</span> query_states<span class="token punctuation">,</span> key_states<span class="token punctuation">)</span> <span class="token operator">*</span> self<span class="token punctuation">.</span>scale        <span class="token punctuation">)</span>        attn_weights <span class="token operator">=</span> attn_weights<span class="token punctuation">.</span>softmax<span class="token punctuation">(</span>dim<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>        <span class="token comment"># A @ V</span>        <span class="token comment"># attn: (bsz, num_heads, q_len, head_dim)</span>        attn_weights <span class="token operator">=</span> torch<span class="token punctuation">.</span>einsum<span class="token punctuation">(</span><span class="token string">"bhln, bhnd -> bhld"</span><span class="token punctuation">,</span> attn_weights<span class="token punctuation">,</span> value_states<span class="token punctuation">)</span>        <span class="token comment"># output: (bsz, q_len, hidden_size)</span>        attn_output <span class="token operator">=</span> self<span class="token punctuation">.</span>o_proj<span class="token punctuation">(</span>            attn_weights<span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>            <span class="token punctuation">.</span>contiguous<span class="token punctuation">(</span><span class="token punctuation">)</span>            <span class="token punctuation">.</span>view<span class="token punctuation">(</span>bsz<span class="token punctuation">,</span> q_len<span class="token punctuation">,</span> self<span class="token punctuation">.</span>num_heads <span class="token operator">*</span> self<span class="token punctuation">.</span>v_head_dim<span class="token punctuation">)</span>        <span class="token punctuation">)</span>        <span class="token keyword">return</span> attn_output<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="mqa"><a class="markdownIt-Anchor" href="#mqa"></a> MQA</h2><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> torch<span class="token keyword">from</span> torch <span class="token keyword">import</span> nn<span class="token keyword">from</span> typing <span class="token keyword">import</span> Optional<span class="token keyword">def</span> <span class="token function">apply_rope</span><span class="token punctuation">(</span>x<span class="token punctuation">:</span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">,</span> <span class="token operator">*</span>args<span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">return</span> x<span class="token keyword">def</span> <span class="token function">update_kv_cache</span><span class="token punctuation">(</span>key_states<span class="token punctuation">:</span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">,</span> value_states<span class="token punctuation">:</span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">return</span> key_states<span class="token punctuation">.</span>repeat<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> value_states<span class="token punctuation">.</span>repeat<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token keyword">def</span> <span class="token function">repeat_kv</span><span class="token punctuation">(</span>hidden_states<span class="token punctuation">:</span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">,</span> n_rep<span class="token punctuation">:</span> <span class="token builtin">int</span> <span class="token operator">=</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    batch<span class="token punctuation">,</span> num_key_value_heads<span class="token punctuation">,</span> slen<span class="token punctuation">,</span> head_dim <span class="token operator">=</span> hidden_states<span class="token punctuation">.</span>shape    <span class="token keyword">if</span> n_rep <span class="token operator">==</span> <span class="token number">1</span><span class="token punctuation">:</span>        <span class="token keyword">return</span> hidden_states    hidden_states <span class="token operator">=</span> hidden_states<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token boolean">None</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">.</span>expand<span class="token punctuation">(</span>        batch<span class="token punctuation">,</span> num_key_value_heads<span class="token punctuation">,</span> n_rep<span class="token punctuation">,</span> slen<span class="token punctuation">,</span> head_dim    <span class="token punctuation">)</span>    <span class="token keyword">return</span> hidden_states<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span>batch<span class="token punctuation">,</span> num_key_value_heads <span class="token operator">*</span> n_rep<span class="token punctuation">,</span> slen<span class="token punctuation">,</span> head_dim<span class="token punctuation">)</span><span class="token keyword">class</span> <span class="token class-name">MultiQueryAttention</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>        self<span class="token punctuation">,</span>        hidden_size<span class="token punctuation">:</span> <span class="token builtin">int</span><span class="token punctuation">,</span>        num_heads<span class="token punctuation">:</span> <span class="token builtin">int</span><span class="token punctuation">,</span>        head_dim<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span><span class="token builtin">int</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span>        use_cache<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span><span class="token builtin">bool</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">False</span><span class="token punctuation">,</span>    <span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>hidden_size <span class="token operator">=</span> hidden_size        self<span class="token punctuation">.</span>num_heads <span class="token operator">=</span> num_heads        self<span class="token punctuation">.</span>head_dim <span class="token operator">=</span> head_dim <span class="token keyword">or</span> hidden_size <span class="token operator">//</span> num_heads        self<span class="token punctuation">.</span>scale <span class="token operator">=</span> self<span class="token punctuation">.</span>head_dim<span class="token operator">**</span><span class="token operator">-</span><span class="token number">0.5</span>        self<span class="token punctuation">.</span>use_cache <span class="token operator">=</span> use_cache        self<span class="token punctuation">.</span>q_proj <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>self<span class="token punctuation">.</span>hidden_size<span class="token punctuation">,</span> self<span class="token punctuation">.</span>num_heads <span class="token operator">*</span> self<span class="token punctuation">.</span>head_dim<span class="token punctuation">)</span>        <span class="token comment"># different from the MHA</span>        self<span class="token punctuation">.</span>k_proj <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>self<span class="token punctuation">.</span>hidden_size<span class="token punctuation">,</span> self<span class="token punctuation">.</span>head_dim<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>v_proj <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>self<span class="token punctuation">.</span>hidden_size<span class="token punctuation">,</span> self<span class="token punctuation">.</span>head_dim<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>o_proj <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>self<span class="token punctuation">.</span>num_heads <span class="token operator">*</span> self<span class="token punctuation">.</span>head_dim<span class="token punctuation">,</span> self<span class="token punctuation">.</span>hidden_size<span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>        self<span class="token punctuation">,</span>        hidden_states<span class="token punctuation">:</span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">,</span>        position_ids<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>torch<span class="token punctuation">.</span>LongTensor<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span>    <span class="token punctuation">)</span><span class="token punctuation">:</span>        bsz<span class="token punctuation">,</span> q_len<span class="token punctuation">,</span> _ <span class="token operator">=</span> hidden_states<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span>        <span class="token comment"># q: (bsz, num_heads, q_len, head_dim)</span>        query_states <span class="token operator">=</span> <span class="token punctuation">(</span>            self<span class="token punctuation">.</span>q_proj<span class="token punctuation">(</span>hidden_states<span class="token punctuation">)</span>            <span class="token punctuation">.</span>view<span class="token punctuation">(</span>bsz<span class="token punctuation">,</span> q_len<span class="token punctuation">,</span> self<span class="token punctuation">.</span>num_heads<span class="token punctuation">,</span> self<span class="token punctuation">.</span>head_dim<span class="token punctuation">)</span>            <span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>        <span class="token punctuation">)</span>        <span class="token comment"># k/v: (bsz, 1, q_len, head_dim)</span>        key_states <span class="token operator">=</span> self<span class="token punctuation">.</span>k_proj<span class="token punctuation">(</span>hidden_states<span class="token punctuation">)</span><span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span>        value_states <span class="token operator">=</span> self<span class="token punctuation">.</span>v_proj<span class="token punctuation">(</span>hidden_states<span class="token punctuation">)</span><span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span>        <span class="token comment"># RoPE</span>        query_states <span class="token operator">=</span> apply_rope<span class="token punctuation">(</span>query_states<span class="token punctuation">)</span>        key_states <span class="token operator">=</span> apply_rope<span class="token punctuation">(</span>key_states<span class="token punctuation">)</span>        <span class="token keyword">if</span> self<span class="token punctuation">.</span>use_cache<span class="token punctuation">:</span>            <span class="token comment"># Update KV Cache and get full kv</span>            <span class="token comment"># k/v: (bsz, num_heads, kv_len, head_dim)</span>            key_states<span class="token punctuation">,</span> value_states <span class="token operator">=</span> update_kv_cache<span class="token punctuation">(</span>key_states<span class="token punctuation">,</span> value_states<span class="token punctuation">)</span>        kv_len <span class="token operator">=</span> key_states<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span>        <span class="token comment"># Repeat kv</span>        <span class="token comment"># k/v: (bsz, num_heads, q_len, head_dim)</span>        key_states <span class="token operator">=</span> repeat_kv<span class="token punctuation">(</span>key_states<span class="token punctuation">,</span> self<span class="token punctuation">.</span>num_heads<span class="token punctuation">)</span>        value_states <span class="token operator">=</span> repeat_kv<span class="token punctuation">(</span>value_states<span class="token punctuation">,</span> self<span class="token punctuation">.</span>num_heads<span class="token punctuation">)</span>        <span class="token comment"># Softmax(Q @ K^T / sqrt(d_k))</span>        <span class="token comment"># attn: (bsz, num_heads, q_len, kv_len)</span>        attn_weights <span class="token operator">=</span> <span class="token punctuation">(</span>            torch<span class="token punctuation">.</span>einsum<span class="token punctuation">(</span><span class="token string">"bhld, bhnd -> bhln"</span><span class="token punctuation">,</span> query_states<span class="token punctuation">,</span> key_states<span class="token punctuation">)</span> <span class="token operator">*</span> self<span class="token punctuation">.</span>scale        <span class="token punctuation">)</span>        attn_weights <span class="token operator">=</span> attn_weights<span class="token punctuation">.</span>softmax<span class="token punctuation">(</span>dim<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>        <span class="token comment"># A @ V</span>        <span class="token comment"># attn: (bsz, num_heads, q_len, head_dim)</span>        attn_weights <span class="token operator">=</span> torch<span class="token punctuation">.</span>einsum<span class="token punctuation">(</span><span class="token string">"bhln, bhnd -> bhld"</span><span class="token punctuation">,</span> attn_weights<span class="token punctuation">,</span> value_states<span class="token punctuation">)</span>        <span class="token comment"># output: (bsz, q_len, hidden_size)</span>        attn_output <span class="token operator">=</span> self<span class="token punctuation">.</span>o_proj<span class="token punctuation">(</span>            attn_weights<span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>            <span class="token punctuation">.</span>contiguous<span class="token punctuation">(</span><span class="token punctuation">)</span>            <span class="token punctuation">.</span>view<span class="token punctuation">(</span>bsz<span class="token punctuation">,</span> q_len<span class="token punctuation">,</span> self<span class="token punctuation">.</span>num_heads <span class="token operator">*</span> self<span class="token punctuation">.</span>head_dim<span class="token punctuation">)</span>        <span class="token punctuation">)</span>        <span class="token keyword">return</span> attn_output<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>]]></content>
      
      
      
        <tags>
            
            <tag> LLM </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ARM 中常用的乘累加（MAC）指令总结</title>
      <link href="/2024/12/ARM%20%E7%B4%AF%E5%8A%A0%E4%B9%98/"/>
      <url>/2024/12/ARM%20%E7%B4%AF%E5%8A%A0%E4%B9%98/</url>
      
        <content type="html"><![CDATA[<hr /><h1 id="arm-中常用的乘累加mac指令总结"><a class="markdownIt-Anchor" href="#arm-中常用的乘累加mac指令总结"></a> ARM 中常用的乘累加（MAC）指令总结</h1><p>在现代 CPU 和 AI 加速器的指令集中，<strong>乘-累加（Multiply-Accumulate, MAC）</strong> 是最核心的计算操作之一。<br />无论是 <strong>卷积</strong>、<strong>矩阵乘法</strong> 还是 <strong>深度学习中的 GEMM</strong>，其本质都是大量的 MAC 运算。</p><p>在 ARM 架构下，针对不同的数据类型和应用场景，演进出了多种 <strong>乘累加指令</strong>。本文将对常见的几类进行梳理，并配合伪代码展示它们的计算方式。</p><hr /><h2 id="1-fmla-基础乘累加"><a class="markdownIt-Anchor" href="#1-fmla-基础乘累加"></a> 1. FMLA —— 基础乘累加</h2><p><strong>FMLA (Floating-point Multiply-Accumulate)</strong> 是最基础的向量乘累加指令。</p><ul><li><p><strong>指令集</strong>：<code>&gt;= ARMv7 (float32)</code>，<code>&gt;= ARMv8 (float16)</code></p></li><li><p><strong>常见 NEON 接口</strong>：</p><ul><li><code>float32x4_t vfmaq_f32(float32x4_t a, float32x4_t b, float32x4_t c)</code></li><li><code>float16x8_t vfmaq_f16(float16x8_t a, float16x8_t b, float16x8_t c)</code></li></ul></li><li><p><strong>运算逻辑</strong>：逐元素乘加</p><pre class="line-numbers language-c" data-language="c"><code class="language-c"><span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> i <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> i <span class="token operator">&lt;</span> N<span class="token punctuation">;</span> i<span class="token operator">++</span><span class="token punctuation">)</span> <span class="token punctuation">&#123;</span>    dst<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">+=</span> src1<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">*</span> src2<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">;</span><span class="token punctuation">&#125;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre></li></ul><blockquote><p>典型用途：浮点向量加速，如卷积核点积、向量化计算。</p></blockquote><hr /><h2 id="2-dot-向量点积"><a class="markdownIt-Anchor" href="#2-dot-向量点积"></a> 2. DOT —— 向量点积</h2><p>为了更高效地处理 <strong>int8 量化推理</strong>，ARM 在 <strong>ARMv8.2</strong> 中引入了 <strong>DOT (Dot Product)</strong> 指令。</p><ul><li><p><strong>指令集</strong>：<code>&gt;= ARMv8.2</code></p></li><li><p><strong>常见 NEON 接口</strong>：</p><ul><li><code>int32x4_t vdotq_s32(int32x4_t r, int8x16_t a, int8x16_t b)</code></li><li><code>int32x4_t vusdotq_s32(int32x4_t r, uint8x16_t a, int8x16_t b)</code></li></ul></li><li><p><strong>运算逻辑</strong>：分块点积 (4 × 4 → int32)</p><pre class="line-numbers language-c" data-language="c"><code class="language-c"><span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> i <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> i <span class="token operator">&lt;</span> <span class="token number">4</span><span class="token punctuation">;</span> i<span class="token operator">++</span><span class="token punctuation">)</span> <span class="token punctuation">&#123;</span>    <span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> j <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> j <span class="token operator">&lt;</span> <span class="token number">4</span><span class="token punctuation">;</span> j<span class="token operator">++</span><span class="token punctuation">)</span> <span class="token punctuation">&#123;</span>        dst<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">+=</span> src1<span class="token punctuation">[</span>i <span class="token operator">*</span> <span class="token number">4</span> <span class="token operator">+</span> j<span class="token punctuation">]</span> <span class="token operator">*</span> src2<span class="token punctuation">[</span>i <span class="token operator">*</span> <span class="token number">4</span> <span class="token operator">+</span> j<span class="token punctuation">]</span><span class="token punctuation">;</span>    <span class="token punctuation">&#125;</span><span class="token punctuation">&#125;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre></li></ul><blockquote><p>典型用途：卷积、点积，特别适合 int8 量化模型。</p></blockquote><hr /><h2 id="3-mmla-矩阵乘累加"><a class="markdownIt-Anchor" href="#3-mmla-矩阵乘累加"></a> 3. MMLA —— 矩阵乘累加</h2><p>在 <strong>ARMv8.6</strong> 之后，进一步推出了 <strong>MMLA (Matrix Multiply-Accumulate)</strong> 指令，可以直接把 <strong>小块向量视为矩阵</strong>，一次性完成 <strong>2×8 × 8×2</strong> 的矩阵乘法。</p><ul><li><p><strong>指令集</strong>：<code>&gt;= ARMv8.6</code></p></li><li><p><strong>常见 NEON 接口</strong>：</p><ul><li><code>int32x4_t vmmlaq_s32(int32x4_t r, int8x16_t a, int8x16_t b)</code></li><li><code>int32x4_t vusmmlaq_s32(int32x4_t r, uint8x16_t a, int8x16_t b)</code></li></ul></li><li><p><strong>运算逻辑</strong>：小矩阵乘法</p><pre class="line-numbers language-c" data-language="c"><code class="language-c"><span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> i <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> i <span class="token operator">&lt;</span> <span class="token number">2</span><span class="token punctuation">;</span> i<span class="token operator">++</span><span class="token punctuation">)</span> <span class="token punctuation">&#123;</span>    <span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> j <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> j <span class="token operator">&lt;</span> <span class="token number">2</span><span class="token punctuation">;</span> j<span class="token operator">++</span><span class="token punctuation">)</span> <span class="token punctuation">&#123;</span>        <span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> k <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> k <span class="token operator">&lt;</span> <span class="token number">8</span><span class="token punctuation">;</span> k<span class="token operator">++</span><span class="token punctuation">)</span> <span class="token punctuation">&#123;</span>            dst<span class="token punctuation">[</span>i <span class="token operator">*</span> <span class="token number">2</span> <span class="token operator">+</span> j<span class="token punctuation">]</span> <span class="token operator">+=</span> src1<span class="token punctuation">[</span>i <span class="token operator">*</span> <span class="token number">8</span> <span class="token operator">+</span> k<span class="token punctuation">]</span> <span class="token operator">*</span> src2<span class="token punctuation">[</span>j <span class="token operator">*</span> <span class="token number">8</span> <span class="token operator">+</span> k<span class="token punctuation">]</span><span class="token punctuation">;</span>        <span class="token punctuation">&#125;</span>    <span class="token punctuation">&#125;</span><span class="token punctuation">&#125;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li></ul><blockquote><p>典型用途：高效的 int8 GEMM，加速深度学习推理。</p></blockquote><hr /><h2 id="4-bfmmla-bfloat16-矩阵乘累加"><a class="markdownIt-Anchor" href="#4-bfmmla-bfloat16-矩阵乘累加"></a> 4. BFMMLA —— BFloat16 矩阵乘累加</h2><p>为了更好地支持 <strong>AI 训练与推理</strong>，ARMv8.6 又引入了 <strong>BFMMLA (BFloat16 Matrix Multiply-Accumulate)</strong>，专门处理 <strong>bfloat16 × bfloat16 → float32</strong>。</p><ul><li><p><strong>指令集</strong>：<code>&gt;= ARMv8.6</code></p></li><li><p><strong>常见 NEON 接口</strong>：</p><ul><li><code>float32x4_t vbfmmlaq_f32(float32x4_t r, bfloat16x8_t a, bfloat16x8_t b)</code></li></ul></li><li><p><strong>运算逻辑</strong>：小矩阵乘法</p><pre class="line-numbers language-c" data-language="c"><code class="language-c"><span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> i <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> i <span class="token operator">&lt;</span> <span class="token number">2</span><span class="token punctuation">;</span> i<span class="token operator">++</span><span class="token punctuation">)</span> <span class="token punctuation">&#123;</span>    <span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> j <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> j <span class="token operator">&lt;</span> <span class="token number">2</span><span class="token punctuation">;</span> j<span class="token operator">++</span><span class="token punctuation">)</span> <span class="token punctuation">&#123;</span>        <span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> k <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> k <span class="token operator">&lt;</span> <span class="token number">4</span><span class="token punctuation">;</span> k<span class="token operator">++</span><span class="token punctuation">)</span> <span class="token punctuation">&#123;</span>            dst<span class="token punctuation">[</span>i <span class="token operator">*</span> <span class="token number">2</span> <span class="token operator">+</span> j<span class="token punctuation">]</span> <span class="token operator">+=</span> src1<span class="token punctuation">[</span>i <span class="token operator">*</span> <span class="token number">4</span> <span class="token operator">+</span> k<span class="token punctuation">]</span> <span class="token operator">*</span> src2<span class="token punctuation">[</span>j <span class="token operator">*</span> <span class="token number">4</span> <span class="token operator">+</span> k<span class="token punctuation">]</span><span class="token punctuation">;</span>        <span class="token punctuation">&#125;</span>    <span class="token punctuation">&#125;</span><span class="token punctuation">&#125;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li></ul><blockquote><p>典型用途：深度学习训练（bfloat16 是目前主流 AI 训练精度格式）。</p></blockquote><table><thead><tr><th>指令</th><th>指令集</th><th>数据类型</th><th>运算规模</th><th>主要用途</th></tr></thead><tbody><tr><td><strong>FMLA</strong></td><td>ARMv7/ARMv8</td><td>FP16/FP32</td><td>逐元素</td><td>浮点向量运算</td></tr><tr><td><strong>SDOT/USDOT</strong></td><td>ARMv8.2</td><td>int8/uint8 → int32</td><td>4×4 点积</td><td>int8 卷积、点积</td></tr><tr><td><strong>SMMLA/USMMLA</strong></td><td>ARMv8.6</td><td>int8/uint8 → int32</td><td>2×2×8 矩阵</td><td>int8 GEMM</td></tr><tr><td><strong>BFMMLA</strong></td><td>ARMv8.6</td><td>bfloat16 → float32</td><td>2×2×4 矩阵</td><td>深度学习 (bfloat16)</td></tr></tbody></table><hr />]]></content>
      
      
      
        <tags>
            
            <tag> ARM </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>flash attention</title>
      <link href="/2024/12/flashattention/"/>
      <url>/2024/12/flashattention/</url>
      
        <content type="html"><![CDATA[<h1 id="flashattention"><a class="markdownIt-Anchor" href="#flashattention"></a> FlashAttention</h1><p><a href="https://courses.cs.washington.edu/courses/cse599m/23sp/notes/flashattn.pdf">https://courses.cs.washington.edu/courses/cse599m/23sp/notes/flashattn.pdf</a></p><h2 id="self-attention"><a class="markdownIt-Anchor" href="#self-attention"></a> self-Attention</h2><p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mtable width="100%"><mtr><mtd width="50%"></mtd><mtd><mrow><mi>O</mi><mo>=</mo><mi>s</mi><mi>o</mi><mi>f</mi><mi>t</mi><mi>m</mi><mi>a</mi><mi>x</mi><mo stretchy="false">(</mo><mi>Q</mi><msup><mi>K</mi><mi>T</mi></msup><mo stretchy="false">)</mo><mi>V</mi></mrow></mtd><mtd width="50%"></mtd><mtd><mtext>(1)</mtext></mtd></mtr></mtable><annotation encoding="application/x-tex">O = softmax(QK^T)V \tag{1}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.02778em;">O</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1.1413309999999999em;vertical-align:-0.25em;"></span><span class="mord mathdefault">s</span><span class="mord mathdefault">o</span><span class="mord mathdefault" style="margin-right:0.10764em;">f</span><span class="mord mathdefault">t</span><span class="mord mathdefault">m</span><span class="mord mathdefault">a</span><span class="mord mathdefault">x</span><span class="mopen">(</span><span class="mord mathdefault">Q</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.07153em;">K</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8913309999999999em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.13889em;">T</span></span></span></span></span></span></span></span><span class="mclose">)</span><span class="mord mathdefault" style="margin-right:0.22222em;">V</span></span><span class="tag"><span class="strut" style="height:1.1413309999999999em;vertical-align:-0.25em;"></span><span class="mord text"><span class="mord">(</span><span class="mord"><span class="mord">1</span></span><span class="mord">)</span></span></span></span></span></span></p><h2 id="softmax"><a class="markdownIt-Anchor" href="#softmax"></a> softmax</h2><img src="https://raw.githubusercontent.com/weikangqi/picgo/main/image-20250904152025097.png" alt="image-20250904152025097" style="zoom: 50%;" /><p><span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>x</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">x_{i}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">i</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 如果比较大，<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msup><mi>e</mi><msub><mi>x</mi><mi>i</mi></msub></msup></mrow><annotation encoding="application/x-tex">e^{x_{i}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.664392em;vertical-align:0em;"></span><span class="mord"><span class="mord mathdefault">e</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.664392em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathdefault mtight">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3280857142857143em;"><span style="top:-2.357em;margin-left:0em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">i</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em;"><span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span> 可能会溢出，比如float16 最大支持65536，当x&gt;11, <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msup><mi>e</mi><mi>x</mi></msup></mrow><annotation encoding="application/x-tex">e^{x}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.664392em;vertical-align:0em;"></span><span class="mord"><span class="mord mathdefault">e</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.664392em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">x</span></span></span></span></span></span></span></span></span></span></span></span> 就超出了float16的表示范围。</p><p>为了解决这个问题，可以上下除以最大值，来解决溢出问题。</p><img src="https://raw.githubusercontent.com/weikangqi/picgo/main/image-20250904152357842.png" alt="image-20250904152357842" style="zoom: 50%;" /><h2 id="safe-softmax-3-pass"><a class="markdownIt-Anchor" href="#safe-softmax-3-pass"></a> Safe softmax (3-Pass)</h2><ul><li>第一步 计算m 最大值</li><li>第二步 计算d 也就是分母，求和</li><li>第三步 分子/分母求每个值</li></ul><p>需要3个循环，来访问[1,N]。</p><h2 id="online-sofamax-2-pass"><a class="markdownIt-Anchor" href="#online-sofamax-2-pass"></a> Online sofamax (2-Pass)</h2><p><strong>将第一步和第二步合成一个pass</strong></p><p>依赖当前最大值<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>m</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">m_{i}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault">m</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">i</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>和当前sum值<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>d</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">d_{i}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.84444em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">i</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span></p><img src="https://raw.githubusercontent.com/weikangqi/picgo/main/image-20250904154033846.png" alt="image-20250904154033846" style="zoom:50%;" /><img src="https://raw.githubusercontent.com/weikangqi/picgo/main/image-20250904153909833.png" alt="image-20250904153909833" style="zoom: 50%;" /><h2 id="flashattention-1-pass"><a class="markdownIt-Anchor" href="#flashattention-1-pass"></a> FlashAttention (1-Pass)</h2><img src="https://raw.githubusercontent.com/weikangqi/picgo/main/image-20250904160034426.png" alt="image-20250904160034426" style="zoom: 50%;" /><p><span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>x</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">x_{i}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">i</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 等于<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>Q</mi></mrow><annotation encoding="application/x-tex">Q</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8777699999999999em;vertical-align:-0.19444em;"></span><span class="mord mathdefault">Q</span></span></span></span>的<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>k</mi></mrow><annotation encoding="application/x-tex">k</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.03148em;">k</span></span></span></span>行乘以<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msup><mi>K</mi><mi>T</mi></msup></mrow><annotation encoding="application/x-tex">K^{T}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8413309999999999em;vertical-align:0em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.07153em;">K</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8413309999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.13889em;">T</span></span></span></span></span></span></span></span></span></span></span></span>的<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>i</mi></mrow><annotation encoding="application/x-tex">i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.65952em;vertical-align:0em;"></span><span class="mord mathdefault">i</span></span></span></span>列。</p><img src="https://raw.githubusercontent.com/weikangqi/picgo/main/image-20250904160413940.png" alt="image-20250904160413940" style="zoom:50%;" /><img src="https://raw.githubusercontent.com/weikangqi/picgo/main/image-20250904160509203.png" alt="image-20250904160509203" style="zoom: 50%;" /><img src="https://raw.githubusercontent.com/weikangqi/picgo/main/image-20250904161026687.png" alt="image-20250904161026687" style="zoom:50%;" /><p>总结：</p><p>softmax 可以做成流式计算，把softmax的分母计算，也就是求和计算和求最大值计算融入到一个pass中，不依赖全局的最大值，而是局部的最大值。借助sharememory来存储中间值，这样就2pass。但是flashattention 可以。softmax之后和v相乘累加，满足加法结合率。对于MQA，GQA通过index的方法来加载KVcache计算，而不是copy一份。</p>]]></content>
      
      
      
        <tags>
            
            <tag> ARM </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ARM 内联汇编</title>
      <link href="/2024/11/%E5%86%85%E8%81%94%E6%B1%87%E7%BC%96/"/>
      <url>/2024/11/%E5%86%85%E8%81%94%E6%B1%87%E7%BC%96/</url>
      
        <content type="html"><![CDATA[<h1 id="arm-内联汇编"><a class="markdownIt-Anchor" href="#arm-内联汇编"></a> ARM 内联汇编</h1><p><a href="https://www.ic.unicamp.br/~celio/mc404-s2-2015/docs/ARM-GCC-Inline-Assembler-Cookbook.pdf">ARM-GCC-Inline-Assembler-Cookbook.pdf</a></p><h2 id="基本结构"><a class="markdownIt-Anchor" href="#基本结构"></a> 基本结构</h2><pre class="line-numbers language-Cpp" data-language="Cpp"><code class="language-Cpp">asm volatile(    &quot;汇编指令&quot;     : 输出操作数     : 输入操作数     : 被修改的寄存器);<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="input-and-output-operands"><a class="markdownIt-Anchor" href="#input-and-output-operands"></a> Input and Output Operands</h2><table><thead><tr><th>修饰符</th><th>限定</th></tr></thead><tbody><tr><td>=</td><td>只写操作数，通常用于所有输出操作数。</td></tr><tr><td>+</td><td>读写操作数（内联汇编器不支持）</td></tr><tr><td>&amp;</td><td>寄存器只能用于输出</td></tr></tbody></table><p><strong><code>=</code></strong>：表示该操作数是输出操作数，指示编译器为其分配寄存器，并将结果存入该寄存器。</p><p><strong><code>&amp;</code></strong>：确保输出操作数分配一个新的寄存器，避免与其他操作数冲突。</p><h3 id="为什么需要"><a class="markdownIt-Anchor" href="#为什么需要"></a> 为什么需要 <code>&amp;</code>？</h3><ul><li><p><strong>确保寄存器不会作为输入和输出的寄存器</strong>：<code>&amp;</code> 确保寄存器被分配为<strong>仅用于输出</strong>。在没有 <code>&amp;</code> 的情况下，编译器可能会选择一个已经作为输入使用的寄存器来作为输出寄存器，这样会导致寄存器冲突或者覆盖输入数据。</p><p>例如，假设我们不使用 <code>&amp;</code>：</p><pre class="line-numbers language-none"><code class="language-none">asm(&quot;add %0, %1, %2&quot; : &quot;&#x3D;r&quot;(c) : &quot;r&quot;(a), &quot;r&quot;(b));<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>在这种情况下，编译器可能会选择 <code>a</code> 或 <code>b</code> 的寄存器来存储 <code>c</code>，而这可能会覆盖其中一个输入寄存器的值。如果不希望发生这种情况，可以使用 <code>&amp;</code> 来强制为输出分配一个新的寄存器。</p></li></ul><h2 id="在汇编代码中引用操作数"><a class="markdownIt-Anchor" href="#在汇编代码中引用操作数"></a> 在汇编代码中引用操作数</h2><p>方法一： <code>&quot;0&quot;</code>、<code>&quot;1&quot;</code>、<code>&quot;2&quot;</code> 等约束符</p><pre class="line-numbers language-Cpp" data-language="Cpp"><code class="language-Cpp">int a &#x3D; 5, b &#x3D; 3, result;asm volatile (    &quot;ADD %0, %1, %2\n&quot;   &#x2F;&#x2F; result &#x3D; a + b    : &quot;&#x3D;r&quot; (result), &quot;&#x3D;r&quot; (b)  &#x2F;&#x2F; 输出部分：result 和 b 都是输出，且要绑定到同一寄存器    : &quot;0&quot; (a), &quot;r&quot; (b)    &#x2F;&#x2F; 输入部分：a 使用第一个输出操作数的寄存器，b 使用不同寄存器);<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>在这个例子中：</p><ul><li><code>&quot;0&quot;</code> 约束符表示 <code>a</code> 和第一个输出操作数（<code>result</code>）共享同一个寄存器。</li><li><code>b</code> 会被分配到一个独立的寄存器中，因为它在输入部分使用了 <code>&quot;r&quot;</code> 约束符。</li></ul><p>方法二：%[操作数名称]</p><pre class="line-numbers language-Cpp" data-language="Cpp"><code class="language-Cpp">int res &#x3D; 0;&#x2F;&#x2F; result，input_i，input_j 就是操作数名称__asm (&quot;ADD %[result], %[input_i], %[input_j]&quot;    : [result] &quot;&#x3D;r&quot; (res)    : [input_i] &quot;r&quot; (i), [input_j] &quot;r&quot; (j));<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="clobber-列表"><a class="markdownIt-Anchor" href="#clobber-列表"></a> clobber 列表</h2><p>在内联汇编中，<code>memory</code> 关键字用于告知编译器：</p><ul><li>汇编代码可能会对内存进行 <strong>读写操作</strong>，并且这些操作可能会影响外部变量。</li><li>编译器需要对内存进行适当的优化，避免错误的寄存器重排（register reordering）或缓存（cache）行为，确保内存操作的正确性。</li></ul><p>通常，<code>memory</code> 关键字在以下情况下使用：</p><ul><li>当汇编代码显式地 <strong>修改内存</strong>，例如通过 <code>store</code>（存储）或 <code>load</code>（加载）指令。</li><li>当内联汇编代码可能会改变程序中某些变量的值，尤其是那些 <strong>未直接出现在汇编代码中的变量</strong>。</li></ul><p>使用 <code>memory</code> 时，编译器会考虑到 <strong>内存屏障</strong> 和 <strong>数据依赖性</strong>，并确保汇编操作与其上下文的一致性。</p><p>memory 强制 gcc 编译器假设 RAM 所有内存单元均被汇编指令修改，这样 cpu 中的 registers 和 cache 中已缓存的内存单元中的数据将作废。cpu 将不得不在需要的时候重新读取内存中的数据。这就阻止了 cpu 又将 registers, cache 中的数据用于去优化指令，而避免去访问内存。</p><h4 id="语法示例"><a class="markdownIt-Anchor" href="#语法示例"></a> 语法示例：</h4><pre class="line-numbers language-Cpp" data-language="Cpp"><code class="language-Cpp">asm volatile (    &quot;prfm   pldl1keep, [%0, #512]   \n&quot;    &quot;ld1    &#123;v0.4s, v1.4s, v2.4s, v3.4s&#125;, [%0] \n&quot;    &quot;fabs   v0.4s, v0.4s            \n&quot;    &quot;fabs   v1.4s, v1.4s            \n&quot;    &quot;fabs   v2.4s, v2.4s            \n&quot;    &quot;fabs   v3.4s, v3.4s            \n&quot;    &quot;st1    &#123;v0.4s, v1.4s, v2.4s, v3.4s&#125;, [%0], #64 \n&quot;    : &quot;&#x3D;r&quot;(ptr) &#x2F;&#x2F; 输出部分    : &quot;0&quot;(ptr)  &#x2F;&#x2F; 输入部分    : &quot;memory&quot;, &quot;v0&quot;, &quot;v1&quot;, &quot;v2&quot;, &quot;v3&quot;  &#x2F;&#x2F; clobber 部分);<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>在这个例子中：</p><ul><li><code>&quot;memory&quot;</code> 表示内联汇编中 <strong>会修改内存</strong>。在这里，内存的修改是通过 <code>ld1</code>（加载）和 <code>st1</code>（存储）指令完成的，它们会访问 <code>ptr</code> 指向的内存。</li><li><code>&quot;v0&quot;, &quot;v1&quot;, &quot;v2&quot;, &quot;v3&quot;</code> 表示这些寄存器会被修改，编译器需要确保这些寄存器不会被错误地优化。</li></ul><p>此外</p><ul><li><code>cc</code> 表示内联汇编代码修改了标志寄存器</li></ul>]]></content>
      
      
      
        <tags>
            
            <tag> ARM </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>opencl cl_arm_import_memory</title>
      <link href="/2024/08/opencl%20cl_arm_import_memory/"/>
      <url>/2024/08/opencl%20cl_arm_import_memory/</url>
      
        <content type="html"><![CDATA[<h1 id="cl_arm_import_memory"><a class="markdownIt-Anchor" href="#cl_arm_import_memory"></a> cl_arm_import_memory</h1><p>在移动端的 GPU 计算场景中，性能和功耗往往是开发者最关心的问题。以 ARM 的 Mali GPU 为例，当我们在 OpenCL 中处理来自相机、视频解码器或其他外设的数据时，经常会遇到这样一个瓶颈：<strong>数据需要在 CPU 和 GPU 之间频繁拷贝</strong>。这种额外的内存搬运不仅浪费带宽，也增加了延迟和功耗。</p><p>为了解决这一问题，ARM 提出了一个扩展 —— <strong><code>cl_arm_import_memory</code></strong>。该扩展允许开发者将外部内存直接导入到 OpenCL 中使用，实现真正的 <strong>零拷贝（zero-copy）</strong> 数据处理。</p><hr /><h2 id="1-为什么需要-cl_arm_import_memory"><a class="markdownIt-Anchor" href="#1-为什么需要-cl_arm_import_memory"></a> 1. 为什么需要 cl_arm_import_memory？</h2><p>在标准的 OpenCL 中，常用的内存分配函数是 <code>clCreateBuffer</code> 和 <code>clCreateImage</code>。这些接口通常会在 GPU 管理的显存区域分配一块空间。如果外部数据（例如相机帧缓冲区）想要被 GPU 使用，通常的做法是：</p><ol><li>驱动层分配 buffer（例如 dma-buf）。</li><li>应用层通过 CPU 将数据拷贝到 OpenCL buffer。</li><li>GPU kernel 处理这块 buffer。</li></ol><p>这种流程至少包含一次拷贝操作，对于实时性要求高的场景（如视频流处理、人脸识别、AR/VR 等）是不可接受的。</p><p><code>cl_arm_import_memory</code> 的出现，正是为了解决这一痛点：它允许我们直接把外部内存导入为 OpenCL 的 <code>cl_mem</code> 对象，从而让 GPU 直接访问外设提供的 buffer，避免拷贝。</p><hr /><h2 id="2-api-概览"><a class="markdownIt-Anchor" href="#2-api-概览"></a> 2. API 概览</h2><p>扩展提供了一个新的接口：</p><pre class="line-numbers language-c" data-language="c"><code class="language-c">cl_mem <span class="token function">clImportMemoryARM</span><span class="token punctuation">(</span>    cl_context context<span class="token punctuation">,</span>    cl_mem_flags flags<span class="token punctuation">,</span>    <span class="token keyword">const</span> cl_import_properties_arm <span class="token operator">*</span>properties<span class="token punctuation">,</span>    <span class="token keyword">void</span> <span class="token operator">*</span>memory<span class="token punctuation">,</span>    <span class="token class-name">size_t</span> size<span class="token punctuation">,</span>    cl_int <span class="token operator">*</span>errcode_ret<span class="token punctuation">)</span><span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>参数说明：</p><ul><li><strong>context</strong>：OpenCL 上下文。</li><li><strong>flags</strong>：访问权限，和 <code>clCreateBuffer</code> 类似。</li><li><strong>properties</strong>：描述导入内存的类型。</li><li><strong>memory</strong>：外部内存对象，可能是一个指针，也可能是一个文件描述符（fd）。</li><li><strong>size</strong>：内存大小。</li><li><strong>errcode_ret</strong>：返回错误码。</li></ul><p>在调用前，需要通过 <code>clGetDeviceInfo</code> 确认设备是否支持该扩展：</p><pre class="line-numbers language-c" data-language="c"><code class="language-c"><span class="token keyword">char</span> ext<span class="token punctuation">[</span><span class="token number">1024</span><span class="token punctuation">]</span><span class="token punctuation">;</span><span class="token function">clGetDeviceInfo</span><span class="token punctuation">(</span>device<span class="token punctuation">,</span> CL_DEVICE_EXTENSIONS<span class="token punctuation">,</span> <span class="token keyword">sizeof</span><span class="token punctuation">(</span>ext<span class="token punctuation">)</span><span class="token punctuation">,</span> ext<span class="token punctuation">,</span> <span class="token constant">NULL</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token keyword">if</span> <span class="token punctuation">(</span><span class="token function">strstr</span><span class="token punctuation">(</span>ext<span class="token punctuation">,</span> <span class="token string">"cl_arm_import_memory"</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token punctuation">&#123;</span>    <span class="token function">printf</span><span class="token punctuation">(</span><span class="token string">"设备支持 cl_arm_import_memory\n"</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token punctuation">&#125;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><hr /><h2 id="3-内存类型与属性配置"><a class="markdownIt-Anchor" href="#3-内存类型与属性配置"></a> 3. 内存类型与属性配置</h2><p><code>cl_arm_import_memory</code> 扩展允许导入不同类型的外部内存，具体通过 <code>properties</code> 进行配置。常见的几种类型包括：</p><table><thead><tr><th>属性</th><th>含义</th></tr></thead><tbody><tr><td><code>CL_IMPORT_TYPE_ARM</code></td><td>指定导入类型标志</td></tr><tr><td><code>CL_IMPORT_TYPE_HOST_ARM</code></td><td>从主机指针导入</td></tr><tr><td><code>CL_IMPORT_TYPE_DMA_BUF_ARM</code></td><td>从 dma-buf fd 导入</td></tr><tr><td><code>CL_IMPORT_TYPE_ANDROID_HARDWARE_BUFFER_ARM</code></td><td>从 Android AHardwareBuffer 导入</td></tr></tbody></table><p>配置示例：</p><pre class="line-numbers language-c" data-language="c"><code class="language-c"><span class="token keyword">const</span> cl_import_properties_arm props<span class="token punctuation">[</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token punctuation">&#123;</span>    CL_IMPORT_TYPE_ARM<span class="token punctuation">,</span> CL_IMPORT_TYPE_DMA_BUF_ARM<span class="token punctuation">,</span>    <span class="token number">0</span><span class="token punctuation">&#125;</span><span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><hr /><h2 id="4-使用示例导入-dma-buf"><a class="markdownIt-Anchor" href="#4-使用示例导入-dma-buf"></a> 4. 使用示例：导入 dma-buf</h2><p>假设我们已经从相机驱动或 V4L2 拿到了一个 dma-buf 文件描述符，可以直接将其导入 OpenCL：</p><pre class="line-numbers language-c" data-language="c"><code class="language-c"><span class="token keyword">int</span> dma_fd <span class="token operator">=</span> <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">;</span>   <span class="token comment">// 相机/解码器提供的 fd</span><span class="token class-name">size_t</span> size <span class="token operator">=</span> <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">;</span>  <span class="token comment">// buffer 大小</span><span class="token keyword">const</span> cl_import_properties_arm props<span class="token punctuation">[</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token punctuation">&#123;</span>    CL_IMPORT_TYPE_ARM<span class="token punctuation">,</span> CL_IMPORT_TYPE_DMA_BUF_ARM<span class="token punctuation">,</span>    <span class="token number">0</span><span class="token punctuation">&#125;</span><span class="token punctuation">;</span>cl_int err<span class="token punctuation">;</span>cl_mem buf <span class="token operator">=</span> <span class="token function">clImportMemoryARM</span><span class="token punctuation">(</span>context<span class="token punctuation">,</span>                               CL_MEM_READ_WRITE<span class="token punctuation">,</span>                               props<span class="token punctuation">,</span>                               <span class="token punctuation">(</span><span class="token keyword">void</span><span class="token operator">*</span><span class="token punctuation">)</span><span class="token punctuation">(</span><span class="token class-name">uintptr_t</span><span class="token punctuation">)</span>dma_fd<span class="token punctuation">,</span> <span class="token comment">// 传入 fd</span>                               size<span class="token punctuation">,</span>                               <span class="token operator">&amp;</span>err<span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token keyword">if</span> <span class="token punctuation">(</span>err <span class="token operator">!=</span> CL_SUCCESS<span class="token punctuation">)</span> <span class="token punctuation">&#123;</span>    <span class="token function">printf</span><span class="token punctuation">(</span><span class="token string">"导入失败，错误码=%d\n"</span><span class="token punctuation">,</span> err<span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token punctuation">&#125;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>此时 <code>buf</code> 已经是一个合法的 <code>cl_mem</code> 对象，背后对应的就是 dma-buf。GPU 可以直接处理这块内存，无需额外拷贝。</p><p>在 Kernel 中的使用方式与普通 <code>cl_mem</code> 相同，例如：</p><pre class="line-numbers language-opencl" data-language="opencl"><code class="language-opencl"><span class="token keyword">__kernel</span> <span class="token keyword">void</span> <span class="token function">process</span><span class="token punctuation">(</span><span class="token keyword">__global</span> <span class="token keyword">uchar</span><span class="token operator">*</span> data<span class="token punctuation">)</span> <span class="token punctuation">&#123;</span>    <span class="token keyword">int</span> gid <span class="token operator">=</span> <span class="token function">get_global_id</span><span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    data<span class="token punctuation">[</span>gid<span class="token punctuation">]</span> <span class="token operator">=</span> data<span class="token punctuation">[</span>gid<span class="token punctuation">]</span> <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">;</span><span class="token punctuation">&#125;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><hr /><h2 id="5-注意事项"><a class="markdownIt-Anchor" href="#5-注意事项"></a> 5. 注意事项</h2><ul><li><p>调用 <code>clReleaseMemObject</code> 只会释放 OpenCL 的引用计数，不会关闭原始的 <code>dma_fd</code>。应用层仍需手动调用 <code>close(fd)</code>。</p></li><li><p>当 CPU 和 GPU 同时访问 dma-buf 时，需要保证同步，否则可能出现数据竞争。常见做法：</p><ul><li>使用 Linux 内核的 <strong>dma-fence</strong>。</li><li>在 OpenCL 中使用 <code>clEnqueueAcquire/Release</code> 控制访问。</li></ul></li><li><p>并非所有 Mali GPU 驱动都支持 <code>cl_arm_import_memory</code>，特别是旧版本可能仅支持 host pointer 导入。实际使用前需通过 <code>clGetDeviceInfo</code> 检查扩展。</p></li></ul>]]></content>
      
      
      
        <tags>
            
            <tag> opencl </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Roofine Model</title>
      <link href="/2024/08/Roofline%20Model%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%80%A7%E8%83%BD%E5%88%86%E6%9E%90/"/>
      <url>/2024/08/Roofline%20Model%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%80%A7%E8%83%BD%E5%88%86%E6%9E%90/</url>
      
        <content type="html"><![CDATA[<p><a href="https://zhuanlan.zhihu.com/p/34204282?utm_psn=1783444641103765504">Roofline Model与深度学习模型的性能分析 - 知乎 (zhihu.com)</a></p><p>最近在不同的计算平台上验证几种经典深度学习模型的训练和预测性能时，经常遇到模型的实际测试性能表现和自己计算出的复杂度并不完全吻合的现象，令人十分困惑。机缘巧合听了Momenta的技术分享后，我意识到问题的答案其实就在于 Roof-line Model 这个理论，于是认真研究了一下相关论文。现在把自己的心得总结出来，分享给大家。</p><p>在真实世界中，任何模型（例如 VGG / MobileNet 等）都必须依赖于具体的计算平台（例如CPU / GPU / ASIC 等）才能展现自己的实力。此时，模型和计算平台的&quot;默契程度&quot;会决定模型的实际表现。<strong>Roofline Model 提出了使用 Operational Intensity（计算强度）进行定量分析的方法，并给出了模型在计算平台上所能达到理论计算性能上限公式。</strong></p><p>有了 Roofline Model，我就可以知道模型在机器上能跑多快喽～做梦都会笑出声来～</p><hr /><h2 id="1-计算平台的两个指标算力-π-与带宽-β"><a class="markdownIt-Anchor" href="#1-计算平台的两个指标算力-π-与带宽-β"></a> 1. 计算平台的两个指标：算力 π 与带宽 β</h2><ul><li><strong>算力</strong> π ：也称为计算平台的<strong>性能上限</strong>，指的是一个计算平台倾尽全力每秒钟所能完成的浮点运算数。单位是 <code>FLOPS</code> or <code>FLOP/s</code>。</li></ul><p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>π</mi><mo>:</mo><mi>M</mi><mi>a</mi><mi>x</mi><mi>i</mi><mi>m</mi><mi>u</mi><mi>m</mi><mi>F</mi><mi>L</mi><mi>O</mi><mi>P</mi><mi>s</mi><mi>P</mi><mi>e</mi><mi>r</mi><mi>S</mi><mi>e</mi><mi>c</mi><mi>o</mi><mi>n</mi><mi>d</mi></mrow><annotation encoding="application/x-tex">π:Maximum FLOPs Per Second</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.03588em;">π</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">:</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.10903em;">M</span><span class="mord mathdefault">a</span><span class="mord mathdefault">x</span><span class="mord mathdefault">i</span><span class="mord mathdefault">m</span><span class="mord mathdefault">u</span><span class="mord mathdefault">m</span><span class="mord mathdefault" style="margin-right:0.13889em;">F</span><span class="mord mathdefault">L</span><span class="mord mathdefault" style="margin-right:0.02778em;">O</span><span class="mord mathdefault" style="margin-right:0.13889em;">P</span><span class="mord mathdefault">s</span><span class="mord mathdefault" style="margin-right:0.13889em;">P</span><span class="mord mathdefault">e</span><span class="mord mathdefault" style="margin-right:0.02778em;">r</span><span class="mord mathdefault" style="margin-right:0.05764em;">S</span><span class="mord mathdefault">e</span><span class="mord mathdefault">c</span><span class="mord mathdefault">o</span><span class="mord mathdefault">n</span><span class="mord mathdefault">d</span></span></span></span></span></p><ul><li><strong>带宽</strong> β ：也即计算平台的<strong>带宽上限</strong>，指的是一个计算平台倾尽全力每秒所能完成的内存交换量。单位是<code>Byte/s</code>。</li></ul><p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>β</mi><mo>:</mo><mi>M</mi><mi>a</mi><mi>x</mi><mi>i</mi><mi>m</mi><mi>u</mi><mi>m</mi><mi>M</mi><mi>e</mi><mi>m</mi><mi>o</mi><mi>r</mi><mi>y</mi><mi>A</mi><mi>c</mi><mi>c</mi><mi>e</mi><mi>s</mi><mi>s</mi><mi>P</mi><mi>e</mi><mi>r</mi><mi>S</mi><mi>e</mi><mi>c</mi><mi>o</mi><mi>n</mi><mi>d</mi></mrow><annotation encoding="application/x-tex">β:Maximum Memory Access Per Second</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="mord mathdefault" style="margin-right:0.05278em;">β</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">:</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="mord mathdefault" style="margin-right:0.10903em;">M</span><span class="mord mathdefault">a</span><span class="mord mathdefault">x</span><span class="mord mathdefault">i</span><span class="mord mathdefault">m</span><span class="mord mathdefault">u</span><span class="mord mathdefault">m</span><span class="mord mathdefault" style="margin-right:0.10903em;">M</span><span class="mord mathdefault">e</span><span class="mord mathdefault">m</span><span class="mord mathdefault">o</span><span class="mord mathdefault" style="margin-right:0.02778em;">r</span><span class="mord mathdefault" style="margin-right:0.03588em;">y</span><span class="mord mathdefault">A</span><span class="mord mathdefault">c</span><span class="mord mathdefault">c</span><span class="mord mathdefault">e</span><span class="mord mathdefault">s</span><span class="mord mathdefault">s</span><span class="mord mathdefault" style="margin-right:0.13889em;">P</span><span class="mord mathdefault">e</span><span class="mord mathdefault" style="margin-right:0.02778em;">r</span><span class="mord mathdefault" style="margin-right:0.05764em;">S</span><span class="mord mathdefault">e</span><span class="mord mathdefault">c</span><span class="mord mathdefault">o</span><span class="mord mathdefault">n</span><span class="mord mathdefault">d</span></span></span></span></span></p><ul><li><strong>计算强度上限 ImaxI_{max}</strong> ：两个指标相除即可得到计算平台的<strong>计算强度上限</strong>。它描述的是在这个计算平台上，单位内存交换最多用来进行多少次计算。单位是<code>FLOPs/Byte</code>。</li></ul><p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>I</mi><mrow><mi>m</mi><mi>a</mi><mi>x</mi></mrow></msub><mo>=</mo><mfrac><mi>π</mi><mi>β</mi></mfrac></mrow><annotation encoding="application/x-tex">I_{max} =\frac{π}{β} </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.07847em;">I</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:-0.07847em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">m</span><span class="mord mathdefault mtight">a</span><span class="mord mathdefault mtight">x</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1.9880000000000002em;vertical-align:-0.8804400000000001em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.10756em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.05278em;">β</span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.03588em;">π</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.8804400000000001em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span></p><blockquote><p><strong>注</strong>：这里所说的“内存”是广义上的内存。对于CPU计算平台而言指的就是真正的内存；而对于GPU计算平台指的则是显存。</p></blockquote><hr /><h2 id="2-模型的两个指标计算量-与-访存量"><a class="markdownIt-Anchor" href="#2-模型的两个指标计算量-与-访存量"></a> <strong>2. 模型的两个指标：计算量 与 访存量</strong></h2><ul><li><strong>计算量：</strong> 指的是输入单个样本（对于CNN而言就是一张图像），模型进行一次完整的前向传播所发生的浮点运算个数，也即模型的<strong>时间复杂度</strong>。单位是 <code>#FLOP</code> or <code>FLOPs</code>。其中卷积层的计算量公式如下（<a href="https://zhuanlan.zhihu.com/p/31575074">具体分析详见我的另一篇文章</a>）</li></ul><p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mtext>Conv Layer Time Complexity</mtext><mo>:</mo><msup><mi>M</mi><mn>2</mn></msup><mo>⋅</mo><msup><mi>K</mi><mn>2</mn></msup><mo>⋅</mo><msub><mi>C</mi><mrow><mi>i</mi><mi>n</mi></mrow></msub><mo>⋅</mo><msub><mi>C</mi><mrow><mi>o</mi><mi>u</mi><mi>t</mi></mrow></msub><mspace width="1em"/><mo stretchy="false">(</mo><mrow><mi mathvariant="bold">F</mi><mi mathvariant="bold">L</mi><mi mathvariant="bold">O</mi><mi mathvariant="bold">P</mi><mi mathvariant="bold">S</mi></mrow><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\text{Conv Layer Time Complexity}:M^2\cdot K^2\cdot C_{in}\cdot C_{out}\quad(\mathbf{FLOPS})</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="mord text"><span class="mord">Conv Layer Time Complexity</span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">:</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.8641079999999999em;vertical-align:0em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.10903em;">M</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8641079999999999em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.8641079999999999em;vertical-align:0em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.07153em;">K</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8641079999999999em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.07153em;">C</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:-0.07153em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">i</span><span class="mord mathdefault mtight">n</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.07153em;">C</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2805559999999999em;"><span style="top:-2.5500000000000003em;margin-left:-0.07153em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">o</span><span class="mord mathdefault mtight">u</span><span class="mord mathdefault mtight">t</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:1em;"></span><span class="mopen">(</span><span class="mord"><span class="mord mathbf">F</span><span class="mord mathbf">L</span><span class="mord mathbf">O</span><span class="mord mathbf">P</span><span class="mord mathbf">S</span></span><span class="mclose">)</span></span></span></span></span></p><ul><li><strong>访存量：</strong> 指的是输入单个样本，模型完成一次前向传播过程中所发生的内存交换总量，也即模型的<strong>空间复杂度</strong>。在理想情况下（即不考虑片上缓存），模型的访存量就是模型各层权重参数的内存占用（Kernel Mem）与每层所输出的特征图的内存占用（Output Mem）之和。单位是<code>Byte</code>。由于数据类型通常为<code>float32</code> ，因此需要乘以四。</li></ul><p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mtable rowspacing="0.24999999999999992em" columnalign="right left" columnspacing="0em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mi mathvariant="normal">C</mi><mi mathvariant="normal">o</mi><mi mathvariant="normal">n</mi><mi mathvariant="normal">v</mi><mtext> </mtext><mi mathvariant="normal">L</mi><mi mathvariant="normal">a</mi><mi mathvariant="normal">y</mi><mi mathvariant="normal">e</mi><mi mathvariant="normal">r</mi><mtext> </mtext><mi mathvariant="normal">S</mi><mi mathvariant="normal">p</mi><mi mathvariant="normal">a</mi><mi mathvariant="normal">c</mi><mi mathvariant="normal">e</mi><mtext> </mtext><mi mathvariant="normal">C</mi><mi mathvariant="normal">o</mi><mi mathvariant="normal">m</mi><mi mathvariant="normal">p</mi><mi mathvariant="normal">l</mi><mi mathvariant="normal">e</mi><mi mathvariant="normal">x</mi><mi mathvariant="normal">i</mi><mi mathvariant="normal">t</mi><mi mathvariant="normal">y</mi></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mrow></mrow><mo>:</mo><mo stretchy="false">(</mo><msup><mi>K</mi><mn>2</mn></msup><mo>⋅</mo><msub><mi>C</mi><mrow><mi>i</mi><mi>n</mi></mrow></msub><mo>⋅</mo><msub><mi>C</mi><mrow><mi>o</mi><mi>u</mi><mi>t</mi></mrow></msub><mo>+</mo><msup><mi>M</mi><mn>2</mn></msup><mo>⋅</mo><msub><mi>C</mi><mrow><mi>o</mi><mi>u</mi><mi>t</mi></mrow></msub><mo stretchy="false">)</mo><mo>⋅</mo><mn>4</mn></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mo stretchy="false">(</mo><mrow><mi mathvariant="bold">B</mi><mi mathvariant="bold">y</mi><mi mathvariant="bold">t</mi><mi mathvariant="bold">e</mi><mi mathvariant="bold">s</mi></mrow><mo stretchy="false">)</mo></mrow></mstyle></mtd></mtr></mtable><annotation encoding="application/x-tex">\begin{aligned}\mathrm{Conv~Layer~Space~Complexity}&amp;:(K^2\cdot C_{in}\cdot C_{out}+M^2\cdot C_{out})\cdot4\\(\mathbf{Bytes})\end{aligned}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:3.024108em;vertical-align:-1.262054em;"></span><span class="mord"><span class="mtable"><span class="col-align-r"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.762054em;"><span style="top:-3.897946em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"><span class="mord mathrm">C</span><span class="mord mathrm">o</span><span class="mord mathrm">n</span><span class="mord mathrm" style="margin-right:0.01389em;">v</span><span class="mspace nobreak"> </span><span class="mord mathrm">L</span><span class="mord mathrm">a</span><span class="mord mathrm" style="margin-right:0.01389em;">y</span><span class="mord mathrm">e</span><span class="mord mathrm">r</span><span class="mspace nobreak"> </span><span class="mord mathrm">S</span><span class="mord mathrm">p</span><span class="mord mathrm">a</span><span class="mord mathrm">c</span><span class="mord mathrm">e</span><span class="mspace nobreak"> </span><span class="mord mathrm">C</span><span class="mord mathrm">o</span><span class="mord mathrm">m</span><span class="mord mathrm">p</span><span class="mord mathrm">l</span><span class="mord mathrm">e</span><span class="mord mathrm">x</span><span class="mord mathrm">i</span><span class="mord mathrm">t</span><span class="mord mathrm" style="margin-right:0.01389em;">y</span></span></span></span><span style="top:-2.397946em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mopen">(</span><span class="mord"><span class="mord mathbf">B</span><span class="mord mathbf" style="margin-right:0.01597em;">y</span><span class="mord mathbf">t</span><span class="mord mathbf">e</span><span class="mord mathbf">s</span></span><span class="mclose">)</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.262054em;"><span></span></span></span></span></span><span class="col-align-l"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:1.762054em;"><span style="top:-3.897946em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">:</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.07153em;">K</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8641079999999999em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.07153em;">C</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:-0.07153em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">i</span><span class="mord mathdefault mtight">n</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.07153em;">C</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2805559999999999em;"><span style="top:-2.5500000000000003em;margin-left:-0.07153em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">o</span><span class="mord mathdefault mtight">u</span><span class="mord mathdefault mtight">t</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.10903em;">M</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8641079999999999em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.07153em;">C</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2805559999999999em;"><span style="top:-2.5500000000000003em;margin-left:-0.07153em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">o</span><span class="mord mathdefault mtight">u</span><span class="mord mathdefault mtight">t</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mord">4</span></span></span></span></span></span></span></span></span></span></span></span></span></p><ul><li><p><strong>模型的计算强度 II ：</strong> 由计算量除以访存量就可以得到模型的计算强度，它表示此模型在计算过程中，每<code>Byte</code>内存交换到底用于进行多少次浮点运算。单位是<code>FLOPs/Byte</code>。可以看到，模计算强度越大，其内存使用效率越高。</p></li><li><p><strong>模型的理论性能 PP ：</strong> 我们最关心的指标，即模型**_在计算平台上_所能达到的每秒浮点运算次数（理论值）**。单位是 <code>FLOPS</code> or <code>FLOP/s</code>。下面我们即将介绍的 Roof-line Model 给出的就是计算这个指标的方法。终于可以进入正题了。</p></li></ul><hr /><h2 id="3-roof-line-model"><a class="markdownIt-Anchor" href="#3-roof-line-model"></a> <strong>3. Roof-line Model</strong></h2><p>其实 Roof-line Model 说的是很简单的一件事：<strong>模型在一个计算平台的限制下，到底能达到多快的浮点计算速度</strong>。更具体的来说，Roof-line Model 解决的，是“<strong>计算量为A且访存量为B的模型在算力为C且带宽为D的计算平台所能达到的理论性能上限E是多少</strong>”这个问题。</p><h2 id="31-roof-line-的形态"><a class="markdownIt-Anchor" href="#31-roof-line-的形态"></a> <strong>3.1 Roof-line 的形态</strong></h2><p>所谓“Roof-line”，指的就是由计算平台的算力和带宽上限这两个参数所决定的“屋顶”形态，如下图所示。</p><ul><li><strong>算力</strong>决定“屋顶”的高度（绿色线段）</li><li><strong>带宽</strong>决定“房檐”的斜率（红色线段）</li></ul><p><img src="https://pic2.zhimg.com/v2-cafb93b9a31fca2d7c84951555762e59_b.jpg" alt="" /></p><h2 id="32-roof-line-划分出的两个瓶颈区域"><a class="markdownIt-Anchor" href="#32-roof-line-划分出的两个瓶颈区域"></a> <strong>3.2 Roof-line 划分出的两个瓶颈区域</strong></h2><p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>P</mi><mo>=</mo><mrow><mo fence="true">{</mo><mtable rowspacing="0.15999999999999992em" columnalign="left left" columnspacing="1em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mi>β</mi><mo>⋅</mo><mi>I</mi><mo separator="true">,</mo></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mi>w</mi><mi>h</mi><mi>e</mi><mi>n</mi></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mi>I</mi><mo>&lt;</mo><msub><mi>I</mi><mrow><mi>m</mi><mi>a</mi><mi>x</mi></mrow></msub></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mtext mathvariant="bold">Memory</mtext><mtext> </mtext><mtext mathvariant="bold">Bound</mtext></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mi>π</mi><mo separator="true">,</mo></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mi>w</mi><mi>h</mi><mi>e</mi><mi>n</mi></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mi>I</mi><mo>⩾</mo><msub><mi>I</mi><mrow><mi>m</mi><mi>a</mi><mi>x</mi></mrow></msub></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mtext mathvariant="bold">Compute</mtext><mtext> </mtext><mtext mathvariant="bold">Bound</mtext></mrow></mstyle></mtd></mtr></mtable></mrow></mrow><annotation encoding="application/x-tex">P=\left\{\begin{array}{ll}\beta\cdot I,&amp;when&amp;I&lt;I_{max}&amp;\textbf{Memory Bound}\\\\\pi,&amp;when&amp;I\geqslant I_{max}&amp;\textbf{Compute Bound}\end{array}\right.</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.13889em;">P</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:3.60004em;vertical-align:-1.55002em;"></span><span class="minner"><span class="mopen"><span class="delimsizing mult"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:2.05002em;"><span style="top:-2.49999em;"><span class="pstrut" style="height:3.15em;"></span><span class="delimsizinginner delim-size4"><span>⎩</span></span></span><span style="top:-3.15001em;"><span class="pstrut" style="height:3.15em;"></span><span class="delimsizinginner delim-size4"><span>⎨</span></span></span><span style="top:-4.30002em;"><span class="pstrut" style="height:3.15em;"></span><span class="delimsizinginner delim-size4"><span>⎧</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.55002em;"><span></span></span></span></span></span></span><span class="mord"><span class="mtable"><span class="arraycolsep" style="width:0.5em;"></span><span class="col-align-l"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:2.05em;"><span style="top:-4.21em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.05278em;">β</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mord mathdefault" style="margin-right:0.07847em;">I</span><span class="mpunct">,</span></span></span><span style="top:-3.0099999999999993em;"><span class="pstrut" style="height:3em;"></span><span class="mord"></span></span><span style="top:-1.8099999999999994em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.03588em;">π</span><span class="mpunct">,</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.5500000000000007em;"><span></span></span></span></span></span><span class="arraycolsep" style="width:0.5em;"></span><span class="arraycolsep" style="width:0.5em;"></span><span class="col-align-l"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:2.05em;"><span style="top:-4.21em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="mord mathdefault">h</span><span class="mord mathdefault">e</span><span class="mord mathdefault">n</span></span></span><span style="top:-1.8099999999999998em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="mord mathdefault">h</span><span class="mord mathdefault">e</span><span class="mord mathdefault">n</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.5500000000000003em;"><span></span></span></span></span></span><span class="arraycolsep" style="width:0.5em;"></span><span class="arraycolsep" style="width:0.5em;"></span><span class="col-align-c"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:2.05em;"><span style="top:-4.21em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.07847em;">I</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">&lt;</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.07847em;">I</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:-0.07847em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">m</span><span class="mord mathdefault mtight">a</span><span class="mord mathdefault mtight">x</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span><span style="top:-1.8099999999999998em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.07847em;">I</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel amsrm">⩾</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.07847em;">I</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:-0.07847em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">m</span><span class="mord mathdefault mtight">a</span><span class="mord mathdefault mtight">x</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.5500000000000003em;"><span></span></span></span></span></span><span class="arraycolsep" style="width:0.5em;"></span><span class="arraycolsep" style="width:0.5em;"></span><span class="col-align-c"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:2.05em;"><span style="top:-4.21em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord text"><span class="mord textbf">Memory Bound</span></span></span></span><span style="top:-1.8099999999999998em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord text"><span class="mord textbf">Compute Bound</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.5500000000000003em;"><span></span></span></span></span></span><span class="arraycolsep" style="width:0.5em;"></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span></p><h2 id="计算瓶颈区域-compute-bound"><a class="markdownIt-Anchor" href="#计算瓶颈区域-compute-bound"></a> <strong>计算瓶颈区域 <code>Compute-Bound</code></strong></h2><p>不管模型的计算强度 II 有多大，它的理论性能 PP 最大只能等于计算平台的算力 π\pi 。当模型的计算强度 II 大于计算平台的计算强度上限 ImaxI_{max} 时，模型在当前计算平台处于 <code>Compute-Bound</code>状态，即模型的理论性能 PP 受到计算平台算力 π\pi 的限制，无法与计算强度 II 成正比。但这其实并不是一件坏事，因为从充分利用计算平台算力的角度上看，此时模型已经 100%100\% 的利用了计算平台的全部算力。可见，计算平台的算力 π\pi 越高，模型进入计算瓶颈区域后的理论性能 PP 也就越大。</p><h2 id="带宽瓶颈区域-memory-bound"><a class="markdownIt-Anchor" href="#带宽瓶颈区域-memory-bound"></a> <strong>带宽瓶颈区域 <code>Memory-Bound</code></strong></h2><p>当模型的计算强度 II 小于计算平台的计算强度上限 ImaxI_{max} 时，由于此时模型位于“房檐”区间，因此模型理论性能 PP 的大小完全由计算平台的带宽上限 β\beta （房檐的斜率）以及模型自身的计算强度 II 所决定，因此这时候就称模型处于 <code>Memory-Bound</code> 状态。可见，在模型处于带宽瓶颈区间的前提下，计算平台的带宽 β\beta 越大（房檐越陡），或者模型的计算强度 II 越大，模型的理论性能 PP 可呈线性增长。</p><hr /><h2 id="4-模型实例分析"><a class="markdownIt-Anchor" href="#4-模型实例分析"></a> <strong>4. 模型实例分析</strong></h2><p>下面让我们先分别给出 VGG16 和 MobileNet 的计算量和访存量统计表格，然后再用 Roof-line Model 理论来对比分析一下下。很有意思哦！</p><h2 id="41-vgg16"><a class="markdownIt-Anchor" href="#41-vgg16"></a> <strong>4.1 VGG16</strong></h2><p><img src="https://pic3.zhimg.com/v2-d5753b4a2a91b3790165ff967cbc7fc2_b.jpg" alt="" /></p><p>VGG-16</p><p><strong>VGG 可以说是在计算强度上登峰造极的一个模型系列，简约不简单</strong>。以 VGG16 为例，从上表可以看到，仅包含一次前向传播的计算量就达到了 15 GFLOPs，如果包含反向传播，则需要再乘二。访存量则是 Kernel Mem 和 Output Mem 之和再乘以四，大约是 600MB。<strong>因此 VGG16 的计算强度就是 25 FLOPs/Byte。</strong></p><p>另外如果把模型顶端那两个硕大无比的全链接层（其参数量占整个模型的80%以上）替换为GAP以降低访存量（事实证明这样修改并不会影响准确率），那么它的实际计算强度可以再提升四倍以上，简直突破天际。</p><blockquote><p><strong>注：</strong> 以上分析仅限于前向传播计算过程（即模型预测）。如果涵盖反向传播（即模型训练），则计算量和访存量都要考虑梯度更新的具体方式，例如计算 Momentum 几个变量时引入的时间和空间复杂度。</p></blockquote><h2 id="42-mobilenet"><a class="markdownIt-Anchor" href="#42-mobilenet"></a> <strong>4.2 MobileNet</strong></h2><p><img src="https://pic1.zhimg.com/v2-c0e8b7651af382009fadcd9a4a9a328c_b.jpg" alt="" /></p><p>MobileNet 是以轻量著称的小网络代表（最近新出了V2版本，<a href="https://zhuanlan.zhihu.com/p/33075914">分析见这里</a>）。相比简单而庞大的 VGG16 结构，MobileNet 的网络更为细长，加入了大量的BN，每一层都通过 DW + PW 的方式降低了计算量，同时也付出了计算效率低的代价。从上面超级长的表格就能有一个感性的的认识。</p><p>MobileNet 的计算量只有大约 0.5 GFLOPs（VGG16 则是 15 GFLOPs），其访存量也只有 74 MB（VGG16 则是约 600 MB）。这样看上去确实轻量了很多，但是由于计算量和访存量都下降了，而且相比之下计算量下降的更厉害，<strong>因此 MobileNet 的计算强度只有 7 FLOPs/Byte。</strong></p><h2 id="43-两个模型在-1080ti-上的对比"><a class="markdownIt-Anchor" href="#43-两个模型在-1080ti-上的对比"></a> <strong>4.3 两个模型在 1080Ti 上的对比</strong></h2><p>作为性价比之王的 1080Ti，我们的两个模型 VGG16 和 MobileNet 的性能将分别位于这个计算平台 Roof-line Model 的什么位置呢？</p><ul><li>1080Ti 的算力 π=11.3TFLOP/s= 11.3 ~\textrm{TFLOP/s}</li><li>1080Ti 的带宽 β=484GB/s = 484 GB/s</li><li>因此 1080Ti 计算平台的最大计算强度 I_max≈24</li><li>VGG16 的计算强度 I_v≈25</li><li>MobileNet 的计算强度 I_m≈7</li></ul><p><img src="https://pic3.zhimg.com/v2-55052a705e6225321fb562cd7d04283a_b.jpg" alt="" /></p><p>Roof-line Model : VGG16 vs MobileNet</p><p>由上图可以非常清晰的看到，</p><ul><li>MobileNet 处于 <strong>Memory-Bound</strong> 区域。在 1080Ti 上的理论性能只有 3.3 TFLOP/s。</li><li>VGG16 刚好迈入 <strong>Compute-Bound</strong> 区域。<strong>完全利用 1080Ti 的全部算力</strong>。</li></ul><p>虽然 MobileNet 进行前向传播的计算量只有 VGG 的三十分之一，但是由于计算平台的带宽限制，它不能像 VGG 那样完全利用 1080Ti 这个计算平台的全部算力，因此它在 1080Ti 上每秒钟可以进行的浮点运算数只能达到 VGG 的 30%，因此理论上的运行速度大约是 VGG 的十倍（实际上会因为各方面其他因素的限制而使得差别更小）。</p><p>MobileNet 这类小型网络更适合运行在嵌入式平台之上。首先这类轻量级的计算平台根本就放不下也运行不起来 VGG 这种大模型。更重要的是，由于这类计算平台本身的计算强度上限就很低，可能比 MobileNet 的计算强度还要小，因此 MobileNet 运行在这类计算平台上的时候，它就不再位于 Memory-Bound 区域，而是农奴翻身把歌唱的进入了 Compute-Bound 区域，此时 MobileNet 和 VGG16 一样可以充分利用计算平台的算力，而且内存消耗和计算量都小了一两个数量级，同时分类准确率只下降了1%，所以大家才愿意用它。</p><p>所以说，屠龙时用屠龙刀，日常吃鸡用小刀就可以了，否则只会弄巧成拙。</p><p>感谢 所指出的：Roofline 模型讲的是程序<strong>在计算平台的算力和带宽这两个指标限制下，所能达到的理论性能上界</strong>，而不是实际达到的性能，因为实际计算过程中<strong>还有除算力和带宽之外的其他重要因素</strong>，它们也会影响模型的实际性能，这是 Roofline Model 未考虑到的。例如矩阵乘法，会因为 cache 大小的限制、GEMM 实现的优劣等其他限制，导致你几乎无法达到 Roofline 模型所定义的边界（屋顶）。</p><hr /><h2 id="5-结语"><a class="markdownIt-Anchor" href="#5-结语"></a> <strong>5. 结语</strong></h2><p>本文系统的介绍了：</p><ul><li>计算平台的两个指标：算力和带宽。</li><li>模型的两个指标：计算量和访存量。</li><li>使用 Roof-line Model 分析模型在计算平台上所能达到的理论计算性能，并分析模型在计算平台上的两种互斥状态：计算受限状态和带宽受限状态。</li><li>以 VGG16 和 MobileNet 为例，在 1080Ti 计算平台上分析对比它们的计算性能。</li></ul>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>frp 内网穿透配置</title>
      <link href="/2024/07/%E6%9D%82%E9%A1%B9/frp%E4%BB%A3%E7%90%86/"/>
      <url>/2024/07/%E6%9D%82%E9%A1%B9/frp%E4%BB%A3%E7%90%86/</url>
      
        <content type="html"><![CDATA[<h1 id="frp-内网穿透配置"><a class="markdownIt-Anchor" href="#frp-内网穿透配置"></a> frp 内网穿透配置</h1><blockquote><p>要放暑假了，在家需要连接实验室服务器，之前通过云主机搭建frp中转，但是云主机比较贵，所以来白嫖Sakura Frp</p></blockquote><h2 id="1-注册账户"><a class="markdownIt-Anchor" href="#1-注册账户"></a> 1. 注册账户</h2><p>普通用户有两个隧道，限速10M,一个月有10G流量，可以每日签到获取额外的流量，不过对于ssh来讲，基本上是够用的，代码文本也消耗不了多少流量。</p><h2 id="2-建立隧道"><a class="markdownIt-Anchor" href="#2-建立隧道"></a> 2. 建立隧道</h2><p>实名制后，可以点击服务来创建一个隧道，来中转服务。</p><p><img src="https://s2.loli.net/2024/07/30/uOVJQl7mUSLnt2C.png" alt="QQ_1722343688110" /></p><p>这样就创建了一个隧道</p><p>frp本质上做的事情就是，将两端的流量进行转发，因为两端没有公网IP，也可以使用<code>zerotier</code></p><p>来打洞，但是如果NAT层比较多，延时还是比较大，在学校里面使用延时比较低，如果在家延时比较高，体验不好。</p><h2 id="3-服务端"><a class="markdownIt-Anchor" href="#3-服务端"></a> 3. 服务端</h2><p>我把你要访问的电脑称为服务端，也就是实验室的服务器</p><p>服务端需要下载frp</p><p><img src="https://s2.loli.net/2024/07/30/LdeWwjQl34Tn9r7.png" alt="QQ_1722342725796" /></p><ul><li>点击复制链接</li></ul><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash"><span class="token function">wget</span> 链接<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><ul><li><p>将frp下载下来，使用<code>tar -xvf natfrp-service_linux_amd64.tar.zst</code>解压缩</p></li><li><p><code>chmod +x ./frpc</code>来赋予可执行权限</p></li><li><p>获取加密参数</p><p><img src="https://s2.loli.net/2024/07/30/7W1t2ougqnmKOTQ.png" alt="QQ_1722342962407" /></p></li><li><p>启动</p><p><code>./frpc -f 参数</code></p><ul><li>可以使用<code>systemctl</code>来管理frp的服务，包括开机自动启动，开始，暂停等</li><li>nohup开控制frpc后端执行 <code>nohup ./frpc -f 参数 &gt; runoob.log 2&gt;&amp;1 &amp;</code></li></ul><p><img src="https://s2.loli.net/2024/07/30/IgwdNfDqC9jY7sR.png" alt="QQ_1722343190952" /></p></li></ul><p>可以获得访问服务器的<code>ip:port</code></p><h2 id="4-客户端"><a class="markdownIt-Anchor" href="#4-客户端"></a> 4. 客户端</h2><ul><li><p>下载认证</p><p><img src="https://s2.loli.net/2024/07/30/EhS36gJaFVGfLle.png" alt="QQ_1722343257332" /></p></li></ul><p><img src="https://s2.loli.net/2024/07/30/LRFC9bnuv1jlDZc.png" alt="QQ_1722343281279" /></p><p>会生成一个exe，点击运行即可</p><ul><li><p>ssh 连接</p><p>注意这里ssh端口不是22</p><p>而是上面获得的那个IP和端口</p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash"><span class="token function">ssh</span> <span class="token parameter variable">-p</span> port yourname@ip<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>这样就连接到服务器了。</p></li></ul>]]></content>
      
      
      <categories>
          
          <category> 工具配置 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> frp </tag>
            
            <tag> 内网穿透 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>vscode 调试coredump 文件</title>
      <link href="/2024/07/%E8%B0%83%E8%AF%95/vscode%20%E8%B0%83%E8%AF%95coredump%E6%96%87%E4%BB%B6/"/>
      <url>/2024/07/%E8%B0%83%E8%AF%95/vscode%20%E8%B0%83%E8%AF%95coredump%E6%96%87%E4%BB%B6/</url>
      
        <content type="html"><![CDATA[<ol><li><p>设置<code>ulimit -c unlimited</code>，如果<code>ulimit -c</code>结果是0的话产生不了coredump文件</p> <pre class="line-numbers language-bash" data-language="bash"><code class="language-bash"><span class="token builtin class-name">ulimit</span> <span class="token parameter variable">-c</span> <span class="token punctuation">[</span>size<span class="token punctuation">]</span>  //这里size一般修改为unlimited,或者是其他数字：2048<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>​上修改只对当前的shell有效，一旦关闭，则恢复原来的值</p></li><li><p><code>cat /var/log/apport.log</code> 可以看到生成的日志信息</p></li><li><p>core文件路径</p><ul><li><p>ubuntu20的生成coredump路径不在可执行路径下，而是在<code>/var/lib/apport/coredump</code>，因为没有写入权限，所以产生不了coredump文件，需要sudo，或者修改产生路径。</p></li><li><p>修改core文件产生位置在可执行文件目录下</p><p><code>sudo bash -c 'echo core.%e.%p &gt; /proc/sys/kernel/core_pattern'</code></p></li></ul></li><li><p>vscode 配置</p><ul><li><p>gdb</p><pre class="line-numbers language-json" data-language="json"><code class="language-json"><span class="token punctuation">&#123;</span>    <span class="token property">"name"</span><span class="token operator">:</span> <span class="token string">"(gdb) Launch"</span><span class="token punctuation">,</span>    <span class="token property">"type"</span><span class="token operator">:</span> <span class="token string">"cppdbg"</span><span class="token punctuation">,</span>    <span class="token property">"request"</span><span class="token operator">:</span> <span class="token string">"launch"</span><span class="token punctuation">,</span>    <span class="token property">"program"</span><span class="token operator">:</span> <span class="token string">"$&#123;workspaceFolder&#125;/a.out"</span><span class="token punctuation">,</span>    <span class="token property">"args"</span><span class="token operator">:</span> <span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">,</span>    <span class="token property">"stopAtEntry"</span><span class="token operator">:</span> <span class="token boolean">false</span><span class="token punctuation">,</span>    <span class="token property">"cwd"</span><span class="token operator">:</span> <span class="token string">"$&#123;workspaceFolder&#125;"</span><span class="token punctuation">,</span>    <span class="token property">"environment"</span><span class="token operator">:</span> <span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">,</span>    <span class="token property">"externalConsole"</span><span class="token operator">:</span> <span class="token boolean">false</span><span class="token punctuation">,</span>    <span class="token property">"MIMode"</span><span class="token operator">:</span> <span class="token string">"gdb"</span><span class="token punctuation">,</span>    <span class="token property">"setupCommands"</span><span class="token operator">:</span> <span class="token punctuation">[</span>        <span class="token punctuation">&#123;</span>            <span class="token property">"description"</span><span class="token operator">:</span> <span class="token string">"Enable pretty-printing for gdb"</span><span class="token punctuation">,</span>            <span class="token property">"text"</span><span class="token operator">:</span> <span class="token string">"-enable-pretty-printing"</span><span class="token punctuation">,</span>            <span class="token property">"ignoreFailures"</span><span class="token operator">:</span> <span class="token boolean">true</span>        <span class="token punctuation">&#125;</span>    <span class="token punctuation">]</span><span class="token punctuation">,</span>    <span class="token property">"coreDumpPath"</span><span class="token operator">:</span> <span class="token string">"$&#123;workspaceFolder&#125;/core.a.out.372125"</span><span class="token punctuation">&#125;</span><span class="token punctuation">,</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li></ul><p>虽然填了program，但是实际上是从coredump启动的</p><ul><li><p>codelldb</p><pre class="line-numbers language-json" data-language="json"><code class="language-json"><span class="token punctuation">&#123;</span>           <span class="token property">"type"</span><span class="token operator">:</span> <span class="token string">"lldb"</span><span class="token punctuation">,</span>           <span class="token property">"request"</span><span class="token operator">:</span> <span class="token string">"custom"</span><span class="token punctuation">,</span>           <span class="token property">"name"</span><span class="token operator">:</span> <span class="token string">"Open a core dump"</span><span class="token punctuation">,</span>           <span class="token property">"initCommands"</span><span class="token operator">:</span> <span class="token punctuation">[</span>               <span class="token string">"target create -c $&#123;workspaceFolder&#125;/core.a.out.372125"</span>           <span class="token punctuation">]</span><span class="token punctuation">&#125;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li></ul></li></ol><p>​</p>]]></content>
      
      
      <categories>
          
          <category> 工具配置 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> frp </tag>
            
            <tag> 内网穿透 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Linux 中使用adb 连接手机</title>
      <link href="/2024/03/%E8%B0%83%E8%AF%95/android%20gdb%20%E8%B0%83%E8%AF%95/"/>
      <url>/2024/03/%E8%B0%83%E8%AF%95/android%20gdb%20%E8%B0%83%E8%AF%95/</url>
      
        <content type="html"><![CDATA[<h1 id="linux-中使用adb-连接手机"><a class="markdownIt-Anchor" href="#linux-中使用adb-连接手机"></a> Linux 中使用adb 连接手机</h1><p>在做实验时，使用linux服务器连接手机，一直出现 connect 后，手机直接 offline 的情况。<br />解决： 不能使用<code>sudo apt install adb</code> 安装adb，来连接手机。<br />解决方法：</p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash"><span class="token function">mkdir</span> cli-tools<span class="token function">wget</span> <span class="token parameter variable">-c</span> https://dl.google.com/android/repository/platform-tools-latest-linux.zip<span class="token function">unzip</span> platform-tools-latest-linux.zip <span class="token builtin class-name">cd</span> platform-tools/./adb connect yourIP:Port<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre>]]></content>
      
      
      
        <tags>
            
            <tag> linux </tag>
            
            <tag> adb </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>使用LLDB 远程调试 安卓native C++ 程序的vscode 配置</title>
      <link href="/2024/03/%E8%B0%83%E8%AF%95/%E4%BD%BF%E7%94%A8LLDB%20%E8%BF%9C%E7%A8%8B%E8%B0%83%E8%AF%95android/"/>
      <url>/2024/03/%E8%B0%83%E8%AF%95/%E4%BD%BF%E7%94%A8LLDB%20%E8%BF%9C%E7%A8%8B%E8%B0%83%E8%AF%95android/</url>
      
        <content type="html"><![CDATA[<h1 id="使用lldb-远程调试-安卓native-c-程序的vscode-配置"><a class="markdownIt-Anchor" href="#使用lldb-远程调试-安卓native-c-程序的vscode-配置"></a> 使用LLDB 远程调试 安卓native C++ 程序的vscode 配置</h1><h2 id="背景"><a class="markdownIt-Anchor" href="#背景"></a> 背景</h2><p>我目前使用windows ssh连接到linux 服务器上,对android native C++代码进行编写,编译(因为服务器核心数量多,编译速度快). 目前高版本的NDK如r26等,已经不再对gdbserver 提供支撑, 所以迁移到LLDB调试,也是主流的技术方向.</p><h2 id="相关配置"><a class="markdownIt-Anchor" href="#相关配置"></a> 相关配置</h2><ol><li>安装vscode 插件<br />CodeLLDB<br /><img src="https://s2.loli.net/2024/03/11/pFxMIkRtHvYUyBL.png" alt="20240311140633" /></li><li>下载NDK<br /><a href="https://developer.android.com/ndk/downloads?hl=zh-cn">NDK下载网站链接</a><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash"><span class="token function">wget</span> https://dl.google.com/android/repository/android-ndk-r26c-linux.zip?hl<span class="token operator">=</span>zh-cn<span class="token function">unzip</span> android-ndk-r26c-linux.zip<span class="token punctuation">\</span>?hl<span class="token punctuation">\</span><span class="token operator">=</span>zh-cn<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre></li><li>adb 上传lldb-server <pre class="line-numbers language-bash" data-language="bash"><code class="language-bash"><span class="token builtin class-name">cd</span>  android-ndk-r26c<span class="token function">find</span> ./ <span class="token parameter variable">-name</span> <span class="token string">"lldb-server"</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><img src="https://s2.loli.net/2024/03/11/1X3Hesi5EPVQ9oy.png" alt="20240311141315" /><br />选取aarch64 版本的lldb-server <pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">adb push ./toolchains/llvm/prebuilt/linux-x86_64/lib/clang/17/lib/linux/aarch64/lldb-server /data/local/tmpadb shell <span class="token string">"chmod +xrw /data/local/tmp/lldb-server"</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre></li><li>启动lldb-server<br /><code>lldb-server platform --server --listen *:9999</code> 端口可以自己选,不冲突就行</li><li>配置vscode 的launch.json 文件<pre class="line-numbers language-json" data-language="json"><code class="language-json"><span class="token punctuation">&#123;</span>    <span class="token property">"name"</span><span class="token operator">:</span> <span class="token string">"Remote launch"</span><span class="token punctuation">,</span>    <span class="token property">"type"</span><span class="token operator">:</span> <span class="token string">"lldb"</span><span class="token punctuation">,</span>    <span class="token property">"request"</span><span class="token operator">:</span> <span class="token string">"launch"</span><span class="token punctuation">,</span>    <span class="token property">"program"</span><span class="token operator">:</span> <span class="token string">"$&#123;workspaceFolder&#125;/build/debuggee"</span><span class="token punctuation">,</span> <span class="token comment">// Local path. </span>    <span class="token property">"initCommands"</span><span class="token operator">:</span> <span class="token punctuation">[</span>        <span class="token string">"platform select &lt;platform>"</span><span class="token punctuation">,</span> <span class="token comment">// For example: 'remote-linux', 'remote-macosx', 'remote-android', etc.</span>        <span class="token string">"platform connect connect://&lt;remote_host>:&lt;port>"</span><span class="token punctuation">,</span>        <span class="token string">"settings set target.inherit-env false"</span><span class="token punctuation">,</span> <span class="token comment">// See note below.</span>    <span class="token punctuation">]</span><span class="token punctuation">,</span>    <span class="token property">"env"</span><span class="token operator">:</span> <span class="token punctuation">&#123;</span>        <span class="token property">"PATH"</span><span class="token operator">:</span> <span class="token string">"xxx"</span><span class="token punctuation">,</span> <span class="token comment">// remote 的path</span>        <span class="token property">"LD_LIBRARY_PATH"</span><span class="token operator">:</span> <span class="token string">"/data/local/tmp/code"</span>    <span class="token punctuation">&#125;</span><span class="token punctuation">,</span>    <span class="token property">"args"</span><span class="token operator">:</span><span class="token punctuation">[</span>        <span class="token string">"/data/local/tmp/code/pose.mnn"</span><span class="token punctuation">,</span>        <span class="token string">"/data/local/tmp/code/input.png"</span><span class="token punctuation">,</span>        <span class="token string">"/data/local/tmp/code/out.png"</span>    <span class="token punctuation">]</span><span class="token punctuation">,</span>    <span class="token property">"breakpointMode"</span><span class="token operator">:</span> <span class="token string">"file"</span><span class="token punctuation">&#125;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>有几个点需要注意<ul><li>program 是linux本地的路径,不是在remote手机上的路径</li><li>args 是程序运行时的参数,如果需要路径,是remote上的路径</li><li>env 配置的是remote上运行的env,比如链接库地址,PATH等</li><li>breakpointMode 设置为file就行</li></ul></li></ol><h2 id="建议"><a class="markdownIt-Anchor" href="#建议"></a> 建议</h2><ul><li>如果配置不成功,首先不使用vscode插件,在自己的电脑上命令行能够启动LLDB,LLDB-server和调试native c++程序.</li><li>codeLLDB 使用的是自己的lldb,需要检查该lldb能够正常运行和调试<br /><img src="https://s2.loli.net/2024/03/11/i1LOAze2saZJ6VK.png" alt="20240311142552" /></li></ul>]]></content>
      
      
      
        <tags>
            
            <tag> linux </tag>
            
            <tag> adb </tag>
            
            <tag> LLDB </tag>
            
            <tag> android </tag>
            
            <tag> Debug </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hello World</title>
      <link href="/2013/12/hello-world/"/>
      <url>/2013/12/hello-world/</url>
      
        <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="quick-start"><a class="markdownIt-Anchor" href="#quick-start"></a> Quick Start</h2><h3 id="create-a-new-post"><a class="markdownIt-Anchor" href="#create-a-new-post"></a> Create a new post</h3><pre class="line-numbers language-cpp" data-language="cpp"><code class="language-cpp"><span class="token macro property"><span class="token directive-hash">#</span><span class="token directive keyword">include</span> <span class="token string">&lt;iostream></span></span><span class="token keyword">using</span> <span class="token keyword">namespace</span> std<span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="run-server"><a class="markdownIt-Anchor" href="#run-server"></a> Run server</h3><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">$ hexo server<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="generate-static-files"><a class="markdownIt-Anchor" href="#generate-static-files"></a> Generate static files</h3><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">$ hexo generate<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="deploy-to-remote-sites"><a class="markdownIt-Anchor" href="#deploy-to-remote-sites"></a> Deploy to remote sites</h3><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">$ hexo deploy<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><img src="https://raw.githubusercontent.com/weikangqi/picgo/main/lookahead-decoding.gif" alt="lookahead-decoding" style="zoom:50%;" /><p>![image-20250822002821998](C:</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>大模型困惑度计算</title>
      <link href="/2013/12/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%9F%BA%E7%A1%80/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BB%B7%E6%8C%87%E6%A0%87%E5%9B%B0%E6%83%91%E5%BA%A6/"/>
      <url>/2013/12/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%9F%BA%E7%A1%80/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BB%B7%E6%8C%87%E6%A0%87%E5%9B%B0%E6%83%91%E5%BA%A6/</url>
      
        <content type="html"><![CDATA[<h2 id="困惑度perplexity"><a class="markdownIt-Anchor" href="#困惑度perplexity"></a> 困惑度（Perplexity）</h2><p><strong>困惑度（Perplexity, PPL）</strong> 是衡量语言模型质量的常用指标。它表示模型对下一词的预测不确定性，数值越低，表示模型的预测越准确。困惑度越高，表示模型对语言的理解越“困惑”。</p><h3 id="困惑度的计算公式"><a class="markdownIt-Anchor" href="#困惑度的计算公式"></a> 困惑度的计算公式</h3><p>给定一个词序列 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>w</mi><mn>1</mn></msub><mo separator="true">,</mo><msub><mi>w</mi><mn>2</mn></msub><mo separator="true">,</mo><mo>…</mo><mo separator="true">,</mo><msub><mi>w</mi><mi>N</mi></msub></mrow><annotation encoding="application/x-tex">w_1, w_2, \dots, w_N</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="minner">…</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.32833099999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.10903em;">N</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>，困惑度的计算公式为：</p><p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mtext>Perplexity</mtext><mo>=</mo><mi>exp</mi><mo>⁡</mo><mrow><mo fence="true">(</mo><mo>−</mo><mfrac><mn>1</mn><mi>N</mi></mfrac><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></munderover><mi>log</mi><mo>⁡</mo><mi>P</mi><mo stretchy="false">(</mo><msub><mi>w</mi><mi>i</mi></msub><mo>∣</mo><msub><mi>w</mi><mn>1</mn></msub><mo separator="true">,</mo><mo>…</mo><mo separator="true">,</mo><msub><mi>w</mi><mrow><mi>i</mi><mo>−</mo><mn>1</mn></mrow></msub><mo stretchy="false">)</mo><mo fence="true">)</mo></mrow></mrow><annotation encoding="application/x-tex">\text{Perplexity} = \exp \left( -\frac{1}{N} \sum_{i=1}^{N} \log P(w_i \mid w_1, \dots, w_{i-1}) \right)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="mord text"><span class="mord">Perplexity</span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:3.106005em;vertical-align:-1.277669em;"></span><span class="mop">exp</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="minner"><span class="mopen delimcenter" style="top:0em;"><span class="delimsizing size4">(</span></span><span class="mord">−</span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.32144em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.10903em;">N</span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.686em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.8283360000000002em;"><span style="top:-1.872331em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">i</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span style="top:-3.050005em;"><span class="pstrut" style="height:3.05em;"></span><span><span class="mop op-symbol large-op">∑</span></span></span><span style="top:-4.3000050000000005em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.10903em;">N</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.277669em;"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mop">lo<span style="margin-right:0.01389em;">g</span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">∣</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="minner">…</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.311664em;"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">i</span><span class="mbin mtight">−</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.208331em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mclose delimcenter" style="top:0em;"><span class="delimsizing size4">)</span></span></span></span></span></span></span></p><p>其中：</p><ul><li><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>N</mi></mrow><annotation encoding="application/x-tex">N</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.10903em;">N</span></span></span></span>表示句子中的词数。</li><li><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>P</mi><mo stretchy="false">(</mo><msub><mi>w</mi><mi>i</mi></msub><mo>∣</mo><msub><mi>w</mi><mn>1</mn></msub><mo separator="true">,</mo><mo>…</mo><mo separator="true">,</mo><msub><mi>w</mi><mrow><mi>i</mi><mo>−</mo><mn>1</mn></mrow></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">P(w_i \mid w_1, \dots, w_{i-1})</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">∣</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="minner">…</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.311664em;"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">i</span><span class="mbin mtight">−</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.208331em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span>是模型在给定上下文的条件下预测下一个词的概率。</li></ul><h3 id="例子计算困惑度"><a class="markdownIt-Anchor" href="#例子计算困惑度"></a> 例子：计算困惑度</h3><p>假设我们有一个句子：<br /><strong>&quot;The cat sat on the mat.&quot;</strong></p><p>语言模型对每个词的预测概率如下：</p><p><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>P</mi><mo stretchy="false">(</mo><mtext>&quot;The&quot;</mtext><mo stretchy="false">)</mo><mo>=</mo><mn>0.4</mn></mrow><annotation encoding="application/x-tex">P(\text{&quot;The&quot;}) = 0.4</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord text"><span class="mord">&quot;The&quot;</span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">0</span><span class="mord">.</span><span class="mord">4</span></span></span></span><br /><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>P</mi><mo stretchy="false">(</mo><mtext>&quot;cat&quot;</mtext><mo>∣</mo><mtext>&quot;The&quot;</mtext><mo stretchy="false">)</mo><mo>=</mo><mn>0.3</mn></mrow><annotation encoding="application/x-tex">P(\text{&quot;cat&quot;} \mid \text{&quot;The&quot;}) = 0.3</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord text"><span class="mord">&quot;cat&quot;</span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">∣</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord text"><span class="mord">&quot;The&quot;</span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">0</span><span class="mord">.</span><span class="mord">3</span></span></span></span><br /><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>P</mi><mo stretchy="false">(</mo><mtext>&quot;sat&quot;</mtext><mo>∣</mo><mtext>&quot;The cat&quot;</mtext><mo stretchy="false">)</mo><mo>=</mo><mn>0.2</mn></mrow><annotation encoding="application/x-tex">P(\text{&quot;sat&quot;} \mid \text{&quot;The cat&quot;}) = 0.2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord text"><span class="mord">&quot;sat&quot;</span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">∣</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord text"><span class="mord">&quot;The cat&quot;</span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">0</span><span class="mord">.</span><span class="mord">2</span></span></span></span><br /><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>P</mi><mo stretchy="false">(</mo><mtext>&quot;on&quot;</mtext><mo>∣</mo><mtext>&quot;The cat sat&quot;</mtext><mo stretchy="false">)</mo><mo>=</mo><mn>0.5</mn></mrow><annotation encoding="application/x-tex">P(\text{&quot;on&quot;} \mid \text{&quot;The cat sat&quot;}) = 0.5</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord text"><span class="mord">&quot;on&quot;</span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">∣</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord text"><span class="mord">&quot;The cat sat&quot;</span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">0</span><span class="mord">.</span><span class="mord">5</span></span></span></span><br /><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>P</mi><mo stretchy="false">(</mo><mtext>&quot;the&quot;</mtext><mo>∣</mo><mtext>&quot;The cat sat on&quot;</mtext><mo stretchy="false">)</mo><mo>=</mo><mn>0.7</mn></mrow><annotation encoding="application/x-tex">P(\text{&quot;the&quot;} \mid \text{&quot;The cat sat on&quot;}) = 0.7</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord text"><span class="mord">&quot;the&quot;</span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">∣</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord text"><span class="mord">&quot;The cat sat on&quot;</span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">0</span><span class="mord">.</span><span class="mord">7</span></span></span></span><br /><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>P</mi><mo stretchy="false">(</mo><mtext>&quot;mat&quot;</mtext><mo>∣</mo><mtext>&quot;The cat sat on the&quot;</mtext><mo stretchy="false">)</mo><mo>=</mo><mn>0.6</mn></mrow><annotation encoding="application/x-tex">P(\text{&quot;mat&quot;} \mid \text{&quot;The cat sat on the&quot;}) = 0.6</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord text"><span class="mord">&quot;mat&quot;</span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">∣</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord text"><span class="mord">&quot;The cat sat on the&quot;</span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">0</span><span class="mord">.</span><span class="mord">6</span></span></span></span></p><h4 id="第一步计算对数概率"><a class="markdownIt-Anchor" href="#第一步计算对数概率"></a> 第一步：计算对数概率</h4><p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>log</mi><mo>⁡</mo><mo stretchy="false">(</mo><mn>0.4</mn><mo stretchy="false">)</mo><mo>=</mo><mo>−</mo><mn>0.9163</mn></mrow><annotation encoding="application/x-tex">\log(0.4) = -0.9163</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mop">lo<span style="margin-right:0.01389em;">g</span></span><span class="mopen">(</span><span class="mord">0</span><span class="mord">.</span><span class="mord">4</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.72777em;vertical-align:-0.08333em;"></span><span class="mord">−</span><span class="mord">0</span><span class="mord">.</span><span class="mord">9</span><span class="mord">1</span><span class="mord">6</span><span class="mord">3</span></span></span></span></span></p><p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>log</mi><mo>⁡</mo><mo stretchy="false">(</mo><mn>0.3</mn><mo stretchy="false">)</mo><mo>=</mo><mo>−</mo><mn>1.204</mn></mrow><annotation encoding="application/x-tex">\log(0.3) = -1.204</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mop">lo<span style="margin-right:0.01389em;">g</span></span><span class="mopen">(</span><span class="mord">0</span><span class="mord">.</span><span class="mord">3</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.72777em;vertical-align:-0.08333em;"></span><span class="mord">−</span><span class="mord">1</span><span class="mord">.</span><span class="mord">2</span><span class="mord">0</span><span class="mord">4</span></span></span></span></span></p><p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>log</mi><mo>⁡</mo><mo stretchy="false">(</mo><mn>0.2</mn><mo stretchy="false">)</mo><mo>=</mo><mo>−</mo><mn>1.6094</mn></mrow><annotation encoding="application/x-tex">\log(0.2) = -1.6094</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mop">lo<span style="margin-right:0.01389em;">g</span></span><span class="mopen">(</span><span class="mord">0</span><span class="mord">.</span><span class="mord">2</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.72777em;vertical-align:-0.08333em;"></span><span class="mord">−</span><span class="mord">1</span><span class="mord">.</span><span class="mord">6</span><span class="mord">0</span><span class="mord">9</span><span class="mord">4</span></span></span></span></span></p><p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>log</mi><mo>⁡</mo><mo stretchy="false">(</mo><mn>0.5</mn><mo stretchy="false">)</mo><mo>=</mo><mo>−</mo><mn>0.6931</mn></mrow><annotation encoding="application/x-tex">\log(0.5) = -0.6931</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mop">lo<span style="margin-right:0.01389em;">g</span></span><span class="mopen">(</span><span class="mord">0</span><span class="mord">.</span><span class="mord">5</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.72777em;vertical-align:-0.08333em;"></span><span class="mord">−</span><span class="mord">0</span><span class="mord">.</span><span class="mord">6</span><span class="mord">9</span><span class="mord">3</span><span class="mord">1</span></span></span></span></span></p><p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>log</mi><mo>⁡</mo><mo stretchy="false">(</mo><mn>0.7</mn><mo stretchy="false">)</mo><mo>=</mo><mo>−</mo><mn>0.3567</mn></mrow><annotation encoding="application/x-tex">\log(0.7) = -0.3567</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mop">lo<span style="margin-right:0.01389em;">g</span></span><span class="mopen">(</span><span class="mord">0</span><span class="mord">.</span><span class="mord">7</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.72777em;vertical-align:-0.08333em;"></span><span class="mord">−</span><span class="mord">0</span><span class="mord">.</span><span class="mord">3</span><span class="mord">5</span><span class="mord">6</span><span class="mord">7</span></span></span></span></span></p><p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>log</mi><mo>⁡</mo><mo stretchy="false">(</mo><mn>0.6</mn><mo stretchy="false">)</mo><mo>=</mo><mo>−</mo><mn>0.5108</mn></mrow><annotation encoding="application/x-tex">\log(0.6) = -0.5108</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mop">lo<span style="margin-right:0.01389em;">g</span></span><span class="mopen">(</span><span class="mord">0</span><span class="mord">.</span><span class="mord">6</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.72777em;vertical-align:-0.08333em;"></span><span class="mord">−</span><span class="mord">0</span><span class="mord">.</span><span class="mord">5</span><span class="mord">1</span><span class="mord">0</span><span class="mord">8</span></span></span></span></span></p><h4 id="第二步计算总和并取平均值"><a class="markdownIt-Anchor" href="#第二步计算总和并取平均值"></a> 第二步：计算总和并取平均值</h4><p>将这些对数概率相加：</p><p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mo>−</mo><mn>0.9163</mn><mo>+</mo><mo>−</mo><mn>1.204</mn><mo>+</mo><mo>−</mo><mn>1.6094</mn><mo>+</mo><mo>−</mo><mn>0.6931</mn><mo>+</mo><mo>−</mo><mn>0.3567</mn><mo>+</mo><mo>−</mo><mn>0.5108</mn><mo>=</mo><mo>−</mo><mn>5.2903</mn></mrow><annotation encoding="application/x-tex">-0.9163 + -1.204 + -1.6094 + -0.6931 + -0.3567 + -0.5108 = -5.2903</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.72777em;vertical-align:-0.08333em;"></span><span class="mord">−</span><span class="mord">0</span><span class="mord">.</span><span class="mord">9</span><span class="mord">1</span><span class="mord">6</span><span class="mord">3</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.72777em;vertical-align:-0.08333em;"></span><span class="mord">−</span><span class="mord">1</span><span class="mord">.</span><span class="mord">2</span><span class="mord">0</span><span class="mord">4</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.72777em;vertical-align:-0.08333em;"></span><span class="mord">−</span><span class="mord">1</span><span class="mord">.</span><span class="mord">6</span><span class="mord">0</span><span class="mord">9</span><span class="mord">4</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.72777em;vertical-align:-0.08333em;"></span><span class="mord">−</span><span class="mord">0</span><span class="mord">.</span><span class="mord">6</span><span class="mord">9</span><span class="mord">3</span><span class="mord">1</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.72777em;vertical-align:-0.08333em;"></span><span class="mord">−</span><span class="mord">0</span><span class="mord">.</span><span class="mord">3</span><span class="mord">5</span><span class="mord">6</span><span class="mord">7</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.72777em;vertical-align:-0.08333em;"></span><span class="mord">−</span><span class="mord">0</span><span class="mord">.</span><span class="mord">5</span><span class="mord">1</span><span class="mord">0</span><span class="mord">8</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.72777em;vertical-align:-0.08333em;"></span><span class="mord">−</span><span class="mord">5</span><span class="mord">.</span><span class="mord">2</span><span class="mord">9</span><span class="mord">0</span><span class="mord">3</span></span></span></span></span></p><p>平均值为：</p><p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mfrac><mrow><mo>−</mo><mn>5.2903</mn></mrow><mn>6</mn></mfrac><mo>=</mo><mo>−</mo><mn>0.8817</mn></mrow><annotation encoding="application/x-tex">\frac{-5.2903}{6} = -0.8817</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:2.00744em;vertical-align:-0.686em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.32144em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">6</span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">−</span><span class="mord">5</span><span class="mord">.</span><span class="mord">2</span><span class="mord">9</span><span class="mord">0</span><span class="mord">3</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.686em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.72777em;vertical-align:-0.08333em;"></span><span class="mord">−</span><span class="mord">0</span><span class="mord">.</span><span class="mord">8</span><span class="mord">8</span><span class="mord">1</span><span class="mord">7</span></span></span></span></span></p><h4 id="第三步计算困惑度"><a class="markdownIt-Anchor" href="#第三步计算困惑度"></a> 第三步：计算困惑度</h4><p>最后，取平均值的指数：</p><p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mtext>Perplexity</mtext><mo>=</mo><mi>exp</mi><mo>⁡</mo><mo stretchy="false">(</mo><mn>0.8817</mn><mo stretchy="false">)</mo><mo>≈</mo><mn>2.414</mn></mrow><annotation encoding="application/x-tex">\text{Perplexity} = \exp(0.8817) \approx 2.414</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="mord text"><span class="mord">Perplexity</span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mop">exp</span><span class="mopen">(</span><span class="mord">0</span><span class="mord">.</span><span class="mord">8</span><span class="mord">8</span><span class="mord">1</span><span class="mord">7</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">≈</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">2</span><span class="mord">.</span><span class="mord">4</span><span class="mord">1</span><span class="mord">4</span></span></span></span></span></p><h3 id="结果分析"><a class="markdownIt-Anchor" href="#结果分析"></a> 结果分析</h3><p>困惑度为 <strong>2.414</strong>，表示该模型在预测下一个词时，平均有 2.4 个可能的词可以选择。数值越小，说明模型对词的预测越有把握。</p><h3 id="另一个模型的对比"><a class="markdownIt-Anchor" href="#另一个模型的对比"></a> 另一个模型的对比</h3><p>假设另一个模型给出的预测概率如下：<br /><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>P</mi><mo stretchy="false">(</mo><mtext>&quot;The&quot;</mtext><mo stretchy="false">)</mo><mo>=</mo><mn>0.6</mn></mrow><annotation encoding="application/x-tex">P(\text{&quot;The&quot;}) = 0.6</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord text"><span class="mord">&quot;The&quot;</span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">0</span><span class="mord">.</span><span class="mord">6</span></span></span></span><br /><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>P</mi><mo stretchy="false">(</mo><mtext>&quot;cat&quot;</mtext><mo>∣</mo><mtext>&quot;The&quot;</mtext><mo stretchy="false">)</mo><mo>=</mo><mn>0.5</mn></mrow><annotation encoding="application/x-tex">P(\text{&quot;cat&quot;} \mid \text{&quot;The&quot;}) = 0.5</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord text"><span class="mord">&quot;cat&quot;</span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">∣</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord text"><span class="mord">&quot;The&quot;</span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">0</span><span class="mord">.</span><span class="mord">5</span></span></span></span><br /><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>P</mi><mo stretchy="false">(</mo><mtext>&quot;sat&quot;</mtext><mo>∣</mo><mtext>&quot;The cat&quot;</mtext><mo stretchy="false">)</mo><mo>=</mo><mn>0.4</mn></mrow><annotation encoding="application/x-tex">P(\text{&quot;sat&quot;} \mid \text{&quot;The cat&quot;}) = 0.4</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord text"><span class="mord">&quot;sat&quot;</span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">∣</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord text"><span class="mord">&quot;The cat&quot;</span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">0</span><span class="mord">.</span><span class="mord">4</span></span></span></span><br /><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>P</mi><mo stretchy="false">(</mo><mtext>&quot;on&quot;</mtext><mo>∣</mo><mtext>&quot;The cat sat&quot;</mtext><mo stretchy="false">)</mo><mo>=</mo><mn>0.8</mn></mrow><annotation encoding="application/x-tex">P(\text{&quot;on&quot;} \mid \text{&quot;The cat sat&quot;}) = 0.8</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord text"><span class="mord">&quot;on&quot;</span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">∣</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord text"><span class="mord">&quot;The cat sat&quot;</span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">0</span><span class="mord">.</span><span class="mord">8</span></span></span></span><br /><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>P</mi><mo stretchy="false">(</mo><mtext>&quot;the&quot;</mtext><mo>∣</mo><mtext>&quot;The cat sat on&quot;</mtext><mo stretchy="false">)</mo><mo>=</mo><mn>0.9</mn></mrow><annotation encoding="application/x-tex">P(\text{&quot;the&quot;} \mid \text{&quot;The cat sat on&quot;}) = 0.9</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord text"><span class="mord">&quot;the&quot;</span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">∣</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord text"><span class="mord">&quot;The cat sat on&quot;</span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">0</span><span class="mord">.</span><span class="mord">9</span></span></span></span><br /><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>P</mi><mo stretchy="false">(</mo><mtext>&quot;mat&quot;</mtext><mo>∣</mo><mtext>&quot;The cat sat on the&quot;</mtext><mo stretchy="false">)</mo><mo>=</mo><mn>0.9</mn></mrow><annotation encoding="application/x-tex">P(\text{&quot;mat&quot;} \mid \text{&quot;The cat sat on the&quot;}) = 0.9</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord text"><span class="mord">&quot;mat&quot;</span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">∣</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord text"><span class="mord">&quot;The cat sat on the&quot;</span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">0</span><span class="mord">.</span><span class="mord">9</span></span></span></span></p><p>我们重新计算困惑度：</p><h4 id="第一步计算对数概率-2"><a class="markdownIt-Anchor" href="#第一步计算对数概率-2"></a> 第一步：计算对数概率</h4><p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>log</mi><mo>⁡</mo><mo stretchy="false">(</mo><mn>0.6</mn><mo stretchy="false">)</mo><mo>=</mo><mo>−</mo><mn>0.5108</mn></mrow><annotation encoding="application/x-tex">\log(0.6) = -0.5108</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mop">lo<span style="margin-right:0.01389em;">g</span></span><span class="mopen">(</span><span class="mord">0</span><span class="mord">.</span><span class="mord">6</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.72777em;vertical-align:-0.08333em;"></span><span class="mord">−</span><span class="mord">0</span><span class="mord">.</span><span class="mord">5</span><span class="mord">1</span><span class="mord">0</span><span class="mord">8</span></span></span></span></span></p><p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>log</mi><mo>⁡</mo><mo stretchy="false">(</mo><mn>0.5</mn><mo stretchy="false">)</mo><mo>=</mo><mo>−</mo><mn>0.6931</mn></mrow><annotation encoding="application/x-tex">\log(0.5) = -0.6931</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mop">lo<span style="margin-right:0.01389em;">g</span></span><span class="mopen">(</span><span class="mord">0</span><span class="mord">.</span><span class="mord">5</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.72777em;vertical-align:-0.08333em;"></span><span class="mord">−</span><span class="mord">0</span><span class="mord">.</span><span class="mord">6</span><span class="mord">9</span><span class="mord">3</span><span class="mord">1</span></span></span></span></span></p><p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>log</mi><mo>⁡</mo><mo stretchy="false">(</mo><mn>0.4</mn><mo stretchy="false">)</mo><mo>=</mo><mo>−</mo><mn>0.9163</mn></mrow><annotation encoding="application/x-tex">\log(0.4) = -0.9163</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mop">lo<span style="margin-right:0.01389em;">g</span></span><span class="mopen">(</span><span class="mord">0</span><span class="mord">.</span><span class="mord">4</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.72777em;vertical-align:-0.08333em;"></span><span class="mord">−</span><span class="mord">0</span><span class="mord">.</span><span class="mord">9</span><span class="mord">1</span><span class="mord">6</span><span class="mord">3</span></span></span></span></span></p><p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>log</mi><mo>⁡</mo><mo stretchy="false">(</mo><mn>0.8</mn><mo stretchy="false">)</mo><mo>=</mo><mo>−</mo><mn>0.2231</mn></mrow><annotation encoding="application/x-tex">\log(0.8) = -0.2231</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mop">lo<span style="margin-right:0.01389em;">g</span></span><span class="mopen">(</span><span class="mord">0</span><span class="mord">.</span><span class="mord">8</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.72777em;vertical-align:-0.08333em;"></span><span class="mord">−</span><span class="mord">0</span><span class="mord">.</span><span class="mord">2</span><span class="mord">2</span><span class="mord">3</span><span class="mord">1</span></span></span></span></span></p><p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>log</mi><mo>⁡</mo><mo stretchy="false">(</mo><mn>0.9</mn><mo stretchy="false">)</mo><mo>=</mo><mo>−</mo><mn>0.1054</mn></mrow><annotation encoding="application/x-tex">\log(0.9) = -0.1054</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mop">lo<span style="margin-right:0.01389em;">g</span></span><span class="mopen">(</span><span class="mord">0</span><span class="mord">.</span><span class="mord">9</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.72777em;vertical-align:-0.08333em;"></span><span class="mord">−</span><span class="mord">0</span><span class="mord">.</span><span class="mord">1</span><span class="mord">0</span><span class="mord">5</span><span class="mord">4</span></span></span></span></span></p><p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>log</mi><mo>⁡</mo><mo stretchy="false">(</mo><mn>0.95</mn><mo stretchy="false">)</mo><mo>=</mo><mo>−</mo><mn>0.0513</mn></mrow><annotation encoding="application/x-tex">\log(0.95) = -0.0513</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mop">lo<span style="margin-right:0.01389em;">g</span></span><span class="mopen">(</span><span class="mord">0</span><span class="mord">.</span><span class="mord">9</span><span class="mord">5</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.72777em;vertical-align:-0.08333em;"></span><span class="mord">−</span><span class="mord">0</span><span class="mord">.</span><span class="mord">0</span><span class="mord">5</span><span class="mord">1</span><span class="mord">3</span></span></span></span></span></p><h4 id="第二步计算总和并取平均值-2"><a class="markdownIt-Anchor" href="#第二步计算总和并取平均值-2"></a> 第二步：计算总和并取平均值</h4><p>相加：</p><p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mo>−</mo><mn>0.5108</mn><mo>+</mo><mo>−</mo><mn>0.6931</mn><mo>+</mo><mo>−</mo><mn>0.9163</mn><mo>+</mo><mo>−</mo><mn>0.2231</mn><mo>+</mo><mo>−</mo><mn>0.1054</mn><mo>+</mo><mo>−</mo><mn>0.0513</mn><mo>=</mo><mo>−</mo><mn>2.499</mn></mrow><annotation encoding="application/x-tex">-0.5108 + -0.6931 + -0.9163 + -0.2231 + -0.1054 + -0.0513 = -2.499</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.72777em;vertical-align:-0.08333em;"></span><span class="mord">−</span><span class="mord">0</span><span class="mord">.</span><span class="mord">5</span><span class="mord">1</span><span class="mord">0</span><span class="mord">8</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.72777em;vertical-align:-0.08333em;"></span><span class="mord">−</span><span class="mord">0</span><span class="mord">.</span><span class="mord">6</span><span class="mord">9</span><span class="mord">3</span><span class="mord">1</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.72777em;vertical-align:-0.08333em;"></span><span class="mord">−</span><span class="mord">0</span><span class="mord">.</span><span class="mord">9</span><span class="mord">1</span><span class="mord">6</span><span class="mord">3</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.72777em;vertical-align:-0.08333em;"></span><span class="mord">−</span><span class="mord">0</span><span class="mord">.</span><span class="mord">2</span><span class="mord">2</span><span class="mord">3</span><span class="mord">1</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.72777em;vertical-align:-0.08333em;"></span><span class="mord">−</span><span class="mord">0</span><span class="mord">.</span><span class="mord">1</span><span class="mord">0</span><span class="mord">5</span><span class="mord">4</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.72777em;vertical-align:-0.08333em;"></span><span class="mord">−</span><span class="mord">0</span><span class="mord">.</span><span class="mord">0</span><span class="mord">5</span><span class="mord">1</span><span class="mord">3</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.72777em;vertical-align:-0.08333em;"></span><span class="mord">−</span><span class="mord">2</span><span class="mord">.</span><span class="mord">4</span><span class="mord">9</span><span class="mord">9</span></span></span></span></span></p><p>平均值为：</p><p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mfrac><mrow><mo>−</mo><mn>2.499</mn></mrow><mn>6</mn></mfrac><mo>=</mo><mo>−</mo><mn>0.4165</mn></mrow><annotation encoding="application/x-tex">\frac{-2.499}{6} = -0.4165</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:2.00744em;vertical-align:-0.686em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.32144em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">6</span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">−</span><span class="mord">2</span><span class="mord">.</span><span class="mord">4</span><span class="mord">9</span><span class="mord">9</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.686em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.72777em;vertical-align:-0.08333em;"></span><span class="mord">−</span><span class="mord">0</span><span class="mord">.</span><span class="mord">4</span><span class="mord">1</span><span class="mord">6</span><span class="mord">5</span></span></span></span></span></p><h4 id="第三步计算困惑度-2"><a class="markdownIt-Anchor" href="#第三步计算困惑度-2"></a> 第三步：计算困惑度</h4><p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mtext>Perplexity</mtext><mo>=</mo><mi>exp</mi><mo>⁡</mo><mo stretchy="false">(</mo><mn>0.4165</mn><mo stretchy="false">)</mo><mo>≈</mo><mn>1.516</mn></mrow><annotation encoding="application/x-tex">\text{Perplexity} = \exp(0.4165) \approx 1.516</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="mord text"><span class="mord">Perplexity</span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mop">exp</span><span class="mopen">(</span><span class="mord">0</span><span class="mord">.</span><span class="mord">4</span><span class="mord">1</span><span class="mord">6</span><span class="mord">5</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">≈</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">1</span><span class="mord">.</span><span class="mord">5</span><span class="mord">1</span><span class="mord">6</span></span></span></span></span></p><h3 id="结果对比"><a class="markdownIt-Anchor" href="#结果对比"></a> 结果对比</h3><p>新模型的困惑度为 <strong>1.516</strong>，比之前的模型 <strong>2.414</strong> 更低，说明新模型对词序列的预测更加准确。</p><hr /><p>通过这个例子，可以看到困惑度越低，说明模型对语言的理解和预测越好。在语言模型的评估中，困惑度是衡量其预测能力的重要指标。</p>]]></content>
      
      
      
        <tags>
            
            <tag> LLM </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
